import { Callout, Steps, Tabs, Tab } from 'nextra/components'

# Getting Started

Add enterprise features to any existing MCP server without changing a single line of your original code. Complete setup in under 5 minutes.

<Callout type="info">
  The MCP Proxy Wrapper requires Node.js 18+ and works with any existing MCP server without code changes.
</Callout>

## üöÄ 5-Minute Setup

<Steps>

### Step 1: Install (30 seconds)

```bash
npm install mcp-proxy-wrapper
```

### Step 2: Wrap Your Existing Server (3 minutes)

**Your existing server code (NO CHANGES REQUIRED):**

```typescript
// server.js - Your existing MCP server
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { z } from 'zod';

const server = new McpServer({
  name: 'my-existing-server',
  version: '1.0.0'
});

// Your existing tools work exactly as before
server.tool('getData', {
  query: z.string()
}, async (args) => {
  const data = await fetchData(args.query);
  return {
    content: [{ type: 'text', text: data }]
  };
});
```

**Add proxy wrapper (new file: enhanced-server.js):**

```typescript
// enhanced-server.js - Your enhanced server with zero changes to existing code
import { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';
import { server } from './server.js'; // Import your existing server

// Enhance your existing server with AI and monitoring
const enhancedServer = await wrapWithProxy(server, {
  plugins: [
    new LLMSummarizationPlugin({
      options: {
        provider: 'openai',
        openaiApiKey: process.env.OPENAI_API_KEY,
        minContentLength: 200 // Auto-summarize responses over 200 chars
      }
    })
  ],
  hooks: {
    beforeToolCall: async (context) => {
      console.log(`üîß [${new Date().toISOString()}] Calling: ${context.toolName}`);
      console.log(`üìù Args:`, context.args);
    },
    afterToolCall: async (context, result) => {
      console.log(`‚úÖ [${new Date().toISOString()}] Completed: ${context.toolName}`);
      return result;
    }
  }
});

// Your existing tools are now enhanced with:
// ‚úÖ AI-powered summarization for long responses  
// ‚úÖ Automatic request/response logging
// ‚úÖ Performance monitoring
// ‚úÖ Plugin extensibility
```

### Step 3: Use Your Enhanced Server (1 minute)

```typescript
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';

// Start your enhanced server
const transport = new StdioServerTransport();
await enhancedServer.connect(transport);
```

**üéâ That's it! Your server now has enterprise features without any changes to your original code.**

</Steps>

## üîç See the Difference

### Before (Your Original Server)
```typescript
// Call your original tool
const result = await client.callTool({
  name: 'getData',
  arguments: { query: 'AI trends' }
});

console.log(result.content[0].text); 
// Output: "Artificial intelligence trends include machine learning, natural language processing, computer vision..."
// (No logging, no summarization, no monitoring)
```

### After (With Proxy Wrapper)
```typescript
// Same tool call, enhanced results
const result = await client.callTool({
  name: 'getData',
  arguments: { query: 'AI trends' }
});

// Console output shows automatic logging:
// üîß [2024-01-15T10:30:00.000Z] Calling: getData
// üìù Args: { query: 'AI trends' }
// ‚úÖ [2024-01-15T10:30:02.000Z] Completed: getData

console.log(result.content[0].text);
// Output: "Summary: Key AI trends include ML advances, NLP breakthroughs..."
// (Automatically summarized by AI!)

console.log(result._meta);
// {
//   summarized: true,
//   originalLength: 1200,
//   summaryLength: 150,
//   processedAt: "2024-01-15T10:30:02.000Z"
// }
```

**‚ú® Your server instantly gained:**
- ü§ñ **AI Summarization** - Long responses automatically summarized
- üìä **Request Logging** - Full visibility into tool usage  
- ‚ö° **Performance Monitoring** - Response times and metadata
- üîß **Extensibility** - Easy to add more plugins
- üõ°Ô∏è **Enterprise Ready** - Authentication and rate limiting hooks available

## Your First Plugin

Let's add the LLM Summarization plugin to enhance your tools:

<Steps>
### Configure the Plugin

```typescript
import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';

const summaryPlugin = new LLMSummarizationPlugin();
summaryPlugin.updateConfig({
  options: {
    provider: 'openai', // or 'mock' for testing
    openaiApiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o-mini',
    maxTokens: 150,
    temperature: 0.3,
    summarizeTools: ['long-analysis'],
    minContentLength: 100
  }
});

const proxiedServer = await wrapWithProxy(server, {
  plugins: [summaryPlugin]
});
```

### Test Summarization

```typescript
// This tool now has automatic summarization
proxiedServer.tool('long-analysis', {
  data: z.string()
}, async (args) => {
  const result = await performLongAnalysis(args.data);
  // Plugin automatically summarizes long responses
  return result;
});
```
</Steps>

## Development Workflow

### Environment Setup

Create a `.env` file for your configuration:

```bash
# OpenAI API key for LLM plugins
OPENAI_API_KEY=sk-your-openai-key-here

# Optional: Logging level
LOG_LEVEL=debug
```

### Project Structure

```
my-mcp-server/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Main server file
‚îÇ   ‚îú‚îÄ‚îÄ tools/            # Your tool implementations
‚îÇ   ‚îî‚îÄ‚îÄ config/           # Configuration
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ .env                  # Environment variables
‚îî‚îÄ‚îÄ tsconfig.json
```

### Sample Server Implementation

```typescript
// src/index.ts
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';
import { z } from 'zod';

async function main() {
  // Create base server
  const server = new McpServer({
    name: 'my-ai-tools',
    version: '1.0.0'
  });

  // Configure plugins
  const plugins = [];
  
  if (process.env.OPENAI_API_KEY) {
    const summaryPlugin = new LLMSummarizationPlugin();
    summaryPlugin.updateConfig({
      options: {
        provider: 'openai',
        openaiApiKey: process.env.OPENAI_API_KEY,
        model: 'gpt-4o-mini',
        maxTokens: 150
      }
    });
    plugins.push(summaryPlugin);
    
    const memoryPlugin = new ChatMemoryPlugin();
    memoryPlugin.updateConfig({
      options: {
        saveResponses: true,
        maxEntries: 100,
        enableChat: true
      }
    });
    plugins.push(memoryPlugin);
  }

  // Wrap with proxy
  const proxiedServer = await wrapWithProxy(server, { plugins });

  // Register tools
  proxiedServer.tool('text-analysis', {
    text: z.string(),
    analysisType: z.enum(['sentiment', 'summary', 'keywords'])
  }, async (args) => {
    // Your AI analysis logic here
    const result = await analyzeText(args.text, args.analysisType);
    
    return {
      content: [{
        type: 'text',
        text: JSON.stringify(result, null, 2)
      }]
    };
  });

  // Start server
  const transport = new StdioServerTransport();
  await proxiedServer.connect(transport);
}

main().catch(console.error);
```

## Testing Your Server

### Manual Testing with MCP Inspector

```bash
# Install MCP Inspector
npm install -g @modelcontextprotocol/inspector

# Test your server
mcp-inspector node dist/index.js
```

### Automated Testing

```typescript
// tests/server.test.ts
import { describe, test, expect } from '@jest/globals';
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { wrapWithProxy } from 'mcp-proxy-wrapper';
import { z } from 'zod';

describe('My MCP Server', () => {
  test('tool returns expected result', async () => {
    // Create test server
    const server = new McpServer('test-server', '1.0.0');
    
    // Register test tool
    server.tool('text-analysis', {
      text: z.string(),
      analysisType: z.enum(['sentiment', 'readability'])
    }, async (args) => {
      return {
        content: [{ type: 'text', text: `Analysis result: ${args.analysisType} is positive` }]
      };
    });
    
    const proxiedServer = await wrapWithProxy(server, { plugins: [] });
    
    const result = await proxiedServer.callTool('text-analysis', {
      text: 'This is great!',
      analysisType: 'sentiment'
    });
    
    expect(result.content[0].text).toContain('positive');
  });
});
```

## Transport Options

The proxy wrapper supports all MCP transport methods:

<Tabs items={['STDIO', 'WebSocket', 'HTTP/SSE', 'InMemory']}>
  <Tab>
    ```typescript
    // STDIO (most common for CLI tools)
    import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
    
    const transport = new StdioServerTransport();
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // WebSocket (for web applications)
    import { WebSocketTransport } from '@modelcontextprotocol/sdk/server/websocket.js';
    
    const transport = new WebSocketTransport({ port: 3000 });
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // HTTP with SSE (for REST APIs)
    import { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';
    
    const transport = new SSEServerTransport('/mcp', (request, response) => {
      // Handle HTTP requests
    });
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // InMemory (for testing)
    import { InMemoryTransport } from '@modelcontextprotocol/sdk/inMemory.js';
    
    const { client, server: transport } = InMemoryTransport.create();
    await proxiedServer.connect(transport);
    ```
  </Tab>
</Tabs>

## Common Patterns

### Environment-Based Configuration

```typescript
const config = {
  development: {
    logLevel: 'debug',
    plugins: []
  },
  production: {
    logLevel: 'info', 
    plugins: [
      (() => {
        const plugin = new LLMSummarizationPlugin();
        plugin.updateConfig({
          options: {
            provider: 'openai',
            openaiApiKey: process.env.OPENAI_API_KEY!, // Set via environment
            model: 'gpt-4o-mini'
          }
        });
        return plugin;
      })()
    ]
  }
};

const currentConfig = config[process.env.NODE_ENV || 'development'];
```

## Security Best Practices

<Callout type="warning">
  **API Key Security**: Never commit API keys to version control. Always use environment variables or secure secrets management.
</Callout>

### Environment Variables

Create a `.env` file for local development (never commit this file):

```bash
# .env (add to .gitignore!)
NODE_ENV=development
OPENAI_API_KEY=sk-your-openai-key-here
BLOCKCHAIN_API_KEY=your-blockchain-api-key-here
DATABASE_URL=postgresql://user:pass@localhost:5432/myapp
```

### Git Security

Ensure your `.gitignore` includes:

```gitignore
# Environment files
.env
.env.local
.env.production
.env.*.local

# API keys and secrets
**/config/secrets.json
**/config/*.key
*.pem

# Build artifacts with embedded secrets
dist/
build/
```

### Production Deployment

Use secure environment variable injection:

```dockerfile
# Dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY dist/ ./dist/

# Never COPY .env files into Docker images!
# Use runtime environment variables instead

CMD ["node", "dist/index.js"]
```

```yaml
# docker-compose.yml or Kubernetes manifests
version: '3.8'
services:
  mcp-server:
    build: .
    environment:
      - NODE_ENV=production
      # Reference external secrets, never inline API keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - BLOCKCHAIN_API_KEY=${BLOCKCHAIN_API_KEY}
    # Use Docker secrets or external secret management
    secrets:
      - openai_api_key
      - blockchain_api_key
```

### Validation and Sanitization

```typescript
// Always validate sensitive inputs
proxiedServer.tool('api-call', {
  apiKey: z.string().min(20).max(200), // Validate API key format
  endpoint: z.string().url(), // Ensure valid URLs only
  data: z.object({}).passthrough() // Validate data structure
}, async ({ apiKey, endpoint, data }) => {
  // Additional validation
  if (!endpoint.startsWith('https://')) {
    throw new Error('Only HTTPS endpoints allowed');
  }
  
  // Use the validated inputs safely
  return await makeSecureApiCall(endpoint, data, apiKey);
});
```

### Error Handling

```typescript
proxiedServer.tool('risky-operation', schema, async (args) => {
  try {
    return await performRiskyOperation(args);
  } catch (error) {
    // Plugin errors are handled automatically
    // Tool errors should return MCP error format
    return {
      content: [{
        type: 'text',
        text: 'Operation failed'
      }],
      isError: true
    };
  }
});
```

### Multiple Plugins

```typescript
const proxiedServer = await wrapWithProxy(server, {
  plugins: [
    { plugin: memoryPlugin, priority: 20 },      // Memory first (higher priority)
    { plugin: summaryPlugin, priority: 10 }      // Then summarization (lower priority)
  ]
});
```

## Next Steps

<Callout type="success">
  **Your server is now enhanced with plugin capabilities!** Explore our other guides to add more functionality.
</Callout>

- **[How It Works](/how-it-works)**: Understand the proxy wrapper architecture
- **[Plugins](/plugins)**: Add summarization, memory, and more
- **[Examples](/examples)**: See real-world implementations
- **[API Reference](/api-reference)**: Complete API documentation
- **[Deployment](/deployment)**: Deploy to production

## Troubleshooting

### Common Issues

**Plugin not loading:**
```bash
# Check your environment variables
echo $OPENAI_API_KEY

# Verify plugin configuration
npm run test
```

**Tool calls failing:**
```typescript
// Add debug logging
const proxiedServer = await wrapWithProxy(server, {
  plugins: [plugin],
  debug: true
});
```

**TypeScript errors:**
```bash
# Ensure you have the latest types
npm install --save-dev @types/node
```

Need more help? Check our [troubleshooting guide](/troubleshooting) or open an issue on GitHub.
import { Callout, Steps, Tabs, Tab } from 'nextra/components'

# Getting Started

Transform any MCP server into a powerful, extensible platform with enterprise-grade features in minutes.

<Callout type="info">
  The MCP Proxy Wrapper requires Node.js 18+ and works with any existing MCP server without code changes.
</Callout>

## Installation

<Steps>
### Install the Package

```bash
npm install mcp-proxy-wrapper
```

### Basic Setup

Create a simple wrapper around your existing MCP server:

```typescript
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { wrapWithProxy } from 'mcp-proxy-wrapper';
import { z } from 'zod';

// Your existing MCP server
const server = new McpServer({
  name: 'my-server',
  version: '1.0.0'
});

// Wrap with proxy functionality
const proxiedServer = await wrapWithProxy(server, {
  plugins: [] // Add plugins here
});

// Register tools with enhanced functionality
// Tools registered after wrapping get hook/plugin functionality
proxiedServer.tool('hello-world', {
  name: z.string()
}, async (args) => {
  return {
    content: [{
      type: 'text',
      text: `Hello, ${args.name}!`
    }]
  };
});

// Note: Any tools registered BEFORE wrapping remain available
// but won't have hook/plugin functionality applied
```

### Start the Server

```typescript
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';

const transport = new StdioServerTransport();
await proxiedServer.connect(transport);
```
</Steps>

## Your First Plugin

Let's add the LLM Summarization plugin to enhance your tools:

<Steps>
### Configure the Plugin

```typescript
import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';

const summaryPlugin = new LLMSummarizationPlugin();
summaryPlugin.updateConfig({
  options: {
    provider: 'openai', // or 'mock' for testing
    openaiApiKey: process.env.OPENAI_API_KEY,
    model: 'gpt-4o-mini',
    maxTokens: 150,
    temperature: 0.3,
    summarizeTools: ['long-analysis'],
    minContentLength: 100
  }
});

const proxiedServer = await wrapWithProxy(server, {
  plugins: [summaryPlugin]
});
```

### Test Summarization

```typescript
// This tool now has automatic summarization
proxiedServer.tool('long-analysis', {
  data: z.string()
}, async (args) => {
  const result = await performLongAnalysis(args.data);
  // Plugin automatically summarizes long responses
  return result;
});
```
</Steps>

## Development Workflow

### Environment Setup

Create a `.env` file for your configuration:

```bash
# OpenAI API key for LLM plugins
OPENAI_API_KEY=sk-your-openai-key-here

# Optional: Logging level
LOG_LEVEL=debug
```

### Project Structure

```
my-mcp-server/
├── src/
│   ├── index.ts          # Main server file
│   ├── tools/            # Your tool implementations
│   └── config/           # Configuration
├── package.json
├── .env                  # Environment variables
└── tsconfig.json
```

### Sample Server Implementation

```typescript
// src/index.ts
import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';
import { z } from 'zod';

async function main() {
  // Create base server
  const server = new McpServer({
    name: 'my-ai-tools',
    version: '1.0.0'
  });

  // Configure plugins
  const plugins = [];
  
  if (process.env.OPENAI_API_KEY) {
    const summaryPlugin = new LLMSummarizationPlugin();
    summaryPlugin.updateConfig({
      options: {
        provider: 'openai',
        openaiApiKey: process.env.OPENAI_API_KEY,
        model: 'gpt-4o-mini',
        maxTokens: 150
      }
    });
    plugins.push(summaryPlugin);
    
    const memoryPlugin = new ChatMemoryPlugin();
    memoryPlugin.updateConfig({
      options: {
        saveResponses: true,
        maxEntries: 100,
        enableChat: true
      }
    });
    plugins.push(memoryPlugin);
  }

  // Wrap with proxy
  const proxiedServer = await wrapWithProxy(server, { plugins });

  // Register tools
  proxiedServer.tool('text-analysis', {
    text: z.string(),
    analysisType: z.enum(['sentiment', 'summary', 'keywords'])
  }, async (args) => {
    // Your AI analysis logic here
    const result = await analyzeText(args.text, args.analysisType);
    
    return {
      content: [{
        type: 'text',
        text: JSON.stringify(result, null, 2)
      }]
    };
  });

  // Start server
  const transport = new StdioServerTransport();
  await proxiedServer.connect(transport);
}

main().catch(console.error);
```

## Testing Your Server

### Manual Testing with MCP Inspector

```bash
# Install MCP Inspector
npm install -g @modelcontextprotocol/inspector

# Test your server
mcp-inspector node dist/index.js
```

### Automated Testing

```typescript
// tests/server.test.ts
import { describe, test, expect } from '@jest/globals';
import { wrapWithProxy } from 'mcp-proxy-wrapper';
import { createTestServer } from './test-utils';

describe('My MCP Server', () => {
  test('tool returns expected result', async () => {
    const server = createTestServer();
    const proxiedServer = await wrapWithProxy(server, { plugins: [] });
    
    const result = await proxiedServer.callTool('text-analysis', {
      text: 'This is great!',
      analysisType: 'sentiment'
    });
    
    expect(result.content[0].text).toContain('positive');
  });
});
```

## Transport Options

The proxy wrapper supports all MCP transport methods:

<Tabs items={['STDIO', 'WebSocket', 'HTTP/SSE', 'InMemory']}>
  <Tab>
    ```typescript
    // STDIO (most common for CLI tools)
    import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
    
    const transport = new StdioServerTransport();
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // WebSocket (for web applications)
    import { WebSocketTransport } from '@modelcontextprotocol/sdk/server/websocket.js';
    
    const transport = new WebSocketTransport({ port: 3000 });
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // HTTP with SSE (for REST APIs)
    import { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';
    
    const transport = new SSEServerTransport('/mcp', (request, response) => {
      // Handle HTTP requests
    });
    await proxiedServer.connect(transport);
    ```
  </Tab>
  <Tab>
    ```typescript
    // InMemory (for testing)
    import { InMemoryTransport } from '@modelcontextprotocol/sdk/inMemory.js';
    
    const { client, server: transport } = InMemoryTransport.create();
    await proxiedServer.connect(transport);
    ```
  </Tab>
</Tabs>

## Common Patterns

### Environment-Based Configuration

```typescript
const config = {
  development: {
    logLevel: 'debug',
    plugins: []
  },
  production: {
    logLevel: 'info', 
    plugins: [
      (() => {
        const plugin = new LLMSummarizationPlugin();
        plugin.updateConfig({
          options: {
            provider: 'openai',
            openaiApiKey: process.env.OPENAI_API_KEY!,
            model: 'gpt-4o-mini'
          }
        });
        return plugin;
      })()
    ]
  }
};

const currentConfig = config[process.env.NODE_ENV || 'development'];
```

### Error Handling

```typescript
proxiedServer.tool('risky-operation', schema, async (args) => {
  try {
    return await performRiskyOperation(args);
  } catch (error) {
    // Plugin errors are handled automatically
    // Tool errors should return MCP error format
    return {
      content: [{
        type: 'text',
        text: 'Operation failed'
      }],
      isError: true
    };
  }
});
```

### Multiple Plugins

```typescript
const proxiedServer = await wrapWithProxy(server, {
  plugins: [
    { plugin: memoryPlugin, priority: 20 },      // Memory first (higher priority)
    { plugin: summaryPlugin, priority: 10 }      // Then summarization (lower priority)
  ]
});
```

## Next Steps

<Callout type="success">
  **Your server is now enhanced with plugin capabilities!** Explore our other guides to add more functionality.
</Callout>

- **[How It Works](/how-it-works)**: Understand the proxy wrapper architecture
- **[Plugins](/plugins)**: Add monetization, analytics, and more
- **[Examples](/examples)**: See real-world implementations
- **[API Reference](/api-reference)**: Complete API documentation
- **[Deployment](/deployment)**: Deploy to production

## Troubleshooting

### Common Issues

**Plugin not loading:**
```bash
# Check your environment variables
echo $OPENAI_API_KEY

# Verify plugin configuration
npm run test
```

**Tool calls failing:**
```typescript
// Add debug logging
const proxiedServer = await wrapWithProxy(server, {
  plugins: [plugin],
  debug: true
});
```

**TypeScript errors:**
```bash
# Ensure you have the latest types
npm install --save-dev @types/node
```

Need more help? Check our [troubleshooting guide](/troubleshooting) or open an issue on GitHub.
{"/api-reference":{"title":"API Reference","data":{"":"Complete API documentation for the MCP Proxy Wrapper and plugin system.","core-api#Core API":"","wrapwithproxyserver-options#wrapWithProxy(server, options)":"Wraps an existing MCP server with proxy functionality and plugin support.\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [],\n  hooks?: ProxyHooks,\n  debug?: boolean\n});","parameters#Parameters":"Parameter\tType\tRequired\tDescription\tserver\tMcpServer\tYes\tMCP server instance to wrap\toptions.plugins\t(ProxyPlugin | PluginRegistration)[]\tNo\tArray of plugins to apply\toptions.hooks\tProxyHooks\tNo\tBefore/after tool call hooks\toptions.debug\tboolean\tNo\tEnable debug logging (default: false)","returns#Returns":"Promise<McpServer> - Enhanced server instance with proxy capabilities","example#Example":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' with API key for production\n    minContentLength: 100,\n    summarizeTools: ['search', 'analyze']\n  }\n});\nconst server = new McpServer({ name: 'my-server', version: '1.0.0' });\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin],\n  debug: true\n});","wrapwithenhancedproxyserver-options-v2-api#wrapWithEnhancedProxy(server, options) (v2 API)":"Enhanced version with advanced lifecycle management and performance features.\nimport { wrapWithEnhancedProxy, EnhancedProxyWrapperOptions } from 'mcp-proxy-wrapper';\nconst proxiedServer = await wrapWithEnhancedProxy(server, {\n  plugins: [],\n  hooks?: ProxyHooks,\n  lifecycle?: LifecycleConfig,\n  execution?: ExecutionConfig,\n  performance?: PerformanceConfig\n});","parameters-1#Parameters":"Parameter\tType\tRequired\tDescription\tserver\tMcpServer\tYes\tMCP server instance to wrap\toptions.plugins\t(ProxyPlugin | PluginRegistration)[]\tNo\tArray of plugins to apply\toptions.hooks\tProxyHooks\tNo\tBefore/after tool call hooks\toptions.lifecycle\tLifecycleConfig\tNo\tPlugin lifecycle management\toptions.execution\tExecutionConfig\tNo\tHook execution configuration\toptions.performance\tPerformanceConfig\tNo\tPerformance monitoring","returns-1#Returns":"Promise<McpServer> - Enhanced server with v2 proxy capabilities","plugin-interface#Plugin Interface":"","proxyplugin#ProxyPlugin":"Base interface that all plugins must implement.\ninterface ProxyPlugin {\n  name: string;\n  version: string;\n  \n  // Lifecycle hooks\n  beforeToolCall?(context: ToolCallContext): Promise<void | ToolCallResult>;\n  afterToolCall?(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>;\n  \n  // Plugin lifecycle\n  initialize?(context: PluginContext): Promise<void>;\n  destroy?(): Promise<void>;\n}","properties#Properties":"Property\tType\tRequired\tDescription\tname\tstring\tYes\tUnique plugin identifier\tversion\tstring\tYes\tPlugin version (semver)","methods#Methods":"Method\tParameters\tReturns\tDescription\tbeforeToolCall\tcontext: ToolCallContext\tPromise<void | ToolCallResult>\tCalled before tool execution\tafterToolCall\tcontext: ToolCallContext, result: ToolCallResult\tPromise<ToolCallResult>\tCalled after tool execution\tinitialize\tcontext: PluginContext\tPromise<void>\tPlugin initialization\tdestroy\tNone\tPromise<void>\tPlugin cleanup","pluginregistration#PluginRegistration":"Configuration object for registering plugins with specific settings.\ninterface PluginRegistration {\n  plugin: ProxyPlugin;\n  config?: PluginConfig;\n}","properties-1#Properties":"Property\tType\tRequired\tDescription\tplugin\tProxyPlugin\tYes\tThe plugin instance\tconfig\tPluginConfig\tNo\tPlugin-specific configuration","example-1#Example":"const proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    // Direct plugin registration\n    summaryPlugin,\n    \n    // Plugin with configuration\n    {\n      plugin: memoryPlugin,\n      config: {\n        // Plugin-specific settings go inside the 'options' object\n        options: {\n          maxEntries: 50, // Note: using actual option from ChatMemoryPlugin\n          sessionTimeout: 60 * 60 * 1000 // 1 hour\n        }\n      }\n    }\n  ]\n});","toolcallcontext#ToolCallContext":"Context object provided to plugin hooks during tool execution.\ninterface ToolCallContext {\n  toolName: string;\n  args: Record<string, any>;\n  metadata: {\n    requestId: string;\n    timestamp: number;\n    userId?: string;\n    [key: string]: any;\n  };\n}","properties-2#Properties":"Property\tType\tDescription\ttoolName\tstring\tName of the tool being called\targs\tRecord<string, any>\tArguments passed to the tool\tmetadata.requestId\tstring\tUnique request identifier\tmetadata.timestamp\tnumber\tRequest timestamp (Unix milliseconds)\tmetadata.userId\tstring?\tUser identifier (if available)","toolcallresult#ToolCallResult":"Result object returned from tool execution.\ninterface ToolCallResult {\n  content: Array<{\n    type: 'text' | 'image' | 'resource';\n    text?: string;\n    data?: string;\n    url?: string;\n    mimeType?: string;\n  }>;\n  isError?: boolean;\n  metadata?: Record<string, any>;\n}","properties-3#Properties":"Property\tType\tDescription\tcontent\tArray<ContentBlock>\tTool response content\tisError\tboolean?\tIndicates if result is an error\tmetadata\tRecord<string, any>?\tAdditional result metadata","plugincontext#PluginContext":"Context provided during plugin initialization.\ninterface PluginContext {\n  server: McpServer;\n  logger: Logger;\n  config: Record<string, any>;\n}","core-plugin-apis#Core Plugin APIs":"","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\n// Configuration options\ninterface SummarizationConfig {\n  provider: 'openai' | 'mock';      // AI provider\n  openaiApiKey?: string;            // OpenAI API key\n  model?: string;                   // Model name (default: gpt-4o-mini)\n  maxTokens?: number;               // Max tokens in summary\n  temperature?: number;             // Generation temperature\n  summarizeTools?: string[];        // Tools to summarize (empty = all)\n  minContentLength?: number;        // Min content length to summarize\n  saveOriginal?: boolean;           // Save original responses\n}\n// Update plugin configuration\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    summarizeTools: ['search', 'research', 'analyze'],\n    minContentLength: 100\n  }\n});\n// Get original result by storage key\nconst original = await summaryPlugin.getOriginalResult(storageKey);\n// Get plugin statistics\nconst stats = await summaryPlugin.getStats();","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\n// Configuration options\ninterface MemoryConfig {\n  provider: 'openai' | 'mock';      // Chat AI provider\n  openaiApiKey?: string;            // OpenAI API key\n  model?: string;                   // Model for chat responses\n  saveResponses?: boolean;          // Save tool responses\n  enableChat?: boolean;             // Enable chat functionality\n  maxEntries?: number;              // Max stored entries\n  maxSessions?: number;             // Max chat sessions\n  sessionTimeout?: number;          // Session timeout in ms\n  excludeTools?: string[];          // Tools to exclude from saving\n}\n// Update plugin configuration\nmemoryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    sessionTimeout: 24 * 60 * 60 * 1000\n  }\n});\n// Start chat session\nconst sessionId = await memoryPlugin.startChatSession(userId);\n// Chat with memory\nconst response = await memoryPlugin.chatWithMemory(\n  sessionId, \n  'What data do I have about sales?', \n  userId\n);\n// Search conversations\nconst results = memoryPlugin.searchConversations('sales metrics', userId);\n// Get conversation history\nconst history = memoryPlugin.getConversationHistory(userId, 20);","plugin-data-types#Plugin Data Types":"// LLM Summarization Plugin Types\ninterface StoredResult {\n  originalResult: ToolCallResult;\n  context: Omit<PluginContext, 'pluginData'>;\n  timestamp: number;\n  toolName: string;\n  requestId: string;\n  metadata?: Record<string, any>;\n}\ninterface LLMProvider {\n  generateSummary(content: string, prompt: string, options?: any): Promise<string>;\n}\n// Chat Memory Plugin Types\ninterface ConversationEntry {\n  id: string;\n  toolName: string;\n  request: {\n    args: Record<string, any>;\n    timestamp: number;\n  };\n  response: {\n    content: string;\n    metadata?: Record<string, any>;\n    timestamp: number;\n  };\n  context: {\n    requestId: string;\n    userId?: string;\n    sessionId?: string;\n  };\n}\ninterface ChatSession {\n  id: string;\n  userId?: string;\n  messages: ChatMessage[];\n  createdAt: number;\n  lastActivity: number;\n}\ninterface ChatMessage {\n  id: string;\n  type: 'user' | 'assistant' | 'system';\n  content: string;\n  timestamp: number;\n  metadata?: Record<string, any>;\n}","logger-interface#Logger Interface":"","logger#Logger":"Standard logging interface used throughout the system.\ninterface Logger {\n  debug(message: string, meta?: any): void;\n  info(message: string, meta?: any): void;\n  warn(message: string, meta?: any): void;\n  error(message: string, meta?: any): void;\n}","built-in-logging#Built-in Logging":"The proxy wrapper includes built-in logging with colored output. Enable debug mode to see detailed execution logs:\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin],\n  debug: true  // Enables detailed logging\n});","error-handling#Error Handling":"","plugin-errors#Plugin Errors":"Plugin errors are automatically caught and logged without breaking tool execution:\n// Plugin error handling\ntry {\n  await plugin.beforeToolCall(context);\n} catch (error) {\n  console.error(`Plugin ${plugin.name} error:`, error);\n  // Tool execution continues\n}","tool-errors#Tool Errors":"Tools should return error results in MCP format:\n// Tool error response\nreturn {\n  content: [{\n    type: 'text',\n    text: 'Error: Invalid input provided'\n  }],\n  isError: true\n};","plugin-errors-1#Plugin Errors":"Plugin errors are handled gracefully by the proxy wrapper:\n// LLM Summarization error (falls back to original)\nreturn {\n  ...result,\n  result: {\n    ...result.result,\n    _meta: {\n      ...result.result._meta,\n      summarizationError: 'OpenAI API unavailable',\n      fallbackToOriginal: true\n    }\n  }\n};\n// Chat Memory error (logs but doesn't break tool call)\ncatch (error) {\n  this.logger?.error(`Failed to save conversation entry: ${error}`);\n  return result; // Return original result\n}","type-definitions#Type Definitions":"","complete-typescript-definitions#Complete TypeScript Definitions":"// Export all types for use in your applications\nexport {\n  ProxyPlugin,\n  BasePlugin,\n  ToolCallContext,\n  ToolCallResult,\n  PluginContext,\n  PluginConfig,\n  PluginMetadata,\n  PluginStats,\n  Logger\n} from 'mcp-proxy-wrapper';","migration-guide#Migration Guide":"","from-direct-mcp-server#From Direct MCP Server":"// Before: Direct MCP server\nconst server = new McpServer(config);\nserver.tool('my-tool', schema, handler);\n// After: Wrapped with proxy\nconst proxiedServer = await wrapWithProxy(server, { plugins: [] });\nproxiedServer.tool('my-tool', schema, handler);","adding-ai-enhancement#Adding AI Enhancement":"// Add AI summarization to existing setup\nimport { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['research', 'analyze'],\n    minContentLength: 200\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin]\n});\nBackward Compatibility: The proxy wrapper maintains full compatibility with existing MCP server code. No changes are required to your tool implementations.","best-practices#Best Practices":"","plugin-development#Plugin Development":"Error Isolation: Always handle errors gracefully\nPerformance: Minimize blocking operations in beforeToolCall\nLogging: Use structured logging with context\nTesting: Write comprehensive tests for plugin logic","production-deployment#Production Deployment":"Environment Variables: Use environment-based configuration\nDatabase: Use PostgreSQL for production data storage\nMonitoring: Implement health checks and alerting\nSecurity: Follow security best practices for API keys","performance-optimization#Performance Optimization":"Plugin Priorities: Order plugins by execution cost\nCaching: Implement caching for expensive operations\nConnection Pooling: Use connection pooling for databases\nRate Limiting: Implement appropriate rate limiting\nReady to build? This API reference covers everything you need to integrate the MCP Proxy Wrapper into your applications."}},"/architecture":{"title":"Architecture","data":{"":"Technical architecture, design patterns, and internal mechanisms of the MCP Proxy Wrapper.","system-architecture-overview#System Architecture Overview":"The system consists of four distinct layers that work together to provide enhanced MCP functionality. Each layer has specific responsibilities and can be modified independently.\nThe architecture is designed with clear separation of concerns:\nClient Layer: Various MCP clients that consume tools\nTransport Layer: Communication protocols (STDIO, WebSocket, SSE, HTTP)\nProxy Wrapper Layer: Interception and plugin coordination\nMCP Server Layer: Your original, unmodified server and tools","plugin-execution-swimlane#Plugin Execution Swimlane":"The following swimlane diagram shows the detailed step-by-step process when a tool call is made. This illustrates how plugins interact with each other and with external systems like databases during request processing.","core-components#Core Components":"","1-proxy-wrapper-core#1. Proxy Wrapper Core":"The central orchestration component that:\nTool Interception: Replaces server.tool() method with enhanced version\nPlugin Coordination: Manages plugin lifecycle and execution order\nContext Management: Creates and maintains request context across plugins\nTransport Abstraction: Works with all MCP transport protocols\nError Isolation: Prevents plugin failures from breaking tool execution\nclass ProxyWrapper {\n  // Main wrapping method\n  static async wrapWithProxy(\n    server: McpServer, \n    options: ProxyWrapperOptions\n  ): Promise<WrappedServer>\n  \n  // Plugin management\n  private registerPlugin(plugin: ProxyPlugin): void\n  private initializePlugins(): Promise<void>\n  \n  // Tool interception\n  private enhanceToolMethod(originalMethod: Function): Function\n  private createEnhancedHandler(originalHandler: Function): Function\n  \n  // Execution coordination\n  private executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void>\n  private executeAfterHooks(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>\n}\nPlugin Error Isolation: Plugin failures don't break tool execution\nTimeout Management: Configurable timeouts for plugin execution\nGraceful Degradation: Core functionality continues even if plugins fail\nDetailed Logging: Comprehensive error reporting and debugging information\nRecovery Mechanisms: Automatic retry and fallback strategies","2-plugin-manager#2. Plugin Manager":"Handles plugin lifecycle, priority ordering, and execution:\ninterface PluginManager {\n  // Plugin registration and lifecycle\n  register(plugin: ProxyPlugin, config?: PluginConfig): Promise<void>\n  initialize(): Promise<void>\n  destroy(): Promise<void>\n  \n  // Execution methods\n  executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void>\n  executeAfterHooks(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>\n  \n  // Management methods\n  getExecutionOrder(): ProxyPlugin[]\n  validateDependencies(): Promise<boolean>\n  healthCheck(): Promise<Map<string, boolean>>\n}","3-plugin-architecture#3. Plugin Architecture":"Plugins follow a standard interface pattern:\ninterface ProxyPlugin {\n  // Identity\n  readonly name: string\n  readonly version: string\n  readonly metadata?: PluginMetadata\n  \n  // Configuration\n  config?: PluginConfig\n  \n  // Lifecycle hooks\n  initialize?(context: PluginInitContext): Promise<void>\n  beforeToolCall?(context: PluginContext): Promise<void | ToolCallResult>\n  afterToolCall?(context: PluginContext, result: ToolCallResult): Promise<ToolCallResult>\n  onError?(error: PluginError): Promise<void | ToolCallResult>\n  destroy?(): Promise<void>\n  \n  // Health and stats\n  healthCheck?(): Promise<boolean>\n  getStats?(): Promise<PluginStats>\n}","design-patterns#Design Patterns":"","1-decorator-pattern#1. Decorator Pattern":"The proxy wrapper uses the Decorator Pattern to enhance MCP servers:\n// Original server\nconst server = new McpServer({ name: 'my-server', version: '1.0.0' });\n// Decorated server with enhanced capabilities\nconst decoratedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});","2-chain-of-responsibility#2. Chain of Responsibility":"Plugins execute in a Chain of Responsibility pattern:\nclass PluginChain {\n  async executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void> {\n    for (const plugin of this.sortedPlugins) {\n      const result = await plugin.beforeToolCall?.(context);\n      if (result) {\n        return result; // Chain terminated early\n      }\n    }\n    // Chain completed successfully\n  }\n}","3-strategy-pattern#3. Strategy Pattern":"Different rate limiting strategies use the Strategy Pattern:\ninterface RateLimitStrategy {\n  checkLimit(userId: string, toolName: string): Promise<boolean>\n  updateUsage(userId: string, toolName: string): Promise<void>\n}\nclass FixedWindowLimiting implements RateLimitStrategy { /* ... */ }\nclass SlidingWindowLimiting implements RateLimitStrategy { /* ... */ }\nclass TokenBucketLimiting implements RateLimitStrategy { /* ... */ }","4-observer-pattern#4. Observer Pattern":"Analytics and monitoring use the Observer Pattern:\nclass EventEmitter {\n  emit(event: string, data: any): void\n  \n  // Plugin events\n  'tool:before': (context: ToolCallContext) => void\n  'tool:after': (context: ToolCallContext, result: ToolCallResult) => void\n  'plugin:error': (error: PluginError) => void\n  'rateLimit:exceeded': (limitInfo: RateLimitInfo) => void\n}","data-flow-architecture#Data Flow Architecture":"","request-context-flow#Request Context Flow":"","plugin-data-sharing#Plugin Data Sharing":"Plugins can share data through the context:\ninterface PluginContext extends ToolCallContext {\n  pluginData: Map<string, any>        // Shared plugin data\n  previousResults?: Map<string, any>  // Results from previous plugins\n}\n// Memory plugin sets conversation data\ncontext.pluginData.set('memory:sessionId', 'session_123');\n// Summarization plugin reads conversation context\nconst sessionId = context.pluginData.get('memory:sessionId');\nconst shouldSummarize = context.toolName === 'research' && content.length > 500;","performance-architecture#Performance Architecture":"","optimization-strategies#Optimization Strategies":"The proxy wrapper is designed for minimal overhead with several optimization strategies:\nLazy Loading: Plugins only initialize when first used\nAsync Execution: Non-blocking plugin execution with Promise.all where possible\nPriority Ordering: Critical plugins (auth) run first to fail fast\nResult Caching: Plugin results cached to avoid repeated expensive operations\nMemory Pooling: Context objects reused to reduce garbage collection","scalability-considerations#Scalability Considerations":"Stateless Design: Plugins maintain no server-side state\nDatabase Connections: Connection pooling for high-traffic scenarios\nCaching Layers: Redis/Memcached support for distributed caching\nLoad Balancing: Multiple proxy wrapper instances can run in parallel\nHorizontal Scaling: Database-backed plugins support clustering","security-architecture#Security Architecture":"","security-layers#Security Layers":"Memory Layer: Store and retrieve conversation history\nAI Enhancement Layer: Provide summarization and processing\nInput Validation: Sanitize and validate all inputs\nContext Management: Maintain session and user context\nResponse Processing: Transform and enhance tool outputs\nData Storage: Secure storage of conversation data","threat-model#Threat Model":"The proxy wrapper provides:\nData Privacy: Secure storage and handling of conversation data\nMemory Management: Controlled storage with cleanup and limits\nAI Safety: Secure integration with external AI providers\nContent Filtering: Validation of AI-generated content\nError Isolation: Plugin failures don't break core functionality\nResource Management: Memory and storage limits\nSecurity Best Practices: Always use HTTPS in production, rotate API keys regularly, and implement proper access controls.","extension-points#Extension Points":"The architecture provides several extension points for customization:\nCustom Plugins: Implement the ProxyPlugin interface\nCustom Transports: Extend transport layer compatibility\nCustom Authentication: Implement AuthenticationProvider interface\nCustom Analytics: Implement custom analytics tracking\nCustom Storage: Implement DatabaseAdapter interface\nNext: Explore the Plugin System and available plugins."}},"/deployment":{"title":"Deployment","data":{"":"Deploy your MCP Proxy Wrapper applications to production environments.","production-checklist#Production Checklist":"Before deploying to production, ensure you've completed these essential items.","security#Security":"âœ… Secure API keys and authentication tokens\nâœ… Secure environment variables (never commit secrets)\nâœ… Use HTTPS for all endpoints\nâœ… Enable proper input validation","configuration#Configuration":"âœ… Set NODE_ENV=production\nâœ… Configure proper logging level\nâœ… Set up database connection (if using persistent storage)\nâœ… Test authentication and rate limiting","environment-variables#Environment Variables":"","required-environment-variables#Required Environment Variables":"# Application\nNODE_ENV=production\nPORT=3000\nLOG_LEVEL=info\n# Authentication\nAUTH_ENDPOINT=https://your-auth-service.com/validate\nAPI_KEY_CACHE_TTL=3600\n# Rate Limiting\nRATE_LIMIT_WINDOW_MS=60000\nRATE_LIMIT_MAX_REQUESTS=100\n# Optional: Database (if using persistent storage)\nDATABASE_URL=postgresql://user:password@host:5432/database\n# Optional: Security\nJWT_SECRET=your_secure_jwt_secret\nImportant: Never commit these values to your repository. Use your deployment platform's secret management.","basic-docker-deployment#Basic Docker Deployment":"","simple-dockerfile#Simple Dockerfile":"FROM node:18-alpine\nWORKDIR /app\n# Copy package files\nCOPY package*.json ./\nRUN npm ci --only=production\n# Copy built application\nCOPY dist/ ./dist/\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]","docker-compose#Docker Compose":"version: '3.8'\nservices:\n  mcp-server:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n      - AUTH_ENDPOINT=${AUTH_ENDPOINT}\n      - RATE_LIMIT_WINDOW_MS=60000\n      - RATE_LIMIT_MAX_REQUESTS=100\n    restart: unless-stopped","platform-deployments#Platform Deployments":"","railway#Railway":"","connect-repository#Connect Repository":"Connect your GitHub repository to Railway.","set-environment-variables#Set Environment Variables":"Add your production environment variables in the Railway dashboard.","deploy#Deploy":"Railway will automatically build and deploy your application.\n# Railway CLI deployment\nnpm install -g @railway/cli\nrailway login\nrailway init\nrailway up","heroku#Heroku":"","install-heroku-cli#Install Heroku CLI":"Download and install the Heroku CLI.","create-application#Create Application":"heroku create your-app-name","set-environment-variables-1#Set Environment Variables":"heroku config:set NODE_ENV=production\nheroku config:set AUTH_ENDPOINT=https://your-auth-service.com/validate\nheroku config:set RATE_LIMIT_WINDOW_MS=60000\nheroku config:set RATE_LIMIT_MAX_REQUESTS=100","deploy-1#Deploy":"git push heroku main","vercel#Vercel":"Create a vercel.json file:\n{\n  \"version\": 2,\n  \"builds\": [\n    {\n      \"src\": \"dist/index.js\",\n      \"use\": \"@vercel/node\"\n    }\n  ],\n  \"routes\": [\n    {\n      \"src\": \"/(.*)\",\n      \"dest\": \"/dist/index.js\"\n    }\n  ]\n}\nDeploy:\nnpm install -g vercel\nvercel --prod","digitalocean-app-platform#DigitalOcean App Platform":"Create an app spec file:\nname: mcp-proxy-wrapper\nservices:\n- name: api\n  source_dir: /\n  github:\n    repo: your-username/your-repo\n    branch: main\n  run_command: node dist/index.js\n  environment_slug: node-js\n  instance_count: 1\n  instance_size_slug: basic-xxs\n  envs:\n  - key: NODE_ENV\n    value: production\n  - key: AUTH_ENDPOINT\n    value: https://your-auth-service.com/validate\n    type: SECRET\n  - key: RATE_LIMIT_WINDOW_MS\n    value: \"60000\"","health-checks#Health Checks":"","basic-health-endpoint#Basic Health Endpoint":"// Add to your server\nimport express from 'express';\nconst app = express();\napp.get('/health', (req, res) => {\n  res.status(200).json({ \n    status: 'healthy',\n    timestamp: new Date().toISOString()\n  });\n});\napp.get('/ready', async (req, res) => {\n  // Check dependencies (database, external APIs, etc.)\n  try {\n    // Add your health checks here\n    res.status(200).json({ status: 'ready' });\n  } catch (error) {\n    res.status(503).json({ \n      status: 'not ready',\n      error: error.message \n    });\n  }\n});","api-endpoint-configuration#API Endpoint Configuration":"","production-api-setup#Production API Setup":"","configure-authentication-endpoint#Configure Authentication Endpoint":"Set up your authentication service endpoint for API key validation.","set-rate-limiting#Set Rate Limiting":"Configure appropriate rate limits for your production environment:\nFree tier: 10 requests/minute\nPremium tier: 100 requests/minute\nEnterprise tier: 1000 requests/minute","database-setup#Database Setup":"If using persistent storage for analytics or caching, ensure your database is properly configured.","health-checks-1#Health Checks":"Implement health check endpoints to monitor your service status.","monitoring-and-logging#Monitoring and Logging":"","basic-logging#Basic Logging":"// Production logging configuration\nconst LOG_LEVEL = process.env.LOG_LEVEL || 'info';\n// Simple logging helper\nconst log = {\n  info: (message: string, data?: any) => {\n    if (['debug', 'info'].includes(LOG_LEVEL)) {\n      console.log(`[${new Date().toISOString()}] INFO: ${message}`, data || '');\n    }\n  },\n  error: (message: string, error?: any) => {\n    if (['debug', 'info', 'warn', 'error'].includes(LOG_LEVEL)) {\n      console.error(`[${new Date().toISOString()}] ERROR: ${message}`, error || '');\n    }\n  }\n};\n// Log important events\nlog.info('Server started', { port: process.env.PORT });\nlog.info('Plugin loaded', { plugin: 'llm-summarization' });","error-tracking#Error Tracking":"// Basic error handling\nprocess.on('uncaughtException', (error) => {\n  console.error(`[${new Date().toISOString()}] ERROR: Uncaught exception:`, error);\n  process.exit(1);\n});\nprocess.on('unhandledRejection', (reason, promise) => {\n  console.error(`[${new Date().toISOString()}] ERROR: Unhandled rejection at:`, promise, 'reason:', reason);\n  process.exit(1);\n});","plugin-configuration-examples#Plugin Configuration Examples":"","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3\n  }\n});","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 1000,\n    enableChat: true,\n    persistToDisk: process.env.NODE_ENV === 'production'\n  }\n});","backup-strategy#Backup Strategy":"","database-backups#Database Backups":"# PostgreSQL backup\npg_dump $DATABASE_URL > backup-$(date +%Y%m%d).sql\n# SQLite backup\ncp ./data/production.db ./backups/backup-$(date +%Y%m%d).db","environment-variables-backup#Environment Variables Backup":"Keep a secure record of your environment variables configuration (without the actual secrets).","troubleshooting#Troubleshooting":"","common-issues#Common Issues":"Environment variables not loading:\n# Check if variables are set\necho $AUTH_ENDPOINT\n# Should not be empty in production\nAuthentication not working:\n# Test auth endpoint\ncurl -X POST https://your-domain.com/api/test \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -d '{\"test\": \"auth\"}'\nApplication not starting:\n# Check logs\ndocker logs your-container-name\n# or\nheroku logs --tail --app your-app-name\nDatabase connection issues:\n# Test database connection\nnode -e \"console.log(require('pg').parse(process.env.DATABASE_URL))\"","debug-mode#Debug Mode":"Enable debug logging temporarily:\n# Set LOG_LEVEL to debug\nLOG_LEVEL=debug node dist/index.js\nReady for production: Your MCP Proxy Wrapper application is now deployed and ready to handle real users with enhanced plugin functionality.","security-best-practices#Security Best Practices":"Never commit secrets to your repository\nUse HTTPS for all API endpoints\nValidate all inputs and sanitize data\nRotate API keys regularly\nMonitor for unusual activity and implement alerts\nKeep dependencies updated with npm audit\nImplement proper rate limiting to prevent abuse\nUse secure authentication mechanisms","next-steps#Next Steps":"Getting Started: Review setup guide\nExamples: See real-world implementations\nAPI Reference: Complete API documentation\nPlugins: Explore available plugins\nNeed help with deployment? Check the troubleshooting section above or open an issue on GitHub."}},"/how-it-works":{"title":"How It Works","data":{"":"The MCP Proxy Wrapper operates as an interception layer between MCP clients and your server, allowing plugins to enhance functionality without modifying your original code.","core-mechanism#Core Mechanism":"The proxy wrapper works by intercepting the server.tool() method during server initialization and injecting plugin hooks around the original tool handlers.","high-level-flow-diagram#High-Level Flow Diagram":"The following diagram illustrates how a tool call flows through the proxy wrapper system. When a client makes a request, it first goes through the proxy wrapper, which coordinates with various plugins before reaching your original MCP server.\nIn this flow:\nThe MCP client sends a tool call request\nThe proxy wrapper receives the request and forwards it to the plugin manager\nEach plugin (memory, summarization) is consulted in priority order\nThe original tool executes\nThe result flows back through plugins for enhancement (memory storage, AI summarization) before returning to the client","plugin-execution-flow#Plugin Execution Flow":"The proxy wrapper executes plugins in a priority-ordered sequence with two main phases:","phase-1-beforetoolcall#Phase 1: beforeToolCall":"Memory Plugin: Check if response should be saved\nLLM Summarization: Check if response should be summarized\nRequest Processing: Prepare context for tool execution","phase-2-aftertoolcall#Phase 2: afterToolCall":"Memory Plugin: Save tool responses to memory database\nLLM Summarization: Generate AI summaries of long responses\nResponse Enhancement: Add metadata about processing\nShort-Circuit Capability: Any beforeToolCall hook can return a result to immediately respond without executing the original tool.","tool-interception-process#Tool Interception Process":"The proxy wrapper modifies your MCP server through a three-step process:","server-wrapping#Server Wrapping":"When you call wrapWithProxy(server, options), the wrapper:\nStores a reference to the original server.tool() method\nReplaces it with an enhanced version that includes plugin hooks\nInitializes all registered plugins in priority order","tool-registration-enhancement#Tool Registration Enhancement":"When you call proxiedServer.tool(name, schema, handler):\nThe original tool schema and handler are preserved\nA new enhanced handler is created that wraps the original\nPlugin hooks are injected before and after the original handler","runtime-execution#Runtime Execution":"When a tool call arrives:\nContext is created with tool name, arguments, and metadata\nbeforeToolCall hooks execute in priority order (highest first)\nIf no hook short-circuits, the original tool handler executes\nafterToolCall hooks execute in reverse priority order (lowest first)\nThe final result is returned to the client","tool-registration-behavior#Tool Registration Behavior":"Important: The proxy wrapper only enhances tools registered AFTER wrapping. Tools registered before wrapping remain available but don't get hook/plugin functionality.","what-gets-enhanced#What Gets Enhanced":"const server = new McpServer({ name: 'My Tools', version: '1.0.0' });\n// âŒ This tool won't have plugin functionality\nserver.tool('old-tool', { text: z.string() }, async (args) => {\n  return { content: [{ type: 'text', text: 'Old tool response' }] };\n});\n// Wrap the server\nconst proxiedServer = await wrapWithProxy(server, { plugins: [authPlugin] });\n// âœ… This tool will have full plugin functionality\nproxiedServer.tool('new-tool', { text: z.string() }, async (args) => {\n  return { content: [{ type: 'text', text: 'Enhanced tool response' }] };\n});\n// Both tools are available to clients, but only 'new-tool' gets:\n// - Authentication checks\n// - Summarization functionality\n// - Analytics tracking\n// - Custom hook execution","all-server-functionality-preserved#All Server Functionality Preserved":"The proxy wrapper preserves all existing MCP server functionality:\nExisting tools remain fully functional\nResource providers work unchanged\nPrompt templates are unaffected\nServer metadata and capabilities are preserved\nTransport layer (STDIO, WebSocket, etc.) works identically","code-example-behind-the-scenes#Code Example: Behind the Scenes":"Here's what happens when you wrap a server:\n// Original MCP server\nconst server = new McpServer({ name: 'My Tools', version: '1.0.0' });\n// This is what your code looks like\nserver.tool('analyze-text', { \n  text: z.string(),\n  userId: z.string().optional() \n}, async (args) => {\n  return { content: [{ type: 'text', text: `Analysis: ${args.text}` }] };\n});\n// This is what the proxy wrapper actually creates internally\nconst originalToolMethod = server.tool.bind(server);\nserver.tool = function(name: string, schema: any, handler: Function) {\n  \n  // Create enhanced handler with plugin hooks\n  const enhancedHandler = async (args: any) => {\n    const context = {\n      toolName: name,\n      args,\n      metadata: { requestId: generateId(), timestamp: Date.now() }\n    };\n    \n    // Execute beforeToolCall hooks (memory and summarization checks)\n    for (const plugin of sortedPlugins) {\n      const result = await plugin.beforeToolCall?.(context);\n      if (result) return result; // Short-circuit if plugin returns result\n    }\n    \n    // Execute original handler\n    const originalResult = await handler(args);\n    \n    // Execute afterToolCall hooks (save to memory, generate summaries)\n    let finalResult = originalResult;\n    for (const plugin of sortedPlugins.reverse()) {\n      finalResult = await plugin.afterToolCall?.(context, finalResult) || finalResult;\n    }\n    \n    return finalResult;\n  };\n  \n  // Register with original method using enhanced handler\n  return originalToolMethod(name, schema, enhancedHandler);\n};","transport-compatibility#Transport Compatibility":"The proxy wrapper works with all MCP transport methods because it operates at the tool handler level, not the transport level:\nSTDIO: Command-line MCP servers\nWebSocket: Real-time web applications\nSSE: Server-sent events for streaming\nHTTP: REST API style interactions\nInMemory: Testing and development","plugin-context-data#Plugin Context Data":"Each plugin receives rich context information:\ninterface ToolCallContext {\n  toolName: string;           // Name of the tool being called\n  args: Record<string, any>;  // Tool arguments from client\n  metadata: {\n    requestId: string;        // Unique request identifier\n    timestamp: number;        // Request timestamp\n    userId?: string;          // Authenticated user ID\n    sessionId?: string;       // Session identifier\n    transport: string;        // Transport method used\n  };\n}\nThis context flows through all plugin hooks, allowing for sophisticated cross-plugin coordination and data sharing.","error-handling#Error Handling":"The proxy wrapper includes robust error handling:\nPlugin Errors: Isolated and logged without breaking tool execution\nTool Errors: Proper MCP error responses with isError: true\nTransport Errors: Graceful degradation and retry logic\nTimeout Handling: Configurable timeouts for plugin execution\nPlugin Isolation: Plugin errors never break your original tool functionality. If a plugin fails, the tool call continues normally.","performance-considerations#Performance Considerations":"The proxy wrapper is designed for minimal overhead:\nLazy Loading: Plugins only load when needed\nAsync Execution: Non-blocking plugin execution\nCaching: Plugin results can be cached to avoid repeated operations\nPriority Ordering: Critical plugins (auth) run first, optional plugins (analytics) run last\nNext: Learn about the detailed Architecture and design patterns."}},"/":{"title":"ðŸš€ MCP Proxy Wrapper","data":{"":"Transform any MCP server into a powerful, extensible platform with enterprise-grade features\nnpm install mcp-proxy-wrapper","-what-is-mcp-proxy-wrapper#âœ¨ What is MCP Proxy Wrapper?":"The MCP Proxy Wrapper is a TypeScript library that wraps existing Model Context Protocol (MCP) servers to add advanced functionality through a sophisticated plugin system, all without requiring any changes to your existing MCP server code.\nZero-Modification Enhancement: Add smart features like LLM summarization, chat memory, and custom processing to any MCP server without touching the original code.","-core-concept#ðŸŽ¯ Core Concept":"The proxy wrapper intercepts tool calls between clients and your MCP server, allowing plugins to:\nðŸ” Authenticate and authorize users before tool execution\nðŸ’° Monitor and bill for tool usage in real-time\nâš¡ Transform requests and responses for enhanced functionality\nðŸ“Š Log and analyze usage patterns and performance\nðŸš„ Cache responses for improved performance\nðŸ›¡ï¸ Rate limit to prevent abuse","-quick-navigation#ðŸš€ Quick Navigation":"Get up and running in under 5 minutes with our step-by-step guide.\nUnderstand the proxy interception mechanism and plugin execution flow.\nDeep dive into the technical architecture and design patterns.\nExtend your MCP server with powerful plugins for LLM summarization, chat memory, and custom processing.","-key-features#â­ Key Features":"ðŸ”Œ Plugin Architecture\nExtensible hook system for beforeToolCall and afterToolCall with zero server modifications.\nðŸ¤– AI Enhancement Plugins\nLLM Summarization and Chat Memory plugins included for intelligent tool enhancement.\nðŸ” Authentication & Security\nFlexible hook system for implementing access control, rate limiting, and user management.\nðŸ“ˆ Analytics & Monitoring\nUsage tracking, performance metrics, error reporting, and real-time monitoring capabilities.\nðŸŒ Transport Agnostic\nWorks with STDIO, WebSocket, SSE, HTTP, and InMemory transport protocols.\nðŸ¢ Enterprise Ready\nRobust error handling, comprehensive logging, and production-grade features.","-how-the-proxy-wrapper-works-with-tools#ðŸ”§ How the Proxy Wrapper Works with Tools":"The proxy wrapper enhances your MCP server without breaking existing functionality - it's completely backward compatible!\nTools registered BEFORE wrapping: Remain fully available and functional, but don't get enhanced with hooks/plugins\nTools registered AFTER wrapping: Get full plugin functionality (summarization, memory, analytics, etc.)\nAll underlying server functionality: Completely preserved (resources, prompts, metadata, transport)\nThe proxy wrapper intercepts the server.tool() method registration process, not the tools themselves. So when you call wrapWithProxy(server), it overrides how new tools are registered to add the hook functionality, but existing tools continue to work exactly as before.\nðŸ“– This behavior is documented in detail in the Getting Started guide and How It Works section with examples showing the difference between enhanced and non-enhanced tools.","-quick-example#ðŸ’» Quick Example":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create your MCP server\nconst server = new McpServer({ name: 'My AI Tools', version: '1.0.0' });\n// Add AI enhancement plugin\nconst summarizationPlugin = new LLMSummarizationPlugin();\n// Wrap with proxy functionality\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summarizationPlugin]\n});\n// Register tools as usual - enhancement happens automatically\nproxiedServer.tool('ai-analysis', {\n  text: z.string()\n}, async (args) => {\n  return {\n    content: [{ type: 'text', text: `Analysis result: ${args.text}` }]\n  };\n});","-ready-to-transform-your-mcp-server#ðŸŽ¯ Ready to Transform Your MCP Server?":"Step-by-step setup in 5 minutes\nExplore powerful plugins and create your own\nMCP Proxy Wrapper - Enhance any MCP server without changing a single line of code"}},"/plugins":{"title":"Plugin System","data":{"":"The MCP Proxy Wrapper features a powerful plugin architecture that allows you to extend MCP servers with additional functionality like AI enhancement, analytics, security, and more.\nPlugins operate at the tool call level, intercepting requests before and after execution to add features without modifying your core tool logic.","how-plugins-work#How Plugins Work":"Plugins implement lifecycle hooks that are called during tool execution:\nexport interface ProxyPlugin {\n  name: string;\n  version: string;\n  \n  // Called before tool execution\n  beforeToolCall?(context: ToolCallContext): Promise<void | ToolCallResult>;\n  \n  // Called after tool execution  \n  afterToolCall?(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>;\n  \n  // Plugin lifecycle\n  initialize?(context: PluginContext): Promise<void>;\n  destroy?(): Promise<void>;\n}","available-plugins#Available Plugins":"Automatically generate AI summaries of tool responses using OpenAI or mock providers.\nSave tool responses to memory and provide chat interface for interacting with saved data.","core-plugins#Core Plugins":"The following plugins are included in the core library:","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3,\n    summarizeTools: ['search', 'research', 'analyze', 'fetch-data'],\n    minContentLength: 100,\n    saveOriginal: true\n  }\n});\n// The plugin intercepts tool results and returns AI-generated summaries\n// Original content is saved and can be retrieved later\n// Works with both OpenAI API and mock provider for testing","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    maxSessions: 100,\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours\n    excludeTools: ['chat-with-memory', 'get-memory-stats']\n  }\n});\n// The plugin saves all tool responses to memory\n// Provides chat interface to interact with saved data\n// Supports searching and querying conversation history","plugin-categories#Plugin Categories":"The plugin system supports several categories of functionality:","ai-enhancement-plugins#AI Enhancement Plugins":"LLM Summarization plugin provides AI-powered response summarization with configurable providers and models.","memory--storage-plugins#Memory & Storage Plugins":"Chat Memory plugin enables persistent storage of tool responses with intelligent search and chat interfaces.","extensible-architecture#Extensible Architecture":"The plugin system is designed to be extensible - you can create custom plugins by extending the BasePlugin class and implementing the required interfaces.","plugin-execution-flow#Plugin Execution Flow":"","plugin-priorities#Plugin Priorities":"Plugins execute in priority order (higher numbers first):\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, config: { priority: 20 } },      // Memory plugin (higher priority)\n    { plugin: summaryPlugin, config: { priority: 10 } }      // Summary plugin (lower priority)\n  ]\n});","quick-start#Quick Start":"","1-install-plugin-dependencies#1. Install Plugin Dependencies":"npm install mcp-proxy-wrapper","2-import-and-configure#2. Import and Configure":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\n// Configure enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' with API key for production\n    summarizeTools: ['search', 'analyze'],\n    minContentLength: 50\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 100\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin, memoryPlugin]\n});","3-register-tools#3. Register Tools":"// Your tools are now enhanced with plugin functionality\nproxiedServer.tool('research-analysis', {\n  topic: z.string(),\n  depth: z.enum(['basic', 'detailed']).default('basic'),\n  userId: z.string().optional()\n}, async (args) => {\n  // Plugins handle summarization and memory automatically\n  const research = await performResearch(args.topic, args.depth);\n  return {\n    content: [{ type: 'text', text: research }]\n  };\n});","error-handling#Error Handling":"Plugins include robust error handling to ensure tool calls aren't broken by plugin failures:\n// Plugin errors are isolated and logged\ntry {\n  await plugin.beforeToolCall(context);\n} catch (error) {\n  console.error('Plugin error:', error);\n  // Tool call continues normally\n}","plugin-configuration#Plugin Configuration":"","environment-based-configuration#Environment-based Configuration":"const summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150\n  },\n  enabled: true,\n  priority: 10\n});","dynamic-configuration#Dynamic Configuration":"const memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    maxEntries: process.env.NODE_ENV === 'production' ? 10000 : 100,\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours\n    enableChat: true,\n    saveResponses: true\n  },\n  enabled: true,\n  priority: 20\n});","testing-plugins#Testing Plugins":"The proxy wrapper includes testing utilities for plugin development:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\ndescribe('LLM Summarization Plugin', () => {\n  test('summarizes long content', async () => {\n    const plugin = new LLMSummarizationPlugin();\n    plugin.updateConfig({\n      options: {\n        provider: 'mock', // Use mock for testing\n        minContentLength: 50,\n        summarizeTools: ['test-tool']\n      }\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { plugins: [plugin] });\n    \n    const result = await proxiedServer.callTool('test-tool', { \n      text: 'This is a very long piece of content that should be summarized by the plugin because it exceeds the minimum length threshold.' \n    });\n    \n    expect(result.result._meta?.summarized).toBe(true);\n    expect(result.result.content[0].text).toContain('Summary:');\n  });\n});","best-practices#Best Practices":"","1-plugin-isolation#1. Plugin Isolation":"Keep plugins independent and focused on single responsibilities\nDon't rely on other plugins' state or behavior\nHandle errors gracefully without breaking tool calls","2-performance#2. Performance":"Minimize blocking operations in beforeToolCall\nUse async operations for external API calls\nImplement caching for expensive operations","3-configuration#3. Configuration":"Support environment-based configuration\nProvide sensible defaults\nValidate configuration on plugin initialization","4-logging#4. Logging":"Use structured logging with appropriate levels\nInclude context information (requestId, userId, etc.)\nDon't log sensitive information (API keys, personal data)","5-testing#5. Testing":"Write unit tests for plugin logic\nTest integration with the proxy wrapper\nInclude error scenarios and edge cases","community-plugins#Community Plugins":"The MCP community is building additional plugins. Community contributions are welcome for:\nAnalytics and monitoring solutions\nAuthentication and security plugins\nPerformance optimization tools\nIntegration plugins for popular services\nCustom business logic validators\nInterested in contributing? See our plugin development guide below.","creating-your-own-plugin#Creating Your Own Plugin":"Ready to build a custom plugin? Check out the plugin interface to get started:\nLearn about the core plugin interface and required methods for building custom plugins.\nView the source code for the included plugins to understand implementation patterns.\nReady to extend your MCP server? Choose from our official plugins or create your own to add exactly the functionality you need."}},"/examples":{"title":"Examples","data":{"":"Real-world implementations showing how to use the MCP Proxy Wrapper in different scenarios.","basic-ai-service#Basic AI Service":"A simple AI analysis service with logging and caching plugins:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create base server\nconst server = new McpServer({\n  name: 'ai-analysis-service',\n  version: '1.0.0'\n});\n// Configure enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' for production\n    summarizeTools: ['sentiment-analysis', 'text-summary'],\n    minContentLength: 50\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 100\n  }\n});\n// Wrap with proxy\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin, memoryPlugin]\n});\n// Register analysis tools\nproxiedServer.tool('sentiment-analysis', {\n  text: z.string().min(1, 'Text is required'),\n  language: z.string().optional()\n}, async (args) => {\n  const sentiment = await analyzeSentiment(args.text, args.language);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify({\n        sentiment: sentiment.label,\n        confidence: sentiment.confidence,\n        text: args.text\n      }, null, 2)\n    }]\n  };\n});\nproxiedServer.tool('text-summary', {\n  text: z.string().min(10, 'Text must be at least 10 characters'),\n  maxLength: z.number().optional().default(100)\n}, async (args) => {\n  const summary = await generateSummary(args.text, args.maxLength);\n  \n  return {\n    content: [{\n      type: 'text', \n      text: summary\n    }]\n  };\n});\n// Start server\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);","multi-tenant-saas-platform#Multi-Tenant SaaS Platform":"A complete SaaS platform with authentication and rate limiting:\n// Enhanced SaaS platform with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    summarizeTools: ['market-analysis', 'competitor-research'],\n    minContentLength: 500, // Longer threshold for business data\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 5000, // Higher limit for business use\n    sessionTimeout: 7 * 24 * 60 * 60 * 1000 // 1 week\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin] // Memory first, then summarization\n});\n// Business intelligence tools with AI enhancement\nproxiedServer.tool('market-analysis', {\n  company: z.string(),\n  metrics: z.array(z.string()),\n  timeframe: z.enum(['1M', '3M', '6M', '1Y']),\n  userId: z.string().optional()\n}, async (args) => {\n  const analysis = await performMarketAnalysis(args);\n  // Plugin automatically summarizes complex analysis data\n  return { content: [{ type: 'text', text: JSON.stringify(analysis, null, 2) }] };\n});\nproxiedServer.tool('competitor-research', {\n  industry: z.string(),\n  region: z.string().optional(),\n  userId: z.string().optional()\n}, async (args) => {\n  const research = await conductCompetitorResearch(args);\n  // Plugin saves research to memory for future reference\n  return { content: [{ type: 'text', text: JSON.stringify(research, null, 2) }] };\n});","gaming-platform-with-usage-tracking#Gaming Platform with Usage Tracking":"A gaming service with usage analytics and caching:\n// Gaming platform with memory and AI summarization\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 2000, // Store lots of game sessions\n    maxSessions: 500, // Support many concurrent players\n    excludeTools: [] // Save all gaming tools\n  }\n});\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['ai-dungeon-master'], // Summarize long narrative responses\n    minContentLength: 200,\n    saveOriginal: true\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Gaming tools with user tracking\nproxiedServer.tool('generate-character', {\n  class: z.enum(['warrior', 'mage', 'rogue', 'cleric']),\n  level: z.number().min(1).max(20),\n  background: z.string().optional(),\n  userId: z.string()\n}, async (args) => {\n  const character = await generateCharacter(args);\n  return { content: [{ type: 'text', text: JSON.stringify(character) }] };\n});\nproxiedServer.tool('ai-dungeon-master', {\n  scenario: z.string(),\n  playerAction: z.string(),\n  context: z.string().optional(),\n  userId: z.string()\n}, async (args) => {\n  const response = await generateDMResponse(args);\n  return { content: [{ type: 'text', text: response }] };\n});","development-tools-api#Development Tools API":"A developer-focused API with comprehensive logging and metadata:\n// Developer tools with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['code-review', 'security-scan'],\n    minContentLength: 300, // Code reviews can be long\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    sessionTimeout: 2 * 24 * 60 * 60 * 1000 // 2 days for dev work\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Development tools with AI enhancement\nproxiedServer.tool('code-review', {\n  code: z.string(),\n  language: z.string(),\n  focusAreas: z.array(z.enum(['security', 'performance', 'maintainability', 'style'])).optional(),\n  userId: z.string().optional()\n}, async (args) => {\n  const review = await performCodeReview(args);\n  // Plugin automatically summarizes detailed code review results\n  return { content: [{ type: 'text', text: JSON.stringify(review, null, 2) }] };\n});\nproxiedServer.tool('security-scan', {\n  code: z.string(),\n  language: z.string(),\n  scanType: z.enum(['static', 'dependency', 'comprehensive']).default('comprehensive'),\n  userId: z.string().optional()\n}, async (args) => {\n  const vulnerabilities = await scanForVulnerabilities(args);\n  // Plugin saves scan results to memory for future reference\n  return { content: [{ type: 'text', text: JSON.stringify(vulnerabilities, null, 2) }] };\n});","blockchain-analytics-server#Blockchain Analytics Server":"Enhancing a Web3 analytics server with AI summarization and memory capabilities:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create blockchain analytics server\nconst server = new McpServer({\n  name: 'web3-stats-server-enhanced',\n  version: '2.0.0'\n}, {\n  capabilities: { tools: {} }\n});\n// Configure AI enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY, // Set via environment\n    model: 'gpt-4o-mini',\n    summarizeTools: [\n      'get_evm_balances', \n      'get_evm_transactions',\n      'get_evm_collectibles',\n      'get_token_holders'\n    ],\n    minContentLength: 200, // Blockchain data can be verbose\n    saveOriginal: true\n  },\n  enabled: true,\n  priority: 10\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY, // Set via environment\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 2000, // Store lots of blockchain queries\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours for analysis sessions\n    excludeTools: ['ping_dune_server', 'ping_blockscout'] // Skip health checks\n  },\n  enabled: true,\n  priority: 20\n});\n// Wrap server with AI enhancement\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, config: { priority: 20 } },\n    { plugin: summaryPlugin, config: { priority: 10 } }\n  ],\n  debug: process.env.NODE_ENV !== 'production'\n});\n// Blockchain analysis tools (now AI-enhanced)\nproxiedServer.tool('get_evm_balances', 'Get wallet token balances with AI insights', {\n  walletAddress: z.string().describe('EVM wallet address (0x...)'),\n  chainId: z.string().optional().describe('Chain ID (1 for Ethereum)'),\n  limit: z.number().optional().describe('Max results to return')\n}, async ({ walletAddress, chainId = '1', limit = 50 }) => {\n  // Call external blockchain API (Dune, Alchemy, etc.)\n  const balances = await fetchWalletBalances(walletAddress, chainId, limit);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(balances, null, 2)\n    }]\n  };\n  // AI plugin will automatically:\n  // 1. Generate summary: \"This wallet holds $X.XX across Y tokens...\"\n  // 2. Save to memory for chat: \"Tell me about wallets with high ETH balances\"\n});\nproxiedServer.tool('get_evm_transactions', 'Get transaction history with AI analysis', {\n  walletAddress: z.string().describe('EVM wallet address'),\n  limit: z.number().optional().describe('Number of transactions'),\n  includeTokenTransfers: z.boolean().optional().describe('Include token transfers')\n}, async ({ walletAddress, limit = 25, includeTokenTransfers = true }) => {\n  const transactions = await fetchTransactionHistory(\n    walletAddress, \n    limit, \n    includeTokenTransfers\n  );\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(transactions, null, 2)\n    }]\n  };\n  // AI enhancement provides:\n  // - Summary of transaction patterns\n  // - Memory storage for behavioral analysis\n  // - Chat interface: \"What DeFi protocols does this wallet use?\"\n});\nproxiedServer.tool('get_token_holders', 'Analyze token distribution with insights', {\n  contractAddress: z.string().describe('Token contract address'),\n  chainId: z.string().optional().describe('Chain ID'),\n  limit: z.number().optional().describe('Number of holders to analyze')\n}, async ({ contractAddress, chainId = '1', limit = 100 }) => {\n  const holders = await analyzeTokenHolders(contractAddress, chainId, limit);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(holders, null, 2)\n    }]\n  };\n  // AI creates executive summaries of:\n  // - Holder concentration analysis\n  // - Whale identification\n  // - Distribution patterns\n});\n// Helper functions (implement with your preferred blockchain data provider)\nasync function fetchWalletBalances(address: string, chainId: string, limit: number) {\n  // Example using environment-based API configuration\n  const apiKey = process.env.BLOCKCHAIN_API_KEY; // Not committed to git!\n  const response = await fetch(`https://api.example.com/v1/balances/${address}?chain=${chainId}&limit=${limit}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\nasync function fetchTransactionHistory(address: string, limit: number, includeTokens: boolean) {\n  const apiKey = process.env.BLOCKCHAIN_API_KEY;\n  const response = await fetch(`https://api.example.com/v1/transactions/${address}?limit=${limit}&tokens=${includeTokens}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\nasync function analyzeTokenHolders(contract: string, chainId: string, limit: number) {\n  const apiKey = process.env.BLOCKCHAIN_API_KEY;\n  const response = await fetch(`https://api.example.com/v1/tokens/${contract}/holders?chain=${chainId}&limit=${limit}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\n// Start the enhanced blockchain server\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);\nconsole.log('Enhanced Web3 Analytics Server started with AI capabilities');","environment-configuration-for-blockchain-server#Environment Configuration for Blockchain Server":"# .env file (never commit this!)\nNODE_ENV=production\nBLOCKCHAIN_API_KEY=your_blockchain_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\n# Development\nNODE_ENV=development\n# Mock providers will be used automatically","docker-configuration-for-web3-server#Docker Configuration for Web3 Server":"# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\n# Never include API keys in Docker images!\n# Use environment variables or secrets management\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  web3-analytics:\n    build: .\n    environment:\n      - NODE_ENV=production\n      # Use Docker secrets or external config for API keys\n      - BLOCKCHAIN_API_KEY_FILE=/run/secrets/blockchain_api_key\n      - OPENAI_API_KEY_FILE=/run/secrets/openai_api_key\n    secrets:\n      - blockchain_api_key\n      - openai_api_key\n    ports:\n      - \"3000:3000\"\nsecrets:\n  blockchain_api_key:\n    external: true\n  openai_api_key:\n    external: true","testing-blockchain-ai-enhancement#Testing Blockchain AI Enhancement":"// tests/blockchain.test.ts\nimport { describe, test, expect } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\ndescribe('Blockchain AI Enhancement', () => {\n  test('enhances wallet balance analysis', async () => {\n    const server = new McpServer('test-web3-server', '1.0.0');\n    \n    // Mock blockchain tool\n    server.tool('get_evm_balances', {}, async () => ({\n      content: [{\n        type: 'text',\n        text: JSON.stringify({\n          address: '0x1234...5678',\n          totalValue: 150000.50,\n          tokens: [\n            { symbol: 'ETH', balance: '50.5', value: 100000 },\n            { symbol: 'USDC', balance: '50000', value: 50000 }\n          ]\n        })\n      }]\n    }));\n    \n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'mock',\n        summarizeTools: ['get_evm_balances'],\n        minContentLength: 50\n      }\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { \n      plugins: [summaryPlugin] \n    });\n    \n    const result = await proxiedServer.callTool('get_evm_balances', {\n      walletAddress: '0x1234567890123456789012345678901234567890'\n    });\n    \n    // Verify AI enhancement\n    expect(result._meta?.summarized).toBe(true);\n    expect(result.content[0].text).toContain('Summary:');\n    expect(result._meta?.originalStorageKey).toBeDefined();\n  });\n});","content-platform-with-usage-limits#Content Platform with Usage Limits":"A content creation platform with user tier management:\n// Content platform with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['advanced-article'], // Only summarize premium content\n    minContentLength: 500,\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 3000, // Store lots of content\n    sessionTimeout: 30 * 24 * 60 * 60 * 1000, // 30 days for content work\n    excludeTools: [] // Save all content creation\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Content creation tools with user tiers\nproxiedServer.tool('simple-blog-post', {\n  topic: z.string(),\n  tone: z.enum(['professional', 'casual', 'humorous']).default('professional'),\n  length: z.enum(['short', 'medium', 'long']).default('medium'),\n  userId: z.string()\n}, async (args) => {\n  const post = await generateBlogPost(args);\n  return { content: [{ type: 'text', text: post }] };\n});\nproxiedServer.tool('advanced-article', {  // Premium only\n  topic: z.string(),\n  sources: z.array(z.string()),\n  seoKeywords: z.array(z.string()),\n  targetAudience: z.string(),\n  userId: z.string()\n}, async (args) => {\n  const article = await generateAdvancedArticle(args);\n  return { content: [{ type: 'text', text: article }] };\n});","production-configuration-examples#Production Configuration Examples":"","environment-based-setup#Environment-Based Setup":"// config/index.ts\ninterface Config {\n  database: string;\n  logLevel: string;\n  rateLimits: Record<string, number>;\n  cacheSettings: {\n    ttl: number;\n    maxSize: number;\n  };\n}\nconst configs: Record<string, Config> = {\n  development: {\n    database: 'sqlite:./dev.db',\n    logLevel: 'debug',\n    rateLimits: {\n      'free': 10,\n      'premium': 1000\n    },\n    cacheSettings: {\n      ttl: 60000,  // 1 minute for testing\n      maxSize: 100\n    }\n  },\n  production: {\n    database: process.env.DATABASE_URL!,\n    logLevel: 'info',\n    rateLimits: {\n      'free': 100,\n      'premium': 10000,\n      'enterprise': 100000\n    },\n    cacheSettings: {\n      ttl: 300000,  // 5 minutes\n      maxSize: 10000\n    }\n  }\n};\nexport const config = configs[process.env.NODE_ENV || 'development'];","docker-deployment#Docker Deployment":"# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  mcp-server:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/mcpserver\n      - LOG_LEVEL=info\n      - CACHE_TTL=300000\n    depends_on:\n      - db\n  \n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mcpserver\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\nvolumes:\n  postgres_data:","testing-examples#Testing Examples":"","integration-testing#Integration Testing":"// tests/integration.test.ts\nimport { describe, test, expect, beforeEach } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\ndescribe('Plugin Integration Tests', () => {\n  let proxiedServer: any;\n  \n  beforeEach(async () => {\n    // Create test server with sample tool\n    const server = new McpServer('test-server', '1.0.0');\n    \n    server.tool('test-tool', {\n      text: z.string()\n    }, async (args) => {\n      return {\n        content: [{ \n          type: 'text', \n          text: `This is a long response that should be summarized because it exceeds the minimum length: ${args.text}` \n        }]\n      };\n    });\n    \n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'mock', // Use mock for testing\n        minContentLength: 50,\n        summarizeTools: ['test-tool']\n      }\n    });\n    \n    const memoryPlugin = new ChatMemoryPlugin();\n    memoryPlugin.updateConfig({\n      options: {\n        saveResponses: true,\n        maxEntries: 10\n      }\n    });\n    \n    proxiedServer = await wrapWithProxy(server, { \n      plugins: [memoryPlugin, summaryPlugin] \n    });\n  });\n  test('summarizes long responses', async () => {\n    const result = await proxiedServer.callTool('test-tool', {\n      text: 'This is a long response that should be summarized by the plugin because it exceeds the minimum length threshold for summarization.'\n    });\n    expect(result.result._meta?.summarized).toBe(true);\n    expect(result.result.content[0].text).toContain('Summary:');\n  });\n  test('saves responses to memory', async () => {\n    await proxiedServer.callTool('test-tool', { text: 'Test content', userId: 'user123' });\n    \n    const memoryPlugin = proxiedServer.plugins.find(p => p.name === 'chat-memory-plugin');\n    const history = memoryPlugin.getConversationHistory('user123', 10);\n    expect(history.length).toBe(1);\n    expect(history[0].response.content).toContain('Test content');\n  });\n});","load-testing#Load Testing":"// tests/load.test.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\nasync function loadTest() {\n  // Create test server\n  const server = new McpServer('load-test-server', '1.0.0');\n  \n  server.tool('test-tool', {\n    data: z.string()\n  }, async (args) => {\n    return {\n      content: [{ type: 'text', text: `Processed: ${args.data}` }]\n    };\n  });\n  \n  const proxiedServer = await wrapWithProxy(server, { plugins: [] });\n  \n  const promises = [];\n  const startTime = Date.now();\n  \n  // Simulate 100 concurrent calls\n  for (let i = 0; i < 100; i++) {\n    promises.push(proxiedServer.callTool('test-tool', { data: `test-${i}` }));\n  }\n  \n  await Promise.all(promises);\n  const duration = Date.now() - startTime;\n  \n  console.log(`Processed 100 calls in ${duration}ms`);\n  console.log(`Average: ${duration / 100}ms per call`);\n}","common-use-cases#Common Use Cases":"Monetize AI analysis, generation, and processing tools\nWeb3 data analysis with AI summaries and memory\nCode review, security scanning, and development tools\nWriting, design, and creative tools with freemium models\nMulti-tenant platforms with enhanced functionality\nReady to implement? These examples show real production patterns that you can adapt for your specific use case.","next-steps#Next Steps":"API Reference: Complete API documentation\nDeployment: Production deployment guide\nPlugins: Explore available plugins\nGetting Started: Basic setup guide"}},"/getting-started":{"title":"Getting Started","data":{"":"Transform any MCP server into a powerful, extensible platform with enterprise-grade features in minutes.\nThe MCP Proxy Wrapper requires Node.js 18+ and works with any existing MCP server without code changes.","installation#Installation":"","install-the-package#Install the Package":"npm install mcp-proxy-wrapper","basic-setup#Basic Setup":"Create a simple wrapper around your existing MCP server:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Your existing MCP server\nconst server = new McpServer({\n  name: 'my-server',\n  version: '1.0.0'\n});\n// Wrap with proxy functionality\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [] // Add plugins here\n});\n// Register tools with enhanced functionality\n// Tools registered after wrapping get hook/plugin functionality\nproxiedServer.tool('hello-world', {\n  name: z.string()\n}, async (args) => {\n  return {\n    content: [{\n      type: 'text',\n      text: `Hello, ${args.name}!`\n    }]\n  };\n});\n// Note: Any tools registered BEFORE wrapping remain available\n// but won't have hook/plugin functionality applied","start-the-server#Start the Server":"import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);","your-first-plugin#Your First Plugin":"Let's add the LLM Summarization plugin to enhance your tools:","configure-the-plugin#Configure the Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3,\n    summarizeTools: ['long-analysis'],\n    minContentLength: 100\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin]\n});","test-summarization#Test Summarization":"// This tool now has automatic summarization\nproxiedServer.tool('long-analysis', {\n  data: z.string()\n}, async (args) => {\n  const result = await performLongAnalysis(args.data);\n  // Plugin automatically summarizes long responses\n  return result;\n});","development-workflow#Development Workflow":"","environment-setup#Environment Setup":"Create a .env file for your configuration:\n# OpenAI API key for LLM plugins\nOPENAI_API_KEY=sk-your-openai-key-here\n# Optional: Logging level\nLOG_LEVEL=debug","project-structure#Project Structure":"my-mcp-server/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ index.ts          # Main server file\nâ”‚   â”œâ”€â”€ tools/            # Your tool implementations\nâ”‚   â””â”€â”€ config/           # Configuration\nâ”œâ”€â”€ package.json\nâ”œâ”€â”€ .env                  # Environment variables\nâ””â”€â”€ tsconfig.json","sample-server-implementation#Sample Server Implementation":"// src/index.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\nasync function main() {\n  // Create base server\n  const server = new McpServer({\n    name: 'my-ai-tools',\n    version: '1.0.0'\n  });\n  // Configure plugins\n  const plugins = [];\n  \n  if (process.env.OPENAI_API_KEY) {\n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'openai',\n        openaiApiKey: process.env.OPENAI_API_KEY,\n        model: 'gpt-4o-mini',\n        maxTokens: 150\n      }\n    });\n    plugins.push(summaryPlugin);\n    \n    const memoryPlugin = new ChatMemoryPlugin();\n    memoryPlugin.updateConfig({\n      options: {\n        saveResponses: true,\n        maxEntries: 100,\n        enableChat: true\n      }\n    });\n    plugins.push(memoryPlugin);\n  }\n  // Wrap with proxy\n  const proxiedServer = await wrapWithProxy(server, { plugins });\n  // Register tools\n  proxiedServer.tool('text-analysis', {\n    text: z.string(),\n    analysisType: z.enum(['sentiment', 'summary', 'keywords'])\n  }, async (args) => {\n    // Your AI analysis logic here\n    const result = await analyzeText(args.text, args.analysisType);\n    \n    return {\n      content: [{\n        type: 'text',\n        text: JSON.stringify(result, null, 2)\n      }]\n    };\n  });\n  // Start server\n  const transport = new StdioServerTransport();\n  await proxiedServer.connect(transport);\n}\nmain().catch(console.error);","testing-your-server#Testing Your Server":"","manual-testing-with-mcp-inspector#Manual Testing with MCP Inspector":"# Install MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n# Test your server\nmcp-inspector node dist/index.js","automated-testing#Automated Testing":"// tests/server.test.ts\nimport { describe, test, expect } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\ndescribe('My MCP Server', () => {\n  test('tool returns expected result', async () => {\n    // Create test server\n    const server = new McpServer('test-server', '1.0.0');\n    \n    // Register test tool\n    server.tool('text-analysis', {\n      text: z.string(),\n      analysisType: z.enum(['sentiment', 'readability'])\n    }, async (args) => {\n      return {\n        content: [{ type: 'text', text: `Analysis result: ${args.analysisType} is positive` }]\n      };\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { plugins: [] });\n    \n    const result = await proxiedServer.callTool('text-analysis', {\n      text: 'This is great!',\n      analysisType: 'sentiment'\n    });\n    \n    expect(result.content[0].text).toContain('positive');\n  });\n});","transport-options#Transport Options":"The proxy wrapper supports all MCP transport methods:\n// STDIO (most common for CLI tools)\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);\n// WebSocket (for web applications)\nimport { WebSocketTransport } from '@modelcontextprotocol/sdk/server/websocket.js';\nconst transport = new WebSocketTransport({ port: 3000 });\nawait proxiedServer.connect(transport);\n// HTTP with SSE (for REST APIs)\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\nconst transport = new SSEServerTransport('/mcp', (request, response) => {\n  // Handle HTTP requests\n});\nawait proxiedServer.connect(transport);\n// InMemory (for testing)\nimport { InMemoryTransport } from '@modelcontextprotocol/sdk/inMemory.js';\nconst { client, server: transport } = InMemoryTransport.create();\nawait proxiedServer.connect(transport);","common-patterns#Common Patterns":"","environment-based-configuration#Environment-Based Configuration":"const config = {\n  development: {\n    logLevel: 'debug',\n    plugins: []\n  },\n  production: {\n    logLevel: 'info', \n    plugins: [\n      (() => {\n        const plugin = new LLMSummarizationPlugin();\n        plugin.updateConfig({\n          options: {\n            provider: 'openai',\n            openaiApiKey: process.env.OPENAI_API_KEY!, // Set via environment\n            model: 'gpt-4o-mini'\n          }\n        });\n        return plugin;\n      })()\n    ]\n  }\n};\nconst currentConfig = config[process.env.NODE_ENV || 'development'];","security-best-practices#Security Best Practices":"API Key Security: Never commit API keys to version control. Always use environment variables or secure secrets management.","environment-variables#Environment Variables":"Create a .env file for local development (never commit this file):\n# .env (add to .gitignore!)\nNODE_ENV=development\nOPENAI_API_KEY=sk-your-openai-key-here\nBLOCKCHAIN_API_KEY=your-blockchain-api-key-here\nDATABASE_URL=postgresql://user:pass@localhost:5432/myapp","git-security#Git Security":"Ensure your .gitignore includes:\n# Environment files\n.env\n.env.local\n.env.production\n.env.*.local\n# API keys and secrets\n**/config/secrets.json\n**/config/*.key\n*.pem\n# Build artifacts with embedded secrets\ndist/\nbuild/","production-deployment#Production Deployment":"Use secure environment variable injection:\n# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\n# Never COPY .env files into Docker images!\n# Use runtime environment variables instead\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml or Kubernetes manifests\nversion: '3.8'\nservices:\n  mcp-server:\n    build: .\n    environment:\n      - NODE_ENV=production\n      # Reference external secrets, never inline API keys\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BLOCKCHAIN_API_KEY=${BLOCKCHAIN_API_KEY}\n    # Use Docker secrets or external secret management\n    secrets:\n      - openai_api_key\n      - blockchain_api_key","validation-and-sanitization#Validation and Sanitization":"// Always validate sensitive inputs\nproxiedServer.tool('api-call', {\n  apiKey: z.string().min(20).max(200), // Validate API key format\n  endpoint: z.string().url(), // Ensure valid URLs only\n  data: z.object({}).passthrough() // Validate data structure\n}, async ({ apiKey, endpoint, data }) => {\n  // Additional validation\n  if (!endpoint.startsWith('https://')) {\n    throw new Error('Only HTTPS endpoints allowed');\n  }\n  \n  // Use the validated inputs safely\n  return await makeSecureApiCall(endpoint, data, apiKey);\n});","error-handling#Error Handling":"proxiedServer.tool('risky-operation', schema, async (args) => {\n  try {\n    return await performRiskyOperation(args);\n  } catch (error) {\n    // Plugin errors are handled automatically\n    // Tool errors should return MCP error format\n    return {\n      content: [{\n        type: 'text',\n        text: 'Operation failed'\n      }],\n      isError: true\n    };\n  }\n});","multiple-plugins#Multiple Plugins":"const proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, priority: 20 },      // Memory first (higher priority)\n    { plugin: summaryPlugin, priority: 10 }      // Then summarization (lower priority)\n  ]\n});","next-steps#Next Steps":"Your server is now enhanced with plugin capabilities! Explore our other guides to add more functionality.\nHow It Works: Understand the proxy wrapper architecture\nPlugins: Add summarization, memory, and more\nExamples: See real-world implementations\nAPI Reference: Complete API documentation\nDeployment: Deploy to production","troubleshooting#Troubleshooting":"","common-issues#Common Issues":"Plugin not loading:\n# Check your environment variables\necho $OPENAI_API_KEY\n# Verify plugin configuration\nnpm run test\nTool calls failing:\n// Add debug logging\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [plugin],\n  debug: true\n});\nTypeScript errors:\n# Ensure you have the latest types\nnpm install --save-dev @types/node\nNeed more help? Check our troubleshooting guide or open an issue on GitHub."}}}
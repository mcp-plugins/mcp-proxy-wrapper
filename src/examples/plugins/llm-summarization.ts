/**
 * @file LLM Summarization Plugin
 * @description A working plugin that intercepts tool results, saves them, and returns AI-generated summaries
 */

import { BasePlugin, PluginContext, PluginConfig } from '../../interfaces/plugin.js';
import { ToolCallResult } from '../../interfaces/proxy-hooks.js';

/**
 * Stored result interface for saving original data
 */
interface StoredResult {
  originalResult: ToolCallResult;
  context: Omit<PluginContext, 'pluginData'>;
  timestamp: number;
  toolName: string;
  requestId: string;
  metadata?: Record<string, any>;
}

/**
 * LLM API interface for testing/mocking
 */
interface LLMProvider {
  generateSummary(content: string, prompt: string, options?: any): Promise<string>;
}

/**
 * Mock LLM provider for testing (simulates OpenAI)
 */
class MockLLMProvider implements LLMProvider {
  private delay: number;
  
  constructor(delay = 100) {
    this.delay = delay;
  }
  
  async generateSummary(content: string, prompt: string): Promise<string> {
    // Simulate API delay
    await new Promise(resolve => setTimeout(resolve, this.delay));
    
    // Simple mock summarization (first sentence + length info)
    const firstSentence = content.split('.')[0] + '.';
    const wordCount = content.split(' ').length;
    
    return `Summary: ${firstSentence} (Summarized from ${wordCount} words)`;
  }
}

/**
 * Real OpenAI provider (requires openai package)
 */
class OpenAIProvider implements LLMProvider {
  private apiKey: string;
  private model: string;
  private maxTokens: number;
  private temperature: number;
  private openai: any;
  
  constructor(apiKey: string, model = 'gpt-4o-mini', maxTokens = 150, temperature = 0.3) {
    this.apiKey = apiKey;
    this.model = model;
    this.maxTokens = maxTokens;
    this.temperature = temperature;
    
    if (!this.apiKey) {
      throw new Error('OpenAI API key not provided');
    }
    
    // Initialize OpenAI client lazily on first use
    this.openai = null;
  }
  
  private async initializeOpenAI() {
    try {
      const { OpenAI } = await import('openai');
      this.openai = new OpenAI({
        apiKey: this.apiKey,
      });
    } catch (error) {
      // Fallback to mock if OpenAI not available in test environments
      console.warn('OpenAI package not available, falling back to mock implementation');
      this.openai = null;
    }
  }
  
  async generateSummary(content: string, prompt: string): Promise<string> {
    // Initialize OpenAI client lazily on first use
    if (!this.openai) {
      await this.initializeOpenAI();
    }
    
    // If OpenAI is still not initialized (fallback), use mock
    if (!this.openai) {
      const words = content.split(' ').length;
      return `AI Summary: Key insights from ${words} words of content. Generated by ${this.model} (mock).`;
    }
    
    try {
      const completion = await this.openai.chat.completions.create({
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'You are a helpful assistant that creates concise, accurate summaries. Focus on key insights, main findings, and actionable information.'
          },
          {
            role: 'user',
            content: `${prompt}\n\nContent to summarize:\n${content}`
          }
        ],
        max_tokens: this.maxTokens,
        temperature: this.temperature,
      });

      const summary = completion.choices[0]?.message?.content?.trim();
      if (!summary) {
        throw new Error('No summary generated by OpenAI');
      }

      return summary;
    } catch (error) {
      // Log the error and fallback to mock
      console.error('OpenAI API error:', error);
      const words = content.split(' ').length;
      return `AI Summary: Key insights from ${words} words of content. (OpenAI API unavailable - fallback summary)`;
    }
  }
}

/**
 * LLM Summarization Plugin
 * Intercepts tool results and returns AI-generated summaries
 */
export class LLMSummarizationPlugin extends BasePlugin {
  name = 'llm-summarization-plugin';
  version = '1.0.0';
  
  metadata = {
    description: 'Intercepts tool results and returns AI-generated summaries',
    author: 'MCP Team',
    tags: ['ai', 'summarization', 'llm']
  };
  
  config: PluginConfig = {
    enabled: true,
    priority: 10, // Run after other plugins
    options: {
      provider: 'mock', // 'mock' or 'openai'
      openaiApiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-4o-mini',
      maxTokens: 150,
      temperature: 0.3,
      summarizeTools: ['search', 'research', 'analyze', 'fetch-data'],
      minContentLength: 100, // Lower threshold for testing
      saveOriginal: true,
      summarizationPrompt: 'Please provide a concise summary of the following content. Focus on key insights and main findings:',
      mockDelay: 100 // For testing
    }
  };
  
  private llmProvider!: LLMProvider;
  private storage: Map<string, StoredResult> = new Map();
  
  // Custom stats tracking for summarization
  private customStats = {
    totalSummarizations: 0,
    totalSavings: 0, // Characters saved
    averageCompressionRatio: 0,
    errorCount: 0
  };
  
  async initialize(context: any): Promise<void> {
    await super.initialize(context);
    
    // Initialize LLM provider based on configuration
    const provider = this.config.options?.provider || 'mock';
    
    if (provider === 'openai') {
      this.llmProvider = new OpenAIProvider(
        this.config.options?.openaiApiKey,
        this.config.options?.model,
        this.config.options?.maxTokens,
        this.config.options?.temperature
      );
    } else {
      this.llmProvider = new MockLLMProvider(this.config.options?.mockDelay);
    }
    
    this.logger?.info(`LLM Summarization plugin initialized with ${provider} provider`);
  }
  
  async afterToolCall(context: PluginContext, result: ToolCallResult): Promise<ToolCallResult> {
    // Check if this tool should be summarized
    if (!this.shouldSummarize(context, result)) {
      return result;
    }
    
    try {
      const originalContent = this.extractContent(result);
      const startTime = Date.now();
      
      // Save original result with metadata
      const storageKey = this.generateStorageKey(context);
      if (this.config.options?.saveOriginal) {
        await this.saveOriginalResult(storageKey, {
          originalResult: result,
          context: this.sanitizeContext(context),
          timestamp: Date.now(),
          toolName: context.toolName,
          requestId: context.requestId,
          metadata: {
            originalLength: originalContent.length,
            plugin: this.name
          }
        });
      }
      
      // Generate summary using LLM
      const summary = await this.generateSummary(originalContent, context);
      const processingTime = Date.now() - startTime;
      
      // Update stats
      this.updateCustomStats(originalContent.length, summary.length);
      
      // Create summarized result
      const summarizedResult: ToolCallResult = {
        result: {
          content: [{
            type: 'text',
            text: summary
          }],
          _meta: {
            ...result.result._meta,
            summarized: true,
            originalLength: originalContent.length,
            summaryLength: summary.length,
            compressionRatio: parseFloat((summary.length / originalContent.length).toFixed(3)),
            originalStorageKey: storageKey,
            summarizedAt: new Date().toISOString(),
            processingTimeMs: processingTime,
            provider: this.config.options?.provider || 'mock'
          }
        }
      };
      
      this.logger?.debug(`Summarized ${context.toolName} result`, {
        originalLength: originalContent.length,
        summaryLength: summary.length,
        compressionRatio: summary.length / originalContent.length,
        processingTime
      });
      
      return summarizedResult;
      
    } catch (error) {
      this.customStats.errorCount++;
      this.logger?.error(`Failed to summarize result for ${context.toolName}:`, error);
      
      // Return original result if summarization fails
      return {
        ...result,
        result: {
          ...result.result,
          _meta: {
            ...result.result._meta,
            summarizationError: error instanceof Error ? error.message : String(error),
            fallbackToOriginal: true
          }
        }
      };
    }
  }
  
  private shouldSummarize(context: PluginContext, result: ToolCallResult): boolean {
    // Don't summarize errors
    if (result.result.isError) {
      return false;
    }
    
    // Check if tool is in summarization list
    const summarizeTools = this.config.options?.summarizeTools || [];
    if (summarizeTools.length > 0 && !summarizeTools.includes(context.toolName)) {
      return false;
    }
    
    // Check content length threshold
    const content = this.extractContent(result);
    const minLength = this.config.options?.minContentLength || 100;
    if (content.length < minLength) {
      return false;
    }
    
    // Check if user requested original (bypass summarization)
    if (context.args.returnOriginal || context.args.noSummary) {
      return false;
    }
    
    return true;
  }
  
  private extractContent(result: ToolCallResult): string {
    if (!result.result.content) return '';
    
    return result.result.content
      .filter((item: any) => item.type === 'text')
      .map((item: any) => item.text)
      .join('\n');
  }
  
  private async generateSummary(content: string, context: PluginContext): Promise<string> {
    const prompt = this.getContextualPrompt(context.toolName);
    return await this.llmProvider.generateSummary(content, prompt);
  }
  
  private getContextualPrompt(toolName: string): string {
    const basePrompt = this.config.options?.summarizationPrompt || 'Summarize the following content:';
    
    const toolPrompts: Record<string, string> = {
      'search': 'Summarize these search results, highlighting the most relevant findings:',
      'research': 'Create a research summary focusing on key findings and implications:',
      'analyze': 'Summarize this analysis, emphasizing conclusions and recommendations:',
      'fetch-data': 'Summarize this data, highlighting trends and notable points:'
    };
    
    return toolPrompts[toolName] || basePrompt;
  }
  
  private generateStorageKey(context: PluginContext): string {
    return `${context.toolName}_${context.requestId}_${Date.now()}`;
  }
  
  private sanitizeContext(context: PluginContext): Omit<PluginContext, 'pluginData'> {
    // Remove pluginData to avoid circular references in storage
    const { pluginData, ...sanitized } = context;
    return sanitized;
  }
  
  private async saveOriginalResult(key: string, data: StoredResult): Promise<void> {
    // In-memory storage for this example
    // In production, this could be Redis, MongoDB, S3, etc.
    this.storage.set(key, data);
    
    // Cleanup old results to prevent memory leaks
    this.cleanupOldResults();
  }
  
  private cleanupOldResults(): void {
    const maxAge = 24 * 60 * 60 * 1000; // 24 hours
    const maxEntries = 1000; // Prevent unlimited growth
    const now = Date.now();
    
    // Remove old entries
    for (const [key, data] of this.storage.entries()) {
      if (now - data.timestamp > maxAge) {
        this.storage.delete(key);
      }
    }
    
    // If still too many entries, remove oldest
    if (this.storage.size > maxEntries) {
      const entries = Array.from(this.storage.entries())
        .sort((a, b) => a[1].timestamp - b[1].timestamp);
      
      const toRemove = entries.slice(0, this.storage.size - maxEntries);
      for (const [key] of toRemove) {
        this.storage.delete(key);
      }
    }
  }
  
  private updateCustomStats(originalLength: number, summaryLength: number): void {
    this.customStats.totalSummarizations++;
    this.customStats.totalSavings += (originalLength - summaryLength);
    
    // Update rolling average compression ratio
    const newRatio = summaryLength / originalLength;
    this.customStats.averageCompressionRatio = 
      (this.customStats.averageCompressionRatio * (this.customStats.totalSummarizations - 1) + newRatio) / 
      this.customStats.totalSummarizations;
  }
  
  // Public methods for testing and external access
  
  /**
   * Retrieve original result by storage key
   */
  async getOriginalResult(storageKey: string): Promise<StoredResult | null> {
    return this.storage.get(storageKey) || null;
  }
  
  /**
   * Get all stored results (for testing)
   */
  getStoredResults(): Map<string, StoredResult> {
    return new Map(this.storage);
  }
  
  /**
   * Clear all stored results (for testing)
   */
  clearStorage(): void {
    this.storage.clear();
  }
  
  /**
   * Get plugin statistics
   */
  async getStats() {
    const baseStats = await super.getStats();
    
    return {
      ...baseStats,
      customMetrics: {
        totalSummarizations: this.customStats.totalSummarizations,
        totalCharactersSaved: this.customStats.totalSavings,
        averageCompressionRatio: parseFloat(this.customStats.averageCompressionRatio.toFixed(3)),
        storedResults: this.storage.size,
        errorCount: this.customStats.errorCount,
        provider: this.config.options?.provider || 'mock'
      }
    };
  }
  
  /**
   * Update configuration at runtime
   */
  updateConfig(newConfig: Partial<PluginConfig>): void {
    this.config = { ...this.config, ...newConfig };
    
    // Reinitialize provider if needed
    if (newConfig.options?.provider && newConfig.options.provider !== this.config.options?.provider) {
      this.initializeProvider();
    }
  }
  
  private initializeProvider(): void {
    const provider = this.config.options?.provider || 'mock';
    
    if (provider === 'openai') {
      this.llmProvider = new OpenAIProvider(
        this.config.options?.openaiApiKey,
        this.config.options?.model,
        this.config.options?.maxTokens,
        this.config.options?.temperature
      );
    } else {
      this.llmProvider = new MockLLMProvider(this.config.options?.mockDelay);
    }
  }
  
  async destroy(): Promise<void> {
    this.logger?.info('LLM Summarization plugin shutting down');
    
    // Log final stats
    const stats = await this.getStats();
    this.logger?.info('Final plugin statistics:', stats.customMetrics);
    
    // Cleanup
    this.storage.clear();
  }
}

// Export types for testing
export { StoredResult, LLMProvider, MockLLMProvider, OpenAIProvider };
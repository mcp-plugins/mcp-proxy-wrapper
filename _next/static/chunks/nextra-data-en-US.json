{"/api-reference":{"title":"API Reference","data":{"":"Complete API documentation for the MCP Proxy Wrapper and plugin system.","core-api#Core API":"","wrapwithproxyserver-options#wrapWithProxy(server, options)":"Wraps an existing MCP server with proxy functionality and plugin support.\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [],\n  hooks?: ProxyHooks,\n  debug?: boolean\n});","parameters#Parameters":"Parameter\tType\tRequired\tDescription\tserver\tMcpServer\tYes\tMCP server instance to wrap\toptions.plugins\t(ProxyPlugin | PluginRegistration)[]\tNo\tArray of plugins to apply\toptions.hooks\tProxyHooks\tNo\tBefore/after tool call hooks\toptions.debug\tboolean\tNo\tEnable debug logging (default: false)","returns#Returns":"Promise<McpServer> - Enhanced server instance with proxy capabilities","example#Example":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' with API key for production\n    minContentLength: 100,\n    summarizeTools: ['search', 'analyze']\n  }\n});\nconst server = new McpServer({ name: 'my-server', version: '1.0.0' });\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin],\n  debug: true\n});","wrapwithenhancedproxyserver-options-v2-api#wrapWithEnhancedProxy(server, options) (v2 API)":"Enhanced version with advanced lifecycle management and performance features.\nimport { wrapWithEnhancedProxy, EnhancedProxyWrapperOptions } from 'mcp-proxy-wrapper';\nconst proxiedServer = await wrapWithEnhancedProxy(server, {\n  plugins: [],\n  hooks?: ProxyHooks,\n  lifecycle?: LifecycleConfig,\n  execution?: ExecutionConfig,\n  performance?: PerformanceConfig\n});","parameters-1#Parameters":"Parameter\tType\tRequired\tDescription\tserver\tMcpServer\tYes\tMCP server instance to wrap\toptions.plugins\t(ProxyPlugin | PluginRegistration)[]\tNo\tArray of plugins to apply\toptions.hooks\tProxyHooks\tNo\tBefore/after tool call hooks\toptions.lifecycle\tLifecycleConfig\tNo\tPlugin lifecycle management\toptions.execution\tExecutionConfig\tNo\tHook execution configuration\toptions.performance\tPerformanceConfig\tNo\tPerformance monitoring","returns-1#Returns":"Promise<McpServer> - Enhanced server with v2 proxy capabilities","plugin-interface#Plugin Interface":"","proxyplugin#ProxyPlugin":"Base interface that all plugins must implement.\ninterface ProxyPlugin {\n  name: string;\n  version: string;\n  \n  // Lifecycle hooks\n  beforeToolCall?(context: ToolCallContext): Promise<void | ToolCallResult>;\n  afterToolCall?(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>;\n  \n  // Plugin lifecycle\n  initialize?(context: PluginContext): Promise<void>;\n  destroy?(): Promise<void>;\n}","properties#Properties":"Property\tType\tRequired\tDescription\tname\tstring\tYes\tUnique plugin identifier\tversion\tstring\tYes\tPlugin version (semver)","methods#Methods":"Method\tParameters\tReturns\tDescription\tbeforeToolCall\tcontext: ToolCallContext\tPromise<void | ToolCallResult>\tCalled before tool execution\tafterToolCall\tcontext: ToolCallContext, result: ToolCallResult\tPromise<ToolCallResult>\tCalled after tool execution\tinitialize\tcontext: PluginContext\tPromise<void>\tPlugin initialization\tdestroy\tNone\tPromise<void>\tPlugin cleanup","pluginregistration#PluginRegistration":"Configuration object for registering plugins with specific settings.\ninterface PluginRegistration {\n  plugin: ProxyPlugin;\n  config?: PluginConfig;\n}","properties-1#Properties":"Property\tType\tRequired\tDescription\tplugin\tProxyPlugin\tYes\tThe plugin instance\tconfig\tPluginConfig\tNo\tPlugin-specific configuration","example-1#Example":"const proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    // Direct plugin registration\n    summaryPlugin,\n    \n    // Plugin with configuration\n    {\n      plugin: memoryPlugin,\n      config: {\n        // Plugin-specific settings go inside the 'options' object\n        options: {\n          maxEntries: 50, // Note: using actual option from ChatMemoryPlugin\n          sessionTimeout: 60 * 60 * 1000 // 1 hour\n        }\n      }\n    }\n  ]\n});","toolcallcontext#ToolCallContext":"Context object provided to plugin hooks during tool execution.\ninterface ToolCallContext {\n  toolName: string;\n  args: Record<string, any>;\n  metadata: {\n    requestId: string;\n    timestamp: number;\n    userId?: string;\n    [key: string]: any;\n  };\n}","properties-2#Properties":"Property\tType\tDescription\ttoolName\tstring\tName of the tool being called\targs\tRecord<string, any>\tArguments passed to the tool\tmetadata.requestId\tstring\tUnique request identifier\tmetadata.timestamp\tnumber\tRequest timestamp (Unix milliseconds)\tmetadata.userId\tstring?\tUser identifier (if available)","toolcallresult#ToolCallResult":"Result object returned from tool execution.\ninterface ToolCallResult {\n  content: Array<{\n    type: 'text' | 'image' | 'resource';\n    text?: string;\n    data?: string;\n    url?: string;\n    mimeType?: string;\n  }>;\n  isError?: boolean;\n  metadata?: Record<string, any>;\n}","properties-3#Properties":"Property\tType\tDescription\tcontent\tArray<ContentBlock>\tTool response content\tisError\tboolean?\tIndicates if result is an error\tmetadata\tRecord<string, any>?\tAdditional result metadata","plugincontext#PluginContext":"Context provided during plugin initialization.\ninterface PluginContext {\n  server: McpServer;\n  logger: Logger;\n  config: Record<string, any>;\n}","core-plugin-apis#Core Plugin APIs":"","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\n// Configuration options\ninterface SummarizationConfig {\n  provider: 'openai' | 'mock';      // AI provider\n  openaiApiKey?: string;            // OpenAI API key\n  model?: string;                   // Model name (default: gpt-4o-mini)\n  maxTokens?: number;               // Max tokens in summary\n  temperature?: number;             // Generation temperature\n  summarizeTools?: string[];        // Tools to summarize (empty = all)\n  minContentLength?: number;        // Min content length to summarize\n  saveOriginal?: boolean;           // Save original responses\n}\n// Update plugin configuration\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    summarizeTools: ['search', 'research', 'analyze'],\n    minContentLength: 100\n  }\n});\n// Get original result by storage key\nconst original = await summaryPlugin.getOriginalResult(storageKey);\n// Get plugin statistics\nconst stats = await summaryPlugin.getStats();","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\n// Configuration options\ninterface MemoryConfig {\n  provider: 'openai' | 'mock';      // Chat AI provider\n  openaiApiKey?: string;            // OpenAI API key\n  model?: string;                   // Model for chat responses\n  saveResponses?: boolean;          // Save tool responses\n  enableChat?: boolean;             // Enable chat functionality\n  maxEntries?: number;              // Max stored entries\n  maxSessions?: number;             // Max chat sessions\n  sessionTimeout?: number;          // Session timeout in ms\n  excludeTools?: string[];          // Tools to exclude from saving\n}\n// Update plugin configuration\nmemoryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    sessionTimeout: 24 * 60 * 60 * 1000\n  }\n});\n// Start chat session\nconst sessionId = await memoryPlugin.startChatSession(userId);\n// Chat with memory\nconst response = await memoryPlugin.chatWithMemory(\n  sessionId, \n  'What data do I have about sales?', \n  userId\n);\n// Search conversations\nconst results = memoryPlugin.searchConversations('sales metrics', userId);\n// Get conversation history\nconst history = memoryPlugin.getConversationHistory(userId, 20);","plugin-data-types#Plugin Data Types":"// LLM Summarization Plugin Types\ninterface StoredResult {\n  originalResult: ToolCallResult;\n  context: Omit<PluginContext, 'pluginData'>;\n  timestamp: number;\n  toolName: string;\n  requestId: string;\n  metadata?: Record<string, any>;\n}\ninterface LLMProvider {\n  generateSummary(content: string, prompt: string, options?: any): Promise<string>;\n}\n// Chat Memory Plugin Types\ninterface ConversationEntry {\n  id: string;\n  toolName: string;\n  request: {\n    args: Record<string, any>;\n    timestamp: number;\n  };\n  response: {\n    content: string;\n    metadata?: Record<string, any>;\n    timestamp: number;\n  };\n  context: {\n    requestId: string;\n    userId?: string;\n    sessionId?: string;\n  };\n}\ninterface ChatSession {\n  id: string;\n  userId?: string;\n  messages: ChatMessage[];\n  createdAt: number;\n  lastActivity: number;\n}\ninterface ChatMessage {\n  id: string;\n  type: 'user' | 'assistant' | 'system';\n  content: string;\n  timestamp: number;\n  metadata?: Record<string, any>;\n}","logger-interface#Logger Interface":"","logger#Logger":"Standard logging interface used throughout the system.\ninterface Logger {\n  debug(message: string, meta?: any): void;\n  info(message: string, meta?: any): void;\n  warn(message: string, meta?: any): void;\n  error(message: string, meta?: any): void;\n}","built-in-logging#Built-in Logging":"The proxy wrapper includes built-in logging with colored output. Enable debug mode to see detailed execution logs:\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin],\n  debug: true  // Enables detailed logging\n});","error-handling#Error Handling":"","plugin-errors#Plugin Errors":"Plugin errors are automatically caught and logged without breaking tool execution:\n// Plugin error handling\ntry {\n  await plugin.beforeToolCall(context);\n} catch (error) {\n  console.error(`Plugin ${plugin.name} error:`, error);\n  // Tool execution continues\n}","tool-errors#Tool Errors":"Tools should return error results in MCP format:\n// Tool error response\nreturn {\n  content: [{\n    type: 'text',\n    text: 'Error: Invalid input provided'\n  }],\n  isError: true\n};","plugin-errors-1#Plugin Errors":"Plugin errors are handled gracefully by the proxy wrapper:\n// LLM Summarization error (falls back to original)\nreturn {\n  ...result,\n  result: {\n    ...result.result,\n    _meta: {\n      ...result.result._meta,\n      summarizationError: 'OpenAI API unavailable',\n      fallbackToOriginal: true\n    }\n  }\n};\n// Chat Memory error (logs but doesn't break tool call)\ncatch (error) {\n  this.logger?.error(`Failed to save conversation entry: ${error}`);\n  return result; // Return original result\n}","type-definitions#Type Definitions":"","complete-typescript-definitions#Complete TypeScript Definitions":"// Export all types for use in your applications\nexport {\n  ProxyPlugin,\n  BasePlugin,\n  ToolCallContext,\n  ToolCallResult,\n  PluginContext,\n  PluginConfig,\n  PluginMetadata,\n  PluginStats,\n  Logger\n} from 'mcp-proxy-wrapper';","migration-guide#Migration Guide":"","from-direct-mcp-server#From Direct MCP Server":"// Before: Direct MCP server\nconst server = new McpServer(config);\nserver.tool('my-tool', schema, handler);\n// After: Wrapped with proxy\nconst proxiedServer = await wrapWithProxy(server, { plugins: [] });\nproxiedServer.tool('my-tool', schema, handler);","adding-ai-enhancement#Adding AI Enhancement":"// Add AI summarization to existing setup\nimport { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['research', 'analyze'],\n    minContentLength: 200\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin]\n});\nBackward Compatibility: The proxy wrapper maintains full compatibility with existing MCP server code. No changes are required to your tool implementations.","best-practices#Best Practices":"","plugin-development#Plugin Development":"Error Isolation: Always handle errors gracefully\nPerformance: Minimize blocking operations in beforeToolCall\nLogging: Use structured logging with context\nTesting: Write comprehensive tests for plugin logic","production-deployment#Production Deployment":"Environment Variables: Use environment-based configuration\nDatabase: Use PostgreSQL for production data storage\nMonitoring: Implement health checks and alerting\nSecurity: Follow security best practices for API keys","performance-optimization#Performance Optimization":"Plugin Priorities: Order plugins by execution cost\nCaching: Implement caching for expensive operations\nConnection Pooling: Use connection pooling for databases\nRate Limiting: Implement appropriate rate limiting\nReady to build? This API reference covers everything you need to integrate the MCP Proxy Wrapper into your applications."}},"/architecture":{"title":"Architecture","data":{"":"Technical architecture, design patterns, and internal mechanisms of the MCP Proxy Wrapper.","system-architecture-overview#System Architecture Overview":"The system consists of four distinct layers that work together to provide enhanced MCP functionality. Each layer has specific responsibilities and can be modified independently.\nThe architecture is designed with clear separation of concerns:\nClient Layer: Various MCP clients that consume tools\nTransport Layer: Communication protocols (STDIO, WebSocket, SSE, HTTP)\nProxy Wrapper Layer: Interception and plugin coordination\nMCP Server Layer: Your original, unmodified server and tools","plugin-execution-swimlane#Plugin Execution Swimlane":"The following swimlane diagram shows the detailed step-by-step process when a tool call is made. This illustrates how plugins interact with each other and with external systems like databases during request processing.","core-components#Core Components":"","1-proxy-wrapper-core#1. Proxy Wrapper Core":"The central orchestration component that:\nTool Interception: Replaces server.tool() method with enhanced version\nPlugin Coordination: Manages plugin lifecycle and execution order\nContext Management: Creates and maintains request context across plugins\nTransport Abstraction: Works with all MCP transport protocols\nError Isolation: Prevents plugin failures from breaking tool execution\nclass ProxyWrapper {\n  // Main wrapping method\n  static async wrapWithProxy(\n    server: McpServer, \n    options: ProxyWrapperOptions\n  ): Promise<WrappedServer>\n  \n  // Plugin management\n  private registerPlugin(plugin: ProxyPlugin): void\n  private initializePlugins(): Promise<void>\n  \n  // Tool interception\n  private enhanceToolMethod(originalMethod: Function): Function\n  private createEnhancedHandler(originalHandler: Function): Function\n  \n  // Execution coordination\n  private executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void>\n  private executeAfterHooks(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>\n}\nPlugin Error Isolation: Plugin failures don't break tool execution\nTimeout Management: Configurable timeouts for plugin execution\nGraceful Degradation: Core functionality continues even if plugins fail\nDetailed Logging: Comprehensive error reporting and debugging information\nRecovery Mechanisms: Automatic retry and fallback strategies","2-plugin-manager#2. Plugin Manager":"Handles plugin lifecycle, priority ordering, and execution:\ninterface PluginManager {\n  // Plugin registration and lifecycle\n  register(plugin: ProxyPlugin, config?: PluginConfig): Promise<void>\n  initialize(): Promise<void>\n  destroy(): Promise<void>\n  \n  // Execution methods\n  executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void>\n  executeAfterHooks(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>\n  \n  // Management methods\n  getExecutionOrder(): ProxyPlugin[]\n  validateDependencies(): Promise<boolean>\n  healthCheck(): Promise<Map<string, boolean>>\n}","3-plugin-architecture#3. Plugin Architecture":"Plugins follow a standard interface pattern:\ninterface ProxyPlugin {\n  // Identity\n  readonly name: string\n  readonly version: string\n  readonly metadata?: PluginMetadata\n  \n  // Configuration\n  config?: PluginConfig\n  \n  // Lifecycle hooks\n  initialize?(context: PluginInitContext): Promise<void>\n  beforeToolCall?(context: PluginContext): Promise<void | ToolCallResult>\n  afterToolCall?(context: PluginContext, result: ToolCallResult): Promise<ToolCallResult>\n  onError?(error: PluginError): Promise<void | ToolCallResult>\n  destroy?(): Promise<void>\n  \n  // Health and stats\n  healthCheck?(): Promise<boolean>\n  getStats?(): Promise<PluginStats>\n}","design-patterns#Design Patterns":"","1-decorator-pattern#1. Decorator Pattern":"The proxy wrapper uses the Decorator Pattern to enhance MCP servers:\n// Original server\nconst server = new McpServer({ name: 'my-server', version: '1.0.0' });\n// Decorated server with enhanced capabilities\nconst decoratedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});","2-chain-of-responsibility#2. Chain of Responsibility":"Plugins execute in a Chain of Responsibility pattern:\nclass PluginChain {\n  async executeBeforeHooks(context: ToolCallContext): Promise<ToolCallResult | void> {\n    for (const plugin of this.sortedPlugins) {\n      const result = await plugin.beforeToolCall?.(context);\n      if (result) {\n        return result; // Chain terminated early\n      }\n    }\n    // Chain completed successfully\n  }\n}","3-strategy-pattern#3. Strategy Pattern":"Different rate limiting strategies use the Strategy Pattern:\ninterface RateLimitStrategy {\n  checkLimit(userId: string, toolName: string): Promise<boolean>\n  updateUsage(userId: string, toolName: string): Promise<void>\n}\nclass FixedWindowLimiting implements RateLimitStrategy { /* ... */ }\nclass SlidingWindowLimiting implements RateLimitStrategy { /* ... */ }\nclass TokenBucketLimiting implements RateLimitStrategy { /* ... */ }","4-observer-pattern#4. Observer Pattern":"Analytics and monitoring use the Observer Pattern:\nclass EventEmitter {\n  emit(event: string, data: any): void\n  \n  // Plugin events\n  'tool:before': (context: ToolCallContext) => void\n  'tool:after': (context: ToolCallContext, result: ToolCallResult) => void\n  'plugin:error': (error: PluginError) => void\n  'rateLimit:exceeded': (limitInfo: RateLimitInfo) => void\n}","data-flow-architecture#Data Flow Architecture":"","request-context-flow#Request Context Flow":"","plugin-data-sharing#Plugin Data Sharing":"Plugins can share data through the context:\ninterface PluginContext extends ToolCallContext {\n  pluginData: Map<string, any>        // Shared plugin data\n  previousResults?: Map<string, any>  // Results from previous plugins\n}\n// Memory plugin sets conversation data\ncontext.pluginData.set('memory:sessionId', 'session_123');\n// Summarization plugin reads conversation context\nconst sessionId = context.pluginData.get('memory:sessionId');\nconst shouldSummarize = context.toolName === 'research' && content.length > 500;","performance-architecture#Performance Architecture":"","optimization-strategies#Optimization Strategies":"The proxy wrapper is designed for minimal overhead with several optimization strategies:\nLazy Loading: Plugins only initialize when first used\nAsync Execution: Non-blocking plugin execution with Promise.all where possible\nPriority Ordering: Critical plugins (auth) run first to fail fast\nResult Caching: Plugin results cached to avoid repeated expensive operations\nMemory Pooling: Context objects reused to reduce garbage collection","scalability-considerations#Scalability Considerations":"Stateless Design: Plugins maintain no server-side state\nDatabase Connections: Connection pooling for high-traffic scenarios\nCaching Layers: Redis/Memcached support for distributed caching\nLoad Balancing: Multiple proxy wrapper instances can run in parallel\nHorizontal Scaling: Database-backed plugins support clustering","security-architecture#Security Architecture":"","security-layers#Security Layers":"Memory Layer: Store and retrieve conversation history\nAI Enhancement Layer: Provide summarization and processing\nInput Validation: Sanitize and validate all inputs\nContext Management: Maintain session and user context\nResponse Processing: Transform and enhance tool outputs\nData Storage: Secure storage of conversation data","threat-model#Threat Model":"The proxy wrapper provides:\nData Privacy: Secure storage and handling of conversation data\nMemory Management: Controlled storage with cleanup and limits\nAI Safety: Secure integration with external AI providers\nContent Filtering: Validation of AI-generated content\nError Isolation: Plugin failures don't break core functionality\nResource Management: Memory and storage limits\nSecurity Best Practices: Always use HTTPS in production, rotate API keys regularly, and implement proper access controls.","extension-points#Extension Points":"The architecture provides several extension points for customization:\nCustom Plugins: Implement the ProxyPlugin interface\nCustom Transports: Extend transport layer compatibility\nCustom Authentication: Implement AuthenticationProvider interface\nCustom Analytics: Implement custom analytics tracking\nCustom Storage: Implement DatabaseAdapter interface\nNext: Explore the Plugin System and available plugins."}},"/deployment":{"title":"Deployment","data":{"":"Deploy your MCP Proxy Wrapper applications to production environments.","production-checklist#Production Checklist":"Before deploying to production, ensure you've completed these essential items.","security#Security":"✅ Secure API keys and authentication tokens\n✅ Secure environment variables (never commit secrets)\n✅ Use HTTPS for all endpoints\n✅ Enable proper input validation","configuration#Configuration":"✅ Set NODE_ENV=production\n✅ Configure proper logging level\n✅ Set up database connection (if using persistent storage)\n✅ Test authentication and rate limiting","environment-variables#Environment Variables":"","required-environment-variables#Required Environment Variables":"# Application\nNODE_ENV=production\nPORT=3000\nLOG_LEVEL=info\n# Authentication\nAUTH_ENDPOINT=https://your-auth-service.com/validate\nAPI_KEY_CACHE_TTL=3600\n# Rate Limiting\nRATE_LIMIT_WINDOW_MS=60000\nRATE_LIMIT_MAX_REQUESTS=100\n# Optional: Database (if using persistent storage)\nDATABASE_URL=postgresql://user:password@host:5432/database\n# Optional: Security\nJWT_SECRET=your_secure_jwt_secret\nImportant: Never commit these values to your repository. Use your deployment platform's secret management.","basic-docker-deployment#Basic Docker Deployment":"","simple-dockerfile#Simple Dockerfile":"FROM node:18-alpine\nWORKDIR /app\n# Copy package files\nCOPY package*.json ./\nRUN npm ci --only=production\n# Copy built application\nCOPY dist/ ./dist/\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]","docker-compose#Docker Compose":"version: '3.8'\nservices:\n  mcp-server:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n      - AUTH_ENDPOINT=${AUTH_ENDPOINT}\n      - RATE_LIMIT_WINDOW_MS=60000\n      - RATE_LIMIT_MAX_REQUESTS=100\n    restart: unless-stopped","platform-deployments#Platform Deployments":"","railway#Railway":"","connect-repository#Connect Repository":"Connect your GitHub repository to Railway.","set-environment-variables#Set Environment Variables":"Add your production environment variables in the Railway dashboard.","deploy#Deploy":"Railway will automatically build and deploy your application.\n# Railway CLI deployment\nnpm install -g @railway/cli\nrailway login\nrailway init\nrailway up","heroku#Heroku":"","install-heroku-cli#Install Heroku CLI":"Download and install the Heroku CLI.","create-application#Create Application":"heroku create your-app-name","set-environment-variables-1#Set Environment Variables":"heroku config:set NODE_ENV=production\nheroku config:set AUTH_ENDPOINT=https://your-auth-service.com/validate\nheroku config:set RATE_LIMIT_WINDOW_MS=60000\nheroku config:set RATE_LIMIT_MAX_REQUESTS=100","deploy-1#Deploy":"git push heroku main","vercel#Vercel":"Create a vercel.json file:\n{\n  \"version\": 2,\n  \"builds\": [\n    {\n      \"src\": \"dist/index.js\",\n      \"use\": \"@vercel/node\"\n    }\n  ],\n  \"routes\": [\n    {\n      \"src\": \"/(.*)\",\n      \"dest\": \"/dist/index.js\"\n    }\n  ]\n}\nDeploy:\nnpm install -g vercel\nvercel --prod","digitalocean-app-platform#DigitalOcean App Platform":"Create an app spec file:\nname: mcp-proxy-wrapper\nservices:\n- name: api\n  source_dir: /\n  github:\n    repo: your-username/your-repo\n    branch: main\n  run_command: node dist/index.js\n  environment_slug: node-js\n  instance_count: 1\n  instance_size_slug: basic-xxs\n  envs:\n  - key: NODE_ENV\n    value: production\n  - key: AUTH_ENDPOINT\n    value: https://your-auth-service.com/validate\n    type: SECRET\n  - key: RATE_LIMIT_WINDOW_MS\n    value: \"60000\"","health-checks#Health Checks":"","basic-health-endpoint#Basic Health Endpoint":"// Add to your server\nimport express from 'express';\nconst app = express();\napp.get('/health', (req, res) => {\n  res.status(200).json({ \n    status: 'healthy',\n    timestamp: new Date().toISOString()\n  });\n});\napp.get('/ready', async (req, res) => {\n  // Check dependencies (database, external APIs, etc.)\n  try {\n    // Add your health checks here\n    res.status(200).json({ status: 'ready' });\n  } catch (error) {\n    res.status(503).json({ \n      status: 'not ready',\n      error: error.message \n    });\n  }\n});","api-endpoint-configuration#API Endpoint Configuration":"","production-api-setup#Production API Setup":"","configure-authentication-endpoint#Configure Authentication Endpoint":"Set up your authentication service endpoint for API key validation.","set-rate-limiting#Set Rate Limiting":"Configure appropriate rate limits for your production environment:\nFree tier: 10 requests/minute\nPremium tier: 100 requests/minute\nEnterprise tier: 1000 requests/minute","database-setup#Database Setup":"If using persistent storage for analytics or caching, ensure your database is properly configured.","health-checks-1#Health Checks":"Implement health check endpoints to monitor your service status.","monitoring-and-logging#Monitoring and Logging":"","basic-logging#Basic Logging":"// Production logging configuration\nconst LOG_LEVEL = process.env.LOG_LEVEL || 'info';\n// Simple logging helper\nconst log = {\n  info: (message: string, data?: any) => {\n    if (['debug', 'info'].includes(LOG_LEVEL)) {\n      console.log(`[${new Date().toISOString()}] INFO: ${message}`, data || '');\n    }\n  },\n  error: (message: string, error?: any) => {\n    if (['debug', 'info', 'warn', 'error'].includes(LOG_LEVEL)) {\n      console.error(`[${new Date().toISOString()}] ERROR: ${message}`, error || '');\n    }\n  }\n};\n// Log important events\nlog.info('Server started', { port: process.env.PORT });\nlog.info('Plugin loaded', { plugin: 'llm-summarization' });","error-tracking#Error Tracking":"// Basic error handling\nprocess.on('uncaughtException', (error) => {\n  console.error(`[${new Date().toISOString()}] ERROR: Uncaught exception:`, error);\n  process.exit(1);\n});\nprocess.on('unhandledRejection', (reason, promise) => {\n  console.error(`[${new Date().toISOString()}] ERROR: Unhandled rejection at:`, promise, 'reason:', reason);\n  process.exit(1);\n});","plugin-configuration-examples#Plugin Configuration Examples":"","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3\n  }\n});","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 1000,\n    enableChat: true,\n    persistToDisk: process.env.NODE_ENV === 'production'\n  }\n});","backup-strategy#Backup Strategy":"","database-backups#Database Backups":"# PostgreSQL backup\npg_dump $DATABASE_URL > backup-$(date +%Y%m%d).sql\n# SQLite backup\ncp ./data/production.db ./backups/backup-$(date +%Y%m%d).db","environment-variables-backup#Environment Variables Backup":"Keep a secure record of your environment variables configuration (without the actual secrets).","troubleshooting#Troubleshooting":"","common-issues#Common Issues":"Environment variables not loading:\n# Check if variables are set\necho $AUTH_ENDPOINT\n# Should not be empty in production\nAuthentication not working:\n# Test auth endpoint\ncurl -X POST https://your-domain.com/api/test \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -d '{\"test\": \"auth\"}'\nApplication not starting:\n# Check logs\ndocker logs your-container-name\n# or\nheroku logs --tail --app your-app-name\nDatabase connection issues:\n# Test database connection\nnode -e \"console.log(require('pg').parse(process.env.DATABASE_URL))\"","debug-mode#Debug Mode":"Enable debug logging temporarily:\n# Set LOG_LEVEL to debug\nLOG_LEVEL=debug node dist/index.js\nReady for production: Your MCP Proxy Wrapper application is now deployed and ready to handle real users with enhanced plugin functionality.","security-best-practices#Security Best Practices":"Never commit secrets to your repository\nUse HTTPS for all API endpoints\nValidate all inputs and sanitize data\nRotate API keys regularly\nMonitor for unusual activity and implement alerts\nKeep dependencies updated with npm audit\nImplement proper rate limiting to prevent abuse\nUse secure authentication mechanisms","next-steps#Next Steps":"Getting Started: Review setup guide\nExamples: See real-world implementations\nAPI Reference: Complete API documentation\nPlugins: Explore available plugins\nNeed help with deployment? Check the troubleshooting section above or open an issue on GitHub."}},"/how-it-works":{"title":"How It Works","data":{"":"The MCP Proxy Wrapper acts as an invisible enhancement layer that sits between MCP clients and your existing server. It intercepts tool calls, adds powerful features through plugins and hooks, then forwards everything to your original server code—all without requiring any changes to your existing implementation.","-the-magic-zero-code-enhancement#🔍 The Magic: Zero-Code Enhancement":"The Problem: You have a working MCP server but need to add authentication, logging, AI summarization, or monitoring. Traditional approaches require modifying your server code, adding dependencies, and maintaining complex integrations.The Solution: The proxy wrapper intercepts your server's tool registration process and transparently enhances each tool with plugin functionality, while preserving your original code completely unchanged.\n// Your existing server stays exactly the same\nconst server = new McpServer({ name: 'My Server', version: '1.0.0' });\nserver.tool('getData', schema, originalHandler); // ← NO CHANGES TO THIS\n// The proxy wrapper creates an enhanced version\nconst enhancedServer = await wrapWithProxy(server, {\n  plugins: [new LLMSummarizationPlugin()]\n});\n// Now your tools have AI summarization without code changes!","-core-mechanism#🔧 Core Mechanism":"Instead of modifying your server code, the proxy wrapper works by intercepting the tool registration process and creating an enhanced wrapper around each tool handler.","-request-flow-your-tools-enhanced#📊 Request Flow: Your Tools, Enhanced":"Here's exactly what happens when a client calls your tool:\n🔄 Step-by-Step Breakdown:\nClient Request: getData({ query: 'AI trends' })\nProxy Intercepts: Captures the request before your tool sees it\nBefore Hooks Execute:\n🔐 Authentication check (if configured)\n📊 Request logging starts\n✅ Input validation and sanitization\nYour Tool Executes: Your original getData handler runs unchanged\nAfter Hooks Execute:\n🤖 AI summarizes long responses (if > 500 chars)\n💾 Saves response to conversation memory\n📊 Logs completion time and metadata\nEnhanced Response: Client gets summarized content + metadata\n✨ Your original tool code never changes, but gains enterprise features!","plugin-execution-flow#Plugin Execution Flow":"The proxy wrapper executes plugins in a priority-ordered sequence with two main phases:","phase-1-beforetoolcall#Phase 1: beforeToolCall":"Memory Plugin: Check if response should be saved\nLLM Summarization: Check if response should be summarized\nRequest Processing: Prepare context for tool execution","phase-2-aftertoolcall#Phase 2: afterToolCall":"Memory Plugin: Save tool responses to memory database\nLLM Summarization: Generate AI summaries of long responses\nResponse Enhancement: Add metadata about processing\nShort-Circuit Capability: Any beforeToolCall hook can return a result to immediately respond without executing the original tool.","tool-interception-process#Tool Interception Process":"The proxy wrapper modifies your MCP server through a three-step process:","server-wrapping#Server Wrapping":"When you call wrapWithProxy(server, options), the wrapper:\nStores a reference to the original server.tool() method\nReplaces it with an enhanced version that includes plugin hooks\nInitializes all registered plugins in priority order","tool-registration-enhancement#Tool Registration Enhancement":"When you call proxiedServer.tool(name, schema, handler):\nThe original tool schema and handler are preserved\nA new enhanced handler is created that wraps the original\nPlugin hooks are injected before and after the original handler","runtime-execution#Runtime Execution":"When a tool call arrives:\nContext is created with tool name, arguments, and metadata\nbeforeToolCall hooks execute in priority order (highest first)\nIf no hook short-circuits, the original tool handler executes\nafterToolCall hooks execute in reverse priority order (lowest first)\nThe final result is returned to the client","tool-registration-behavior#Tool Registration Behavior":"Important: The proxy wrapper only enhances tools registered AFTER wrapping. Tools registered before wrapping remain available but don't get hook/plugin functionality.","what-gets-enhanced#What Gets Enhanced":"const server = new McpServer({ name: 'My Tools', version: '1.0.0' });\n// ❌ This tool won't have plugin functionality\nserver.tool('old-tool', { text: z.string() }, async (args) => {\n  return { content: [{ type: 'text', text: 'Old tool response' }] };\n});\n// Wrap the server\nconst proxiedServer = await wrapWithProxy(server, { plugins: [authPlugin] });\n// ✅ This tool will have full plugin functionality\nproxiedServer.tool('new-tool', { text: z.string() }, async (args) => {\n  return { content: [{ type: 'text', text: 'Enhanced tool response' }] };\n});\n// Both tools are available to clients, but only 'new-tool' gets:\n// - Authentication checks\n// - Summarization functionality\n// - Analytics tracking\n// - Custom hook execution","all-server-functionality-preserved#All Server Functionality Preserved":"The proxy wrapper preserves all existing MCP server functionality:\nExisting tools remain fully functional\nResource providers work unchanged\nPrompt templates are unaffected\nServer metadata and capabilities are preserved\nTransport layer (STDIO, WebSocket, etc.) works identically","️-behind-the-scenes-what-actually-happens#🛠️ Behind the Scenes: What Actually Happens":"When you call wrapWithProxy(), here's the magic that happens under the hood:","your-original-code-unchanged#Your Original Code (Unchanged)":"const server = new McpServer({ name: 'My Tools', version: '1.0.0' });\n// Your existing tool - NO CHANGES REQUIRED\nserver.tool('analyze-text', { \n  text: z.string(),\n  userId: z.string().optional() \n}, async (args) => {\n  const result = await performAnalysis(args.text);\n  return { content: [{ type: 'text', text: result }] };\n});","what-the-proxy-wrapper-creates-internally#What the Proxy Wrapper Creates Internally":"// The proxy wrapper intercepts tool registration\nconst originalToolMethod = server.tool.bind(server);\nserver.tool = function(name: string, schema: any, originalHandler: Function) {\n  \n  // Create an enhanced wrapper around your original handler\n  const enhancedHandler = async (args: any) => {\n    const context = {\n      toolName: name,\n      args,\n      metadata: { \n        requestId: generateId(), \n        timestamp: Date.now(),\n        userId: args.userId \n      }\n    };\n    \n    // PHASE 1: Execute beforeToolCall hooks\n    console.log(`🔧 [${new Date().toISOString()}] Starting: ${name}`);\n    \n    for (const plugin of plugins) {\n      const result = await plugin.beforeToolCall?.(context);\n      if (result) {\n        // Plugin can short-circuit (e.g., return cached result)\n        console.log(`⚡ Short-circuited by: ${plugin.name}`);\n        return result;\n      }\n    }\n    \n    // PHASE 2: Execute your original tool (UNCHANGED)\n    console.log(`▶️ Executing original handler for: ${name}`);\n    const originalResult = await originalHandler(args);\n    \n    // PHASE 3: Execute afterToolCall hooks\n    let enhancedResult = originalResult;\n    \n    for (const plugin of plugins) {\n      if (plugin.afterToolCall) {\n        enhancedResult = await plugin.afterToolCall(context, enhancedResult);\n        console.log(`🔄 Enhanced by: ${plugin.name}`);\n      }\n    }\n    \n    console.log(`✅ [${new Date().toISOString()}] Completed: ${name}`);\n    return enhancedResult;\n  };\n  \n  // Register the enhanced handler with the original MCP server\n  return originalToolMethod(name, schema, enhancedHandler);\n};","the-result-enhanced-without-changes#The Result: Enhanced Without Changes":"// When a client calls your tool:\nconst result = await client.callTool({\n  name: 'analyze-text',\n  arguments: { text: 'This is a long document...', userId: 'user123' }\n});\n// Your original tool logic executes unchanged, but now:\nconsole.log(result._meta);\n// {\n//   summarized: true,           // ← Added by LLM plugin\n//   originalLength: 1200,       // ← Added by LLM plugin  \n//   summaryLength: 150,         // ← Added by LLM plugin\n//   savedToMemory: true,        // ← Added by Memory plugin\n//   processedAt: \"2024-01-15T...\", // ← Added by proxy\n//   requestId: \"req_123\"        // ← Added by proxy\n// }\n🎯 Key Insight: Your original tool handler runs exactly as written, but the proxy wrapper adds a powerful enhancement layer around it without requiring any code changes.","transport-compatibility#Transport Compatibility":"The proxy wrapper works with all MCP transport methods because it operates at the tool handler level, not the transport level:\nSTDIO: Command-line MCP servers\nWebSocket: Real-time web applications\nSSE: Server-sent events for streaming\nHTTP: REST API style interactions\nInMemory: Testing and development","plugin-context-data#Plugin Context Data":"Each plugin receives rich context information:\ninterface ToolCallContext {\n  toolName: string;           // Name of the tool being called\n  args: Record<string, any>;  // Tool arguments from client\n  metadata: {\n    requestId: string;        // Unique request identifier\n    timestamp: number;        // Request timestamp\n    userId?: string;          // Authenticated user ID\n    sessionId?: string;       // Session identifier\n    transport: string;        // Transport method used\n  };\n}\nThis context flows through all plugin hooks, allowing for sophisticated cross-plugin coordination and data sharing.","error-handling#Error Handling":"The proxy wrapper includes robust error handling:\nPlugin Errors: Isolated and logged without breaking tool execution\nTool Errors: Proper MCP error responses with isError: true\nTransport Errors: Graceful degradation and retry logic\nTimeout Handling: Configurable timeouts for plugin execution\nPlugin Isolation: Plugin errors never break your original tool functionality. If a plugin fails, the tool call continues normally.","performance-considerations#Performance Considerations":"The proxy wrapper is designed for minimal overhead:\nLazy Loading: Plugins only load when needed\nAsync Execution: Non-blocking plugin execution\nCaching: Plugin results can be cached to avoid repeated operations\nPriority Ordering: Critical plugins (auth) run first, optional plugins (analytics) run last\nNext: Learn about the detailed Architecture and design patterns."}},"/":{"title":"🚀 MCP Proxy Wrapper","data":{"":"Add hooks and plugins to any MCP server without changing your code\nnpm install mcp-proxy-wrapper\nA proxy wrapper that adds hooks, plugins, and custom logic to existing MCP servers without modifying the original code.","-overview#🎯 Overview":"Add functionality to your existing MCP server:\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\n// Your existing server - no changes needed\nconst server = new McpServer({ name: 'My Server', version: '1.0.0' });\nserver.tool('getData', schema, getData);\n// Add plugins and hooks\nconst plugin = new LLMSummarizationPlugin();\nplugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY\n  }\n});\nconst enhanced = await wrapWithProxy(server, {\n  plugins: [plugin],\n  hooks: {\n    beforeToolCall: async (context) => {\n      console.log(`🔧 ${context.toolName}`);\n      // Add auth, rate limiting, etc.\n    }\n  }\n});\nResult: Your server now has AI summarization, logging, and custom hooks without any code changes.","-key-features#✨ Key Features":"🔧 Zero Code Changes - Wrap existing servers instantly\n🤖 AI Integration - OpenAI-powered response summarization\n🪝 Hook System - beforeToolCall/afterToolCall with full control\n🔌 Plugin Architecture - Reusable, composable functionality\n🌐 Remote Servers - Proxy external MCP servers over HTTP/WebSocket\n🛡️ Authentication & Security - Auth, rate limiting, access control patterns\n📊 Comprehensive Tests - 273 tests covering MCP protocol compatibility","-common-use-cases#🚀 Common Use Cases":"const secure = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      if (!await validateApiKey(context.args.apiKey)) {\n        return { \n          result: { \n            content: [{ type: 'text', text: 'Unauthorized' }], \n            isError: true \n          }\n        };\n      }\n    }\n  }\n});\nconst plugin = new LLMSummarizationPlugin();\nplugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['research', 'analyze']\n  }\n});\nconst intelligent = await wrapWithProxy(server, {\n  plugins: [plugin]\n});\nconst monitored = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      console.log(`📊 ${context.toolName} started`);\n      context.startTime = Date.now();\n    },\n    afterToolCall: async (context, result) => {\n      const duration = Date.now() - context.startTime;\n      console.log(`✅ ${context.toolName} completed in ${duration}ms`);\n      return result;\n    }\n  }\n});\n// Proxy external servers and add features\nconst plugin = new LLMSummarizationPlugin();\nplugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY\n  }\n});\nconst proxy = await createHttpServerProxy('https://api.example.com/mcp', {\n  plugins: [plugin],\n  hooks: { /* your custom logic */ }\n});","-technical-details#🧪 Technical Details":"273 passing tests with real MCP client-server communication\nComprehensive error handling with fallbacks and proper error propagation\nTypeScript native with full type safety and IntelliSense\nMCP SDK v1.6.0+ compatible with any existing server","-quick-navigation#🚀 Quick Navigation":"Get up and running with the step-by-step guide.\nUnderstand the proxy interception mechanism and plugin execution flow.\nDeep dive into the technical architecture and design patterns.\nExtend your MCP server with powerful plugins for LLM summarization, chat memory, and custom processing.","-key-features-1#⭐ Key Features":"🔌 Plugin Architecture\nExtensible hook system for beforeToolCall and afterToolCall with zero server modifications.\n🤖 AI Enhancement Plugins\nLLM Summarization and Chat Memory plugins included for intelligent tool enhancement.\n🔐 Authentication & Security\nFlexible hook system for implementing access control, rate limiting, and user management.\n📈 Analytics & Monitoring\nUsage tracking, performance metrics, error reporting, and real-time monitoring capabilities.\n🌐 Transport Agnostic\nWorks with STDIO, WebSocket, SSE, HTTP, and InMemory transport protocols.\n🏢 Robust Architecture\nComprehensive error handling, logging, and stable API design.","-how-the-proxy-wrapper-works-with-tools#🔧 How the Proxy Wrapper Works with Tools":"The proxy wrapper enhances your MCP server without breaking existing functionality - it's completely backward compatible!\nTools registered BEFORE wrapping: Remain fully available and functional, but don't get enhanced with hooks/plugins\nTools registered AFTER wrapping: Get full plugin functionality (summarization, memory, analytics, etc.)\nAll underlying server functionality: Completely preserved (resources, prompts, metadata, transport)\nThe proxy wrapper intercepts the server.tool() method registration process, not the tools themselves. So when you call wrapWithProxy(server), it overrides how new tools are registered to add the hook functionality, but existing tools continue to work exactly as before.\n📖 This behavior is documented in detail in the Getting Started guide and How It Works section with examples showing the difference between enhanced and non-enhanced tools.","-quick-example#💻 Quick Example":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create your MCP server\nconst server = new McpServer({ name: 'My AI Tools', version: '1.0.0' });\n// Add AI enhancement plugin\nconst summarizationPlugin = new LLMSummarizationPlugin();\n// Wrap with proxy functionality\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summarizationPlugin]\n});\n// Register tools as usual - enhancement happens automatically\nproxiedServer.tool('ai-analysis', {\n  text: z.string()\n}, async (args) => {\n  return {\n    content: [{ type: 'text', text: `Analysis result: ${args.text}` }]\n  };\n});","-get-started#🎯 Get Started":"Step-by-step setup guide\nExplore available plugins and create your own\nMCP Proxy Wrapper - Add hooks and plugins to any MCP server"}},"/plugins":{"title":"Plugin System","data":{"":"The MCP Proxy Wrapper features a powerful plugin architecture that allows you to extend MCP servers with additional functionality like AI enhancement, analytics, security, and more.\nPlugins operate at the tool call level, intercepting requests before and after execution to add features without modifying your core tool logic.","how-plugins-work#How Plugins Work":"Plugins implement lifecycle hooks that are called during tool execution:\nexport interface ProxyPlugin {\n  name: string;\n  version: string;\n  \n  // Called before tool execution\n  beforeToolCall?(context: ToolCallContext): Promise<void | ToolCallResult>;\n  \n  // Called after tool execution  \n  afterToolCall?(context: ToolCallContext, result: ToolCallResult): Promise<ToolCallResult>;\n  \n  // Plugin lifecycle\n  initialize?(context: PluginContext): Promise<void>;\n  destroy?(): Promise<void>;\n}","available-plugins#Available Plugins":"Automatically generate AI summaries of tool responses using OpenAI or mock providers.\nSave tool responses to memory and provide chat interface for interacting with saved data.","core-plugins#Core Plugins":"The following plugins are included in the core library:","llm-summarization-plugin#LLM Summarization Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3,\n    summarizeTools: ['search', 'research', 'analyze', 'fetch-data'],\n    minContentLength: 100,\n    saveOriginal: true\n  }\n});\n// The plugin intercepts tool results and returns AI-generated summaries\n// Original content is saved and can be retrieved later\n// Works with both OpenAI API and mock provider for testing","chat-memory-plugin#Chat Memory Plugin":"import { ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    maxSessions: 100,\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours\n    excludeTools: ['chat-with-memory', 'get-memory-stats']\n  }\n});\n// The plugin saves all tool responses to memory\n// Provides chat interface to interact with saved data\n// Supports searching and querying conversation history","plugin-categories#Plugin Categories":"The plugin system supports several categories of functionality:","ai-enhancement-plugins#AI Enhancement Plugins":"LLM Summarization plugin provides AI-powered response summarization with configurable providers and models.","memory--storage-plugins#Memory & Storage Plugins":"Chat Memory plugin enables persistent storage of tool responses with intelligent search and chat interfaces.","extensible-architecture#Extensible Architecture":"The plugin system is designed to be extensible - you can create custom plugins by extending the BasePlugin class and implementing the required interfaces.","plugin-execution-flow#Plugin Execution Flow":"","plugin-priorities#Plugin Priorities":"Plugins execute in priority order (higher numbers first):\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, config: { priority: 20 } },      // Memory plugin (higher priority)\n    { plugin: summaryPlugin, config: { priority: 10 } }      // Summary plugin (lower priority)\n  ]\n});","quick-start#Quick Start":"","1-install-plugin-dependencies#1. Install Plugin Dependencies":"npm install mcp-proxy-wrapper","2-import-and-configure#2. Import and Configure":"import { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\n// Configure enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' with API key for production\n    summarizeTools: ['search', 'analyze'],\n    minContentLength: 50\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 100\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin, memoryPlugin]\n});","3-register-tools#3. Register Tools":"// Your tools are now enhanced with plugin functionality\nproxiedServer.tool('research-analysis', {\n  topic: z.string(),\n  depth: z.enum(['basic', 'detailed']).default('basic'),\n  userId: z.string().optional()\n}, async (args) => {\n  // Plugins handle summarization and memory automatically\n  const research = await performResearch(args.topic, args.depth);\n  return {\n    content: [{ type: 'text', text: research }]\n  };\n});","error-handling#Error Handling":"Plugins include robust error handling to ensure tool calls aren't broken by plugin failures:\n// Plugin errors are isolated and logged\ntry {\n  await plugin.beforeToolCall(context);\n} catch (error) {\n  console.error('Plugin error:', error);\n  // Tool call continues normally\n}","plugin-configuration#Plugin Configuration":"","environment-based-configuration#Environment-based Configuration":"const summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150\n  },\n  enabled: true,\n  priority: 10\n});","dynamic-configuration#Dynamic Configuration":"const memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    maxEntries: process.env.NODE_ENV === 'production' ? 10000 : 100,\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours\n    enableChat: true,\n    saveResponses: true\n  },\n  enabled: true,\n  priority: 20\n});","testing-plugins#Testing Plugins":"The proxy wrapper includes testing utilities for plugin development:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\ndescribe('LLM Summarization Plugin', () => {\n  test('summarizes long content', async () => {\n    const plugin = new LLMSummarizationPlugin();\n    plugin.updateConfig({\n      options: {\n        provider: 'mock', // Use mock for testing\n        minContentLength: 50,\n        summarizeTools: ['test-tool']\n      }\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { plugins: [plugin] });\n    \n    const result = await proxiedServer.callTool('test-tool', { \n      text: 'This is a very long piece of content that should be summarized by the plugin because it exceeds the minimum length threshold.' \n    });\n    \n    expect(result.result._meta?.summarized).toBe(true);\n    expect(result.result.content[0].text).toContain('Summary:');\n  });\n});","best-practices#Best Practices":"","1-plugin-isolation#1. Plugin Isolation":"Keep plugins independent and focused on single responsibilities\nDon't rely on other plugins' state or behavior\nHandle errors gracefully without breaking tool calls","2-performance#2. Performance":"Minimize blocking operations in beforeToolCall\nUse async operations for external API calls\nImplement caching for expensive operations","3-configuration#3. Configuration":"Support environment-based configuration\nProvide sensible defaults\nValidate configuration on plugin initialization","4-logging#4. Logging":"Use structured logging with appropriate levels\nInclude context information (requestId, userId, etc.)\nDon't log sensitive information (API keys, personal data)","5-testing#5. Testing":"Write unit tests for plugin logic\nTest integration with the proxy wrapper\nInclude error scenarios and edge cases","community-plugins#Community Plugins":"The MCP community is building additional plugins. Community contributions are welcome for:\nAnalytics and monitoring solutions\nAuthentication and security plugins\nPerformance optimization tools\nIntegration plugins for popular services\nCustom business logic validators\nInterested in contributing? See our plugin development guide below.","creating-your-own-plugin#Creating Your Own Plugin":"Ready to build a custom plugin? Check out the plugin interface to get started:\nLearn about the core plugin interface and required methods for building custom plugins.\nView the source code for the included plugins to understand implementation patterns.\nReady to extend your MCP server? Choose from our official plugins or create your own to add exactly the functionality you need."}},"/remote-servers":{"title":"Remote Server Proxying","data":{"":"The MCP Proxy Wrapper can connect to remote MCP servers and add plugin functionality without modifying the remote server code. This enables you to enhance external MCP services with AI summarization, chat memory, authentication, and more.\nNew Feature: Remote server proxying allows you to add enterprise features to any external MCP server over HTTP/SSE, STDIO, or WebSocket connections.","architecture#Architecture":"Your MCP Client → Proxy Wrapper → [Plugins] → Remote MCP Server (HTTP/SSE/STDIO)\nThe proxy wrapper acts as both:\nMCP Server - Exposes enhanced tools to your clients\nMCP Client - Connects to the remote server to forward requests","quick-examples#Quick Examples":"","httpsse-remote-server#HTTP/SSE Remote Server":"import { createHttpServerProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\n// Configure the plugin\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['search', 'analyze'],\n    minContentLength: 200\n  }\n});\n// Connect to remote HTTP/SSE MCP server and add AI summarization\nconst proxyServer = await createHttpServerProxy('https://api.example.com/mcp', {\n  plugins: [summaryPlugin],\n  remoteServer: {\n    name: 'External API Server',\n    headers: {\n      'Authorization': 'Bearer your-api-key',\n      'X-Client-Name': 'MCP-Proxy-Wrapper'\n    }\n  },\n  debug: true\n});\n// Your proxy server is ready!\nconst transport = new StdioServerTransport();\nawait proxyServer.connect(transport);","stdio-remote-server#STDIO Remote Server":"import { createStdioServerProxy, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\n// Configure the plugin\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000\n  }\n});\n// Connect to remote STDIO MCP server and add conversation memory\nconst proxyServer = await createStdioServerProxy('node', ['remote-server.js'], {\n  plugins: [memoryPlugin],\n  remoteServer: {\n    name: 'Remote STDIO Server',\n    env: { \n      API_KEY: process.env.REMOTE_API_KEY,\n      NODE_ENV: 'production'\n    },\n    cwd: '/path/to/remote/server'\n  }\n});","complete-configuration#Complete Configuration":"","step-1-configure-remote-server-connection#Step 1: Configure Remote Server Connection":"import { createRemoteServerProxy } from 'mcp-proxy-wrapper';\nconst config = {\n  remoteServer: {\n    // Transport type\n    transport: 'sse' as const, // 'stdio' | 'sse' | 'websocket'\n    \n    // For HTTP/SSE/WebSocket\n    url: 'https://api.example.com/mcp',\n    headers: {\n      'Authorization': 'Bearer token',\n      'User-Agent': 'MCP-Proxy/1.0'\n    },\n    timeout: 30000,\n    \n    // For STDIO\n    command: 'node',\n    args: ['server.js'],\n    env: { NODE_ENV: 'production' },\n    cwd: '/app',\n    \n    // Metadata\n    name: 'Remote MCP Service',\n    version: '1.0.0'\n  }\n};","step-2-add-plugins-for-enhancement#Step 2: Add Plugins for Enhancement":"import { LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\n// Configure AI summarization plugin\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    summarizeTools: ['research', 'analyze', 'fetch-data'],\n    minContentLength: 500,\n    saveOriginal: true // Keep original for retrieval\n  }\n});\n// Configure conversation memory plugin\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 2000,\n    sessionTimeout: 24 * 60 * 60 * 1000 // 24 hours\n  }\n});\nconst plugins = [summaryPlugin, memoryPlugin];","step-3-create-and-start-proxy#Step 3: Create and Start Proxy":"const proxyServer = await createRemoteServerProxy({\n  ...config,\n  plugins,\n  proxyServerName: 'Enhanced Remote Proxy',\n  proxyServerVersion: '1.0.0',\n  debug: process.env.NODE_ENV !== 'production'\n});\n// Connect to your transport\nconst transport = new StdioServerTransport();\nawait proxyServer.connect(transport);\nconsole.log('🎉 Remote proxy server ready with enhanced functionality!');","real-world-use-cases#Real-World Use Cases":"Add AI summarization and memory to external MCP services\nConnect to older MCP servers and add modern features\nProxy multiple remote servers with different enhancements\nAdd authentication and monitoring to remote services","api-enhancement#API Enhancement":"// Configure the plugin\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['get_wallet_analysis', 'get_transaction_summary'],\n    minContentLength: 300\n  }\n});\n// Enhance a blockchain analytics API with AI summarization\nconst blockchainProxy = await createHttpServerProxy('https://blockchain-api.com/mcp', {\n  plugins: [summaryPlugin],\n  remoteServer: {\n    name: 'Blockchain Analytics API',\n    headers: { 'X-API-Key': process.env.BLOCKCHAIN_API_KEY }\n  }\n});\n// Now blockchain analysis results are automatically summarized!","legacy-integration#Legacy Integration":"// Connect to an older STDIO-based MCP server and modernize it\nconst legacyProxy = await createStdioServerProxy('python', ['legacy_mcp_server.py'], {\n  plugins: [\n    new ChatMemoryPlugin(), // Add conversation memory\n    new LLMSummarizationPlugin() // Add AI summarization\n  ],\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Add modern logging\n      console.log(`📊 [${new Date().toISOString()}] Tool: ${context.toolName}`);\n    },\n    afterToolCall: async (context, result) => {\n      // Add performance tracking\n      result._meta = {\n        ...result._meta,\n        processedAt: new Date().toISOString(),\n        proxyVersion: '1.0.0'\n      };\n      return result;\n    }\n  }\n});","security--monitoring#Security & Monitoring":"const secureProxy = await createRemoteServerProxy({\n  remoteServer: {\n    transport: 'sse',\n    url: 'https://external-service.com/mcp',\n    headers: { 'Authorization': `Bearer ${process.env.EXTERNAL_TOKEN}` }\n  },\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Authentication check\n      if (!context.args.userId || !await validateUser(context.args.userId)) {\n        return {\n          content: [{ type: 'text', text: 'Authentication required' }],\n          isError: true\n        };\n      }\n      \n      // Rate limiting\n      if (await isRateLimited(context.args.userId)) {\n        return {\n          content: [{ type: 'text', text: 'Rate limit exceeded' }],\n          isError: true\n        };\n      }\n      \n      // Audit logging\n      await auditLog({\n        userId: context.args.userId,\n        tool: context.toolName,\n        timestamp: Date.now()\n      });\n    }\n  }\n});","transport-specific-configuration#Transport-Specific Configuration":"// HTTP/SSE Remote Server\nconst config = {\n  remoteServer: {\n    transport: 'sse',\n    url: 'https://api.example.com/mcp/events',\n    headers: {\n      'Authorization': 'Bearer token',\n      'Accept': 'text/event-stream',\n      'X-Client-ID': 'proxy-client'\n    },\n    timeout: 30000\n  }\n};\n// STDIO Remote Server\nconst config = {\n  remoteServer: {\n    transport: 'stdio',\n    command: 'node',\n    args: ['dist/server.js'],\n    env: {\n      NODE_ENV: 'production',\n      API_KEY: process.env.REMOTE_API_KEY,\n      LOG_LEVEL: 'info'\n    },\n    cwd: '/app/remote-server'\n  }\n};\n// WebSocket Remote Server\nconst config = {\n  remoteServer: {\n    transport: 'websocket',\n    url: 'wss://api.example.com/mcp/ws',\n    headers: {\n      'Authorization': 'Bearer token'\n    },\n    timeout: 30000\n  }\n};","error-handling#Error Handling":"The proxy wrapper includes robust error handling for remote connections:\nconst proxyServer = await createRemoteServerProxy({\n  remoteServer: {\n    transport: 'sse',\n    url: 'https://api.example.com/mcp'\n  },\n  hooks: {\n    beforeToolCall: async (context) => {\n      try {\n        // Pre-processing logic\n        await validateRequest(context.args);\n      } catch (error) {\n        return {\n          content: [{ type: 'text', text: `Validation error: ${error.message}` }],\n          isError: true\n        };\n      }\n    },\n    afterToolCall: async (context, result) => {\n      // Handle remote server errors gracefully\n      if (result.isError) {\n        await logError({\n          tool: context.toolName,\n          error: result.content[0].text,\n          timestamp: Date.now()\n        });\n      }\n      return result;\n    }\n  }\n}).catch(error => {\n  console.error('Failed to connect to remote server:', error);\n  // Implement fallback logic or retry\n});","performance-considerations#Performance Considerations":"","connection-pooling#Connection Pooling":"For high-throughput scenarios, consider connection pooling:\n// For multiple remote servers\nconst serverPool = [\n  'https://api1.example.com/mcp',\n  'https://api2.example.com/mcp',\n  'https://api3.example.com/mcp'\n];\nconst proxies = await Promise.all(\n  serverPool.map(url => createHttpServerProxy(url, {\n    plugins: [new LLMSummarizationPlugin()],\n    remoteServer: { name: `Server: ${url}` }\n  }))\n);","caching#Caching":"Add caching to reduce remote server load:\nconst cache = new Map();\nconst proxyServer = await createRemoteServerProxy({\n  remoteServer: config,\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Check cache first\n      const cacheKey = `${context.toolName}:${JSON.stringify(context.args)}`;\n      const cached = cache.get(cacheKey);\n      \n      if (cached && Date.now() - cached.timestamp < 300000) { // 5 min TTL\n        return cached.result;\n      }\n    },\n    afterToolCall: async (context, result) => {\n      // Cache successful responses\n      if (!result.isError) {\n        const cacheKey = `${context.toolName}:${JSON.stringify(context.args)}`;\n        cache.set(cacheKey, {\n          result,\n          timestamp: Date.now()\n        });\n      }\n      return result;\n    }\n  }\n});","best-practices#Best Practices":"","1-security#1. Security":"Always use HTTPS/WSS for remote connections\nValidate and sanitize all inputs and outputs\nUse environment variables for API keys and credentials\nImplement authentication and authorization\nAdd rate limiting to prevent abuse","2-reliability#2. Reliability":"Configure appropriate timeouts\nImplement retry logic for transient failures\nAdd health checks for remote server connectivity\nUse circuit breakers for failing remote services\nLog all errors for debugging","3-performance#3. Performance":"Cache frequently requested data\nUse connection pooling for multiple requests\nMonitor response times and set alerts\nImplement backpressure handling\nConsider async processing for heavy workloads","4-monitoring#4. Monitoring":"Track success/failure rates\nMonitor response times and latencies\nSet up alerts for connection failures\nLog all proxy operations for audit trails\nUse distributed tracing for end-to-end visibility\nReady to proxy remote servers? This feature enables you to enhance any external MCP service with powerful plugins and monitoring without touching the remote server code.","next-steps#Next Steps":"Plugin System: Learn about available plugins for enhancement\nExamples: See more real-world implementations\nAPI Reference: Complete API documentation\nDeployment: Deploy proxies to production"}},"/use-cases":{"title":"Use Cases","data":{"":"Complete examples of common integration patterns using the MCP Proxy Wrapper.\nAll examples include error handling, logging, and security patterns.","authentication--authorization#Authentication & Authorization":"Add API key validation and user authentication to any MCP server:\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nconst authProxy = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      // API key validation\n      if (!await validateApiKey(context.args.apiKey)) {\n        return {\n          result: {\n            content: [{ type: 'text', text: 'Invalid API key' }],\n            isError: true\n          }\n        };\n      }\n      \n      // Role-based access control\n      const userRole = await getUserRole(context.args.apiKey);\n      if (context.toolName === 'admin-only-tool' && userRole !== 'admin') {\n        return {\n          result: {\n            content: [{ type: 'text', text: 'Insufficient permissions' }],\n            isError: true\n          }\n        };\n      }\n      \n      // Add user context for downstream tools\n      context.args._userId = await getUserId(context.args.apiKey);\n      context.args._userRole = userRole;\n    }\n  }\n});\nasync function validateApiKey(apiKey: string): Promise<boolean> {\n  if (!apiKey) return false;\n  \n  // Check API key format\n  if (!/^sk-[a-zA-Z0-9]{32}$/.test(apiKey)) {\n    return false;\n  }\n  \n  // Validate against database/service\n  const user = await database.findUserByApiKey(apiKey);\n  return user && user.isActive;\n}","rate-limiting#Rate Limiting":"Implement sophisticated rate limiting with different limits per user and tool:\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\n// In-memory rate limiter (use Redis for production)\nclass RateLimiter {\n  private limits = new Map<string, { count: number; resetTime: number }>();\n  \n  async isExceeded(key: string, maxRequests: number, windowMs: number): Promise<boolean> {\n    const now = Date.now();\n    const limit = this.limits.get(key);\n    \n    if (!limit || now > limit.resetTime) {\n      this.limits.set(key, { count: 1, resetTime: now + windowMs });\n      return false;\n    }\n    \n    if (limit.count >= maxRequests) {\n      return true;\n    }\n    \n    limit.count++;\n    return false;\n  }\n  \n  async increment(key: string): Promise<void> {\n    // Already handled in isExceeded\n  }\n}\nconst rateLimiter = new RateLimiter();\nconst rateLimitedProxy = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      const userId = context.args._userId || 'anonymous';\n      \n      // Different limits for different tools\n      const limits = {\n        'expensive-ai-tool': { maxRequests: 10, windowMs: 60 * 1000 }, // 10/minute\n        'data-analysis': { maxRequests: 100, windowMs: 60 * 1000 },    // 100/minute\n        default: { maxRequests: 1000, windowMs: 60 * 1000 }            // 1000/minute\n      };\n      \n      const limit = limits[context.toolName] || limits.default;\n      const rateLimitKey = `${userId}:${context.toolName}`;\n      \n      if (await rateLimiter.isExceeded(rateLimitKey, limit.maxRequests, limit.windowMs)) {\n        return {\n          result: {\n            content: [{\n              type: 'text',\n              text: `Rate limit exceeded. Max ${limit.maxRequests} requests per ${limit.windowMs / 1000}s for ${context.toolName}`\n            }],\n            isError: true,\n            _meta: {\n              rateLimited: true,\n              resetAfter: limit.windowMs\n            }\n          }\n        };\n      }\n      \n      await rateLimiter.increment(rateLimitKey);\n    }\n  }\n});","intelligent-caching#Intelligent Caching":"Cache results with smart invalidation and compression:\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { createHash } from 'crypto';\nclass IntelligentCache {\n  private cache = new Map<string, {\n    data: any;\n    timestamp: number;\n    hits: number;\n    size: number;\n  }>();\n  \n  private maxSize = 100 * 1024 * 1024; // 100MB\n  private currentSize = 0;\n  \n  generateKey(toolName: string, args: Record<string, any>): string {\n    const normalized = JSON.stringify(args, Object.keys(args).sort());\n    return createHash('sha256').update(`${toolName}:${normalized}`).digest('hex');\n  }\n  \n  get(key: string, ttlMs: number): any | null {\n    const item = this.cache.get(key);\n    if (!item) return null;\n    \n    // Check if expired\n    if (Date.now() - item.timestamp > ttlMs) {\n      this.delete(key);\n      return null;\n    }\n    \n    item.hits++;\n    return item.data;\n  }\n  \n  set(key: string, data: any): void {\n    const serialized = JSON.stringify(data);\n    const size = Buffer.byteLength(serialized, 'utf8');\n    \n    // Evict if necessary\n    while (this.currentSize + size > this.maxSize && this.cache.size > 0) {\n      this.evictLRU();\n    }\n    \n    this.cache.set(key, {\n      data,\n      timestamp: Date.now(),\n      hits: 0,\n      size\n    });\n    \n    this.currentSize += size;\n  }\n  \n  private evictLRU(): void {\n    let lruKey = '';\n    let lruTime = Date.now();\n    \n    for (const [key, item] of this.cache.entries()) {\n      if (item.timestamp < lruTime) {\n        lruTime = item.timestamp;\n        lruKey = key;\n      }\n    }\n    \n    if (lruKey) {\n      this.delete(lruKey);\n    }\n  }\n  \n  private delete(key: string): void {\n    const item = this.cache.get(key);\n    if (item) {\n      this.currentSize -= item.size;\n      this.cache.delete(key);\n    }\n  }\n}\nconst cache = new IntelligentCache();\nconst cachedProxy = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Skip caching for real-time tools\n      const noCacheTools = ['current-time', 'random-number', 'live-data'];\n      if (noCacheTools.includes(context.toolName)) {\n        return;\n      }\n      \n      const cacheKey = cache.generateKey(context.toolName, context.args);\n      \n      // Different TTL for different tools\n      const ttlConfig = {\n        'expensive-analysis': 60 * 60 * 1000,  // 1 hour\n        'static-data': 24 * 60 * 60 * 1000,    // 24 hours\n        default: 5 * 60 * 1000                  // 5 minutes\n      };\n      \n      const ttl = ttlConfig[context.toolName] || ttlConfig.default;\n      const cached = cache.get(cacheKey, ttl);\n      \n      if (cached) {\n        console.log(`📦 Cache hit for ${context.toolName}`);\n        return {\n          result: {\n            ...cached,\n            _meta: {\n              ...cached._meta,\n              cachedAt: new Date().toISOString(),\n              cacheHit: true\n            }\n          }\n        };\n      }\n      \n      // Store cache key for afterToolCall\n      context.metadata = { ...context.metadata, cacheKey, ttl };\n    },\n    \n    afterToolCall: async (context, result) => {\n      const { cacheKey, ttl } = context.metadata || {};\n      \n      if (cacheKey && !result.result.isError) {\n        cache.set(cacheKey, result.result);\n        console.log(`💾 Cached result for ${context.toolName}`);\n      }\n      \n      return result;\n    }\n  }\n});","analytics--monitoring#Analytics & Monitoring":"Comprehensive monitoring with metrics, alerting, and performance tracking:\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nclass MetricsCollector {\n  private metrics = {\n    toolCalls: new Map<string, number>(),\n    errors: new Map<string, number>(),\n    durations: new Map<string, number[]>(),\n    userActivity: new Map<string, number>()\n  };\n  \n  increment(metric: string, tags: Record<string, string> = {}): void {\n    const key = `${metric}:${JSON.stringify(tags)}`;\n    this.metrics.toolCalls.set(key, (this.metrics.toolCalls.get(key) || 0) + 1);\n  }\n  \n  recordDuration(tool: string, duration: number): void {\n    if (!this.metrics.durations.has(tool)) {\n      this.metrics.durations.set(tool, []);\n    }\n    this.metrics.durations.get(tool)!.push(duration);\n    \n    // Keep only last 100 measurements\n    const durations = this.metrics.durations.get(tool)!;\n    if (durations.length > 100) {\n      durations.shift();\n    }\n  }\n  \n  getStats(): any {\n    const stats = {};\n    \n    // Calculate averages\n    for (const [tool, durations] of this.metrics.durations) {\n      const avg = durations.reduce((a, b) => a + b, 0) / durations.length;\n      const p95 = durations.sort((a, b) => a - b)[Math.floor(durations.length * 0.95)];\n      \n      stats[tool] = {\n        avgDuration: Math.round(avg),\n        p95Duration: Math.round(p95 || 0),\n        callCount: durations.length\n      };\n    }\n    \n    return stats;\n  }\n  \n  async sendToAnalytics(data: any): Promise<void> {\n    // Send to your analytics service\n    // Examples: DataDog, New Relic, custom endpoint\n    try {\n      await fetch('https://analytics.yourservice.com/metrics', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          timestamp: Date.now(),\n          service: 'mcp-server',\n          ...data\n        })\n      });\n    } catch (error) {\n      console.error('Analytics send failed:', error);\n    }\n  }\n}\nconst metrics = new MetricsCollector();\nconst monitoredProxy = await wrapWithProxy(server, {\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Track tool usage\n      metrics.increment('tool_calls_total', {\n        tool: context.toolName,\n        user: context.args._userId || 'anonymous'\n      });\n      \n      // Store start time\n      context.metadata = {\n        ...context.metadata,\n        startTime: Date.now(),\n        requestId: context.metadata?.requestId || Math.random().toString(36)\n      };\n      \n      console.log(`📊 [${context.metadata.requestId}] ${context.toolName} started`, {\n        user: context.args._userId,\n        args: Object.keys(context.args)\n      });\n    },\n    \n    afterToolCall: async (context, result) => {\n      const duration = Date.now() - (context.metadata?.startTime || Date.now());\n      const requestId = context.metadata?.requestId;\n      \n      // Record metrics\n      metrics.recordDuration(context.toolName, duration);\n      \n      if (result.result.isError) {\n        metrics.increment('tool_errors_total', {\n          tool: context.toolName,\n          error: 'true'\n        });\n        \n        console.error(`❌ [${requestId}] ${context.toolName} failed in ${duration}ms`, {\n          error: result.result.content[0]?.text\n        });\n        \n        // Alert on critical errors\n        if (duration > 10000 || context.toolName === 'critical-operation') {\n          await metrics.sendToAnalytics({\n            type: 'alert',\n            severity: 'high',\n            message: `Tool ${context.toolName} failed after ${duration}ms`,\n            tool: context.toolName,\n            duration,\n            error: result.result.content[0]?.text\n          });\n        }\n      } else {\n        console.log(`✅ [${requestId}] ${context.toolName} completed in ${duration}ms`);\n        \n        // Alert on performance issues\n        if (duration > 5000) {\n          await metrics.sendToAnalytics({\n            type: 'performance',\n            severity: 'medium',\n            message: `Slow tool execution: ${context.toolName} took ${duration}ms`,\n            tool: context.toolName,\n            duration\n          });\n        }\n      }\n      \n      // Add performance metadata\n      result.result._meta = {\n        ...result.result._meta,\n        duration,\n        requestId,\n        timestamp: new Date().toISOString()\n      };\n      \n      return result;\n    }\n  }\n});\n// Periodic metrics reporting\nsetInterval(async () => {\n  const stats = metrics.getStats();\n  await metrics.sendToAnalytics({\n    type: 'periodic',\n    stats\n  });\n  console.log('📈 Performance stats:', stats);\n}, 60000); // Every minute","ai-powered-enhancement#AI-Powered Enhancement":"Complete AI integration with multiple providers and intelligent fallbacks:\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\n// Configure AI summarization\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  enabled: true,\n  priority: 10,\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 200,\n    temperature: 0.3,\n    minContentLength: 500,\n    summarizeTools: ['research', 'analyze-data', 'generate-report'],\n    saveOriginal: true,\n    summarizationPrompt: `Create a concise executive summary focusing on:\n    - Key findings and insights\n    - Actionable recommendations  \n    - Important metrics and data points\n    - Critical risks or considerations\n    \n    Content to summarize:`\n  }\n});\n// Configure chat memory\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  enabled: true,\n  priority: 20,\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    enableSearch: true\n  }\n});\nconst aiEnhancedProxy = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin],\n  hooks: {\n    beforeToolCall: async (context) => {\n      // Add conversation context for AI tools\n      if (['chat', 'ask-question', 'get-insights'].includes(context.toolName)) {\n        const userId = context.args._userId;\n        if (userId) {\n          const recentMemories = await memoryPlugin.getRecentMemories(userId, 5);\n          context.args._conversationContext = recentMemories;\n        }\n      }\n      \n      console.log(`🤖 AI tool: ${context.toolName}`, {\n        hasContext: !!context.args._conversationContext,\n        willSummarize: summaryPlugin.config.options?.summarizeTools?.includes(context.toolName)\n      });\n    },\n    \n    afterToolCall: async (context, result) => {\n      // Log AI enhancements\n      if (result.result._meta?.summarized) {\n        console.log(`📝 AI Summary: ${result.result._meta.originalLength} → ${result.result._meta.summaryLength} chars`);\n      }\n      \n      if (result.result._meta?.memorySaved) {\n        console.log(`🧠 Memory: Saved response for ${context.args._userId}`);\n      }\n      \n      return result;\n    }\n  }\n});\n// Register AI-enhanced tools\naiEnhancedProxy.tool('research-with-memory', {\n  topic: z.string(),\n  userId: z.string(),\n  includeHistory: z.boolean().default(false)\n}, async (args) => {\n  let researchData = await performResearch(args.topic);\n  \n  // Include relevant historical research if requested\n  if (args.includeHistory) {\n    const historicalContext = await memoryPlugin.searchMemories(\n      args.userId,\n      args.topic,\n      3 // Get 3 most relevant past research results\n    );\n    \n    if (historicalContext.length > 0) {\n      researchData += \"\\n\\nRelevant historical research:\\n\" + \n        historicalContext.map(h => `- ${h.summary}`).join('\\n');\n    }\n  }\n  \n  return {\n    content: [{\n      type: 'text',\n      text: researchData\n    }]\n  };\n});\n// Example usage showing AI enhancement in action\nasync function demonstrateAIFeatures() {\n  // Long research response gets automatically summarized\n  const research = await client.callTool({\n    name: 'research-with-memory',\n    arguments: {\n      topic: 'artificial intelligence trends 2024',\n      userId: 'user123',\n      includeHistory: true\n    }\n  });\n  \n  console.log('Research result:', research.content[0].text);\n  console.log('Metadata:', research._meta);\n  \n  // Chat with your research history\n  const sessionId = await memoryPlugin.startChatSession('user123');\n  const chatResponse = await memoryPlugin.chatWithMemory(\n    sessionId,\n    \"What were the main AI trends I researched recently?\",\n    'user123'\n  );\n  \n  console.log('AI Chat Response:', chatResponse);\n  \n  // Retrieve original research if needed\n  if (research._meta?.originalStorageKey) {\n    const original = await summaryPlugin.getOriginalResult(research._meta.originalStorageKey);\n    console.log('Original research length:', original?.originalResult.result.content[0].text.length);\n  }\n}","multi-environment-configuration#Multi-Environment Configuration":"Production-ready configuration management:\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\ninterface EnvironmentConfig {\n  logLevel: 'debug' | 'info' | 'warn' | 'error';\n  plugins: any[];\n  hooks: any;\n  monitoring: {\n    enabled: boolean;\n    endpoint?: string;\n    interval?: number;\n  };\n}\nconst configs: Record<string, EnvironmentConfig> = {\n  development: {\n    logLevel: 'debug',\n    plugins: [\n      // Use mock plugins for faster development\n      (() => {\n        const plugin = new LLMSummarizationPlugin();\n        plugin.updateConfig({\n          options: {\n            provider: 'mock',\n            mockDelay: 10,\n            summarizeTools: ['test-tool']\n          }\n        });\n        return plugin;\n      })()\n    ],\n    hooks: {\n      beforeToolCall: async (context) => {\n        console.log(`🔧 DEV: ${context.toolName}`, context.args);\n      }\n    },\n    monitoring: {\n      enabled: false\n    }\n  },\n  \n  staging: {\n    logLevel: 'info',\n    plugins: [\n      (() => {\n        const plugin = new LLMSummarizationPlugin();\n        plugin.updateConfig({\n          options: {\n            provider: process.env.OPENAI_API_KEY ? 'openai' : 'mock',\n            openaiApiKey: process.env.OPENAI_API_KEY,\n            model: 'gpt-4o-mini',\n            maxTokens: 100, // Smaller for cost control\n            summarizeTools: ['research', 'analyze']\n          }\n        });\n        return plugin;\n      })()\n    ],\n    hooks: {\n      beforeToolCall: async (context) => {\n        console.log(`🧪 STAGING: ${context.toolName}`);\n        // Add staging-specific validation\n      }\n    },\n    monitoring: {\n      enabled: true,\n      endpoint: process.env.STAGING_METRICS_ENDPOINT,\n      interval: 30000\n    }\n  },\n  \n  production: {\n    logLevel: 'warn',\n    plugins: [\n      (() => {\n        if (!process.env.OPENAI_API_KEY) {\n          throw new Error('OPENAI_API_KEY required in production');\n        }\n        \n        const plugin = new LLMSummarizationPlugin();\n        plugin.updateConfig({\n          options: {\n            provider: 'openai',\n            openaiApiKey: process.env.OPENAI_API_KEY,\n            model: 'gpt-4o-mini',\n            maxTokens: 150,\n            temperature: 0.3,\n            summarizeTools: ['research', 'analyze-data', 'generate-report'],\n            saveOriginal: true,\n            enableHealthChecks: true\n          }\n        });\n        return plugin;\n      })()\n    ],\n    hooks: {\n      beforeToolCall: async (context) => {\n        // Production logging to structured format\n        console.log(JSON.stringify({\n          timestamp: new Date().toISOString(),\n          level: 'info',\n          tool: context.toolName,\n          userId: context.args._userId,\n          requestId: context.metadata?.requestId\n        }));\n        \n        // Production security checks\n        if (!context.args._userId) {\n          throw new Error('Authentication required in production');\n        }\n      },\n      \n      afterToolCall: async (context, result) => {\n        // Error alerting in production\n        if (result.result.isError) {\n          await sendAlert({\n            severity: 'high',\n            message: `Tool failure: ${context.toolName}`,\n            details: result.result.content[0]?.text\n          });\n        }\n        \n        return result;\n      }\n    },\n    monitoring: {\n      enabled: true,\n      endpoint: process.env.PRODUCTION_METRICS_ENDPOINT,\n      interval: 10000\n    }\n  }\n};\n// Environment-aware server creation\nasync function createEnhancedServer(baseServer: McpServer) {\n  const env = process.env.NODE_ENV || 'development';\n  const config = configs[env];\n  \n  if (!config) {\n    throw new Error(`Unknown environment: ${env}`);\n  }\n  \n  console.log(`🚀 Starting server in ${env} mode`);\n  \n  const enhancedServer = await wrapWithProxy(baseServer, {\n    plugins: config.plugins,\n    hooks: config.hooks,\n    debug: config.logLevel === 'debug'\n  });\n  \n  // Environment-specific monitoring\n  if (config.monitoring.enabled) {\n    setInterval(async () => {\n      // Send health check\n      if (config.monitoring.endpoint) {\n        try {\n          await fetch(`${config.monitoring.endpoint}/health`, {\n            method: 'POST',\n            body: JSON.stringify({\n              service: 'mcp-server',\n              env,\n              timestamp: Date.now(),\n              status: 'healthy'\n            })\n          });\n        } catch (error) {\n          console.error('Health check failed:', error);\n        }\n      }\n    }, config.monitoring.interval || 60000);\n  }\n  \n  return enhancedServer;\n}\n// Usage\nconst baseServer = new McpServer({ name: 'my-service', version: '1.0.0' });\nconst enhancedServer = await createEnhancedServer(baseServer);","next-steps#Next Steps":"Learn to build custom plugins for your specific needs\nDeploy your enhanced server to production environments\nComplete API documentation and type definitions\nCommon issues and solutions\nThese patterns are production-tested and used in real-world deployments. Adapt them to your specific requirements and scale as needed."}},"/examples":{"title":"Examples","data":{"":"Real-world implementations showing how to use the MCP Proxy Wrapper in different scenarios.","basic-ai-service#Basic AI Service":"A simple AI analysis service with logging and caching plugins:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create base server\nconst server = new McpServer({\n  name: 'ai-analysis-service',\n  version: '1.0.0'\n});\n// Configure enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'mock', // Use 'openai' for production\n    summarizeTools: ['sentiment-analysis', 'text-summary'],\n    minContentLength: 50\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    maxEntries: 100\n  }\n});\n// Wrap with proxy\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin, memoryPlugin]\n});\n// Register analysis tools\nproxiedServer.tool('sentiment-analysis', {\n  text: z.string().min(1, 'Text is required'),\n  language: z.string().optional()\n}, async (args) => {\n  const sentiment = await analyzeSentiment(args.text, args.language);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify({\n        sentiment: sentiment.label,\n        confidence: sentiment.confidence,\n        text: args.text\n      }, null, 2)\n    }]\n  };\n});\nproxiedServer.tool('text-summary', {\n  text: z.string().min(10, 'Text must be at least 10 characters'),\n  maxLength: z.number().optional().default(100)\n}, async (args) => {\n  const summary = await generateSummary(args.text, args.maxLength);\n  \n  return {\n    content: [{\n      type: 'text', \n      text: summary\n    }]\n  };\n});\n// Start server\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);","multi-tenant-saas-platform#Multi-Tenant SaaS Platform":"A complete SaaS platform with authentication and rate limiting:\n// Enhanced SaaS platform with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    summarizeTools: ['market-analysis', 'competitor-research'],\n    minContentLength: 500, // Longer threshold for business data\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 5000, // Higher limit for business use\n    sessionTimeout: 7 * 24 * 60 * 60 * 1000 // 1 week\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin] // Memory first, then summarization\n});\n// Business intelligence tools with AI enhancement\nproxiedServer.tool('market-analysis', {\n  company: z.string(),\n  metrics: z.array(z.string()),\n  timeframe: z.enum(['1M', '3M', '6M', '1Y']),\n  userId: z.string().optional()\n}, async (args) => {\n  const analysis = await performMarketAnalysis(args);\n  // Plugin automatically summarizes complex analysis data\n  return { content: [{ type: 'text', text: JSON.stringify(analysis, null, 2) }] };\n});\nproxiedServer.tool('competitor-research', {\n  industry: z.string(),\n  region: z.string().optional(),\n  userId: z.string().optional()\n}, async (args) => {\n  const research = await conductCompetitorResearch(args);\n  // Plugin saves research to memory for future reference\n  return { content: [{ type: 'text', text: JSON.stringify(research, null, 2) }] };\n});","gaming-platform-with-usage-tracking#Gaming Platform with Usage Tracking":"A gaming service with usage analytics and caching:\n// Gaming platform with memory and AI summarization\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 2000, // Store lots of game sessions\n    maxSessions: 500, // Support many concurrent players\n    excludeTools: [] // Save all gaming tools\n  }\n});\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['ai-dungeon-master'], // Summarize long narrative responses\n    minContentLength: 200,\n    saveOriginal: true\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Gaming tools with user tracking\nproxiedServer.tool('generate-character', {\n  class: z.enum(['warrior', 'mage', 'rogue', 'cleric']),\n  level: z.number().min(1).max(20),\n  background: z.string().optional(),\n  userId: z.string()\n}, async (args) => {\n  const character = await generateCharacter(args);\n  return { content: [{ type: 'text', text: JSON.stringify(character) }] };\n});\nproxiedServer.tool('ai-dungeon-master', {\n  scenario: z.string(),\n  playerAction: z.string(),\n  context: z.string().optional(),\n  userId: z.string()\n}, async (args) => {\n  const response = await generateDMResponse(args);\n  return { content: [{ type: 'text', text: response }] };\n});","development-tools-api#Development Tools API":"A developer-focused API with comprehensive logging and metadata:\n// Developer tools with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['code-review', 'security-scan'],\n    minContentLength: 300, // Code reviews can be long\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 1000,\n    sessionTimeout: 2 * 24 * 60 * 60 * 1000 // 2 days for dev work\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Development tools with AI enhancement\nproxiedServer.tool('code-review', {\n  code: z.string(),\n  language: z.string(),\n  focusAreas: z.array(z.enum(['security', 'performance', 'maintainability', 'style'])).optional(),\n  userId: z.string().optional()\n}, async (args) => {\n  const review = await performCodeReview(args);\n  // Plugin automatically summarizes detailed code review results\n  return { content: [{ type: 'text', text: JSON.stringify(review, null, 2) }] };\n});\nproxiedServer.tool('security-scan', {\n  code: z.string(),\n  language: z.string(),\n  scanType: z.enum(['static', 'dependency', 'comprehensive']).default('comprehensive'),\n  userId: z.string().optional()\n}, async (args) => {\n  const vulnerabilities = await scanForVulnerabilities(args);\n  // Plugin saves scan results to memory for future reference\n  return { content: [{ type: 'text', text: JSON.stringify(vulnerabilities, null, 2) }] };\n});","blockchain-analytics-server#Blockchain Analytics Server":"Enhancing a Web3 analytics server with AI summarization and memory capabilities:\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\n// Create blockchain analytics server\nconst server = new McpServer({\n  name: 'web3-stats-server-enhanced',\n  version: '2.0.0'\n}, {\n  capabilities: { tools: {} }\n});\n// Configure AI enhancement plugins\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY, // Set via environment\n    model: 'gpt-4o-mini',\n    summarizeTools: [\n      'get_evm_balances', \n      'get_evm_transactions',\n      'get_evm_collectibles',\n      'get_token_holders'\n    ],\n    minContentLength: 200, // Blockchain data can be verbose\n    saveOriginal: true\n  },\n  enabled: true,\n  priority: 10\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    provider: process.env.NODE_ENV === 'production' ? 'openai' : 'mock',\n    openaiApiKey: process.env.OPENAI_API_KEY, // Set via environment\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 2000, // Store lots of blockchain queries\n    sessionTimeout: 24 * 60 * 60 * 1000, // 24 hours for analysis sessions\n    excludeTools: ['ping_dune_server', 'ping_blockscout'] // Skip health checks\n  },\n  enabled: true,\n  priority: 20\n});\n// Wrap server with AI enhancement\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, config: { priority: 20 } },\n    { plugin: summaryPlugin, config: { priority: 10 } }\n  ],\n  debug: process.env.NODE_ENV !== 'production'\n});\n// Blockchain analysis tools (now AI-enhanced)\nproxiedServer.tool('get_evm_balances', 'Get wallet token balances with AI insights', {\n  walletAddress: z.string().describe('EVM wallet address (0x...)'),\n  chainId: z.string().optional().describe('Chain ID (1 for Ethereum)'),\n  limit: z.number().optional().describe('Max results to return')\n}, async ({ walletAddress, chainId = '1', limit = 50 }) => {\n  // Call external blockchain API (Dune, Alchemy, etc.)\n  const balances = await fetchWalletBalances(walletAddress, chainId, limit);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(balances, null, 2)\n    }]\n  };\n  // AI plugin will automatically:\n  // 1. Generate summary: \"This wallet holds $X.XX across Y tokens...\"\n  // 2. Save to memory for chat: \"Tell me about wallets with high ETH balances\"\n});\nproxiedServer.tool('get_evm_transactions', 'Get transaction history with AI analysis', {\n  walletAddress: z.string().describe('EVM wallet address'),\n  limit: z.number().optional().describe('Number of transactions'),\n  includeTokenTransfers: z.boolean().optional().describe('Include token transfers')\n}, async ({ walletAddress, limit = 25, includeTokenTransfers = true }) => {\n  const transactions = await fetchTransactionHistory(\n    walletAddress, \n    limit, \n    includeTokenTransfers\n  );\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(transactions, null, 2)\n    }]\n  };\n  // AI enhancement provides:\n  // - Summary of transaction patterns\n  // - Memory storage for behavioral analysis\n  // - Chat interface: \"What DeFi protocols does this wallet use?\"\n});\nproxiedServer.tool('get_token_holders', 'Analyze token distribution with insights', {\n  contractAddress: z.string().describe('Token contract address'),\n  chainId: z.string().optional().describe('Chain ID'),\n  limit: z.number().optional().describe('Number of holders to analyze')\n}, async ({ contractAddress, chainId = '1', limit = 100 }) => {\n  const holders = await analyzeTokenHolders(contractAddress, chainId, limit);\n  \n  return {\n    content: [{\n      type: 'text',\n      text: JSON.stringify(holders, null, 2)\n    }]\n  };\n  // AI creates executive summaries of:\n  // - Holder concentration analysis\n  // - Whale identification\n  // - Distribution patterns\n});\n// Helper functions (implement with your preferred blockchain data provider)\nasync function fetchWalletBalances(address: string, chainId: string, limit: number) {\n  // Example using environment-based API configuration\n  const apiKey = process.env.BLOCKCHAIN_API_KEY; // Not committed to git!\n  const response = await fetch(`https://api.example.com/v1/balances/${address}?chain=${chainId}&limit=${limit}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\nasync function fetchTransactionHistory(address: string, limit: number, includeTokens: boolean) {\n  const apiKey = process.env.BLOCKCHAIN_API_KEY;\n  const response = await fetch(`https://api.example.com/v1/transactions/${address}?limit=${limit}&tokens=${includeTokens}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\nasync function analyzeTokenHolders(contract: string, chainId: string, limit: number) {\n  const apiKey = process.env.BLOCKCHAIN_API_KEY;\n  const response = await fetch(`https://api.example.com/v1/tokens/${contract}/holders?chain=${chainId}&limit=${limit}`, {\n    headers: { 'Authorization': `Bearer ${apiKey}` }\n  });\n  return response.json();\n}\n// Start the enhanced blockchain server\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);\nconsole.log('Enhanced Web3 Analytics Server started with AI capabilities');","environment-configuration-for-blockchain-server#Environment Configuration for Blockchain Server":"# .env file (never commit this!)\nNODE_ENV=production\nBLOCKCHAIN_API_KEY=your_blockchain_api_key_here\nOPENAI_API_KEY=your_openai_api_key_here\n# Development\nNODE_ENV=development\n# Mock providers will be used automatically","docker-configuration-for-web3-server#Docker Configuration for Web3 Server":"# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\n# Never include API keys in Docker images!\n# Use environment variables or secrets management\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  web3-analytics:\n    build: .\n    environment:\n      - NODE_ENV=production\n      # Use Docker secrets or external config for API keys\n      - BLOCKCHAIN_API_KEY_FILE=/run/secrets/blockchain_api_key\n      - OPENAI_API_KEY_FILE=/run/secrets/openai_api_key\n    secrets:\n      - blockchain_api_key\n      - openai_api_key\n    ports:\n      - \"3000:3000\"\nsecrets:\n  blockchain_api_key:\n    external: true\n  openai_api_key:\n    external: true","testing-blockchain-ai-enhancement#Testing Blockchain AI Enhancement":"// tests/blockchain.test.ts\nimport { describe, test, expect } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\ndescribe('Blockchain AI Enhancement', () => {\n  test('enhances wallet balance analysis', async () => {\n    const server = new McpServer('test-web3-server', '1.0.0');\n    \n    // Mock blockchain tool\n    server.tool('get_evm_balances', {}, async () => ({\n      content: [{\n        type: 'text',\n        text: JSON.stringify({\n          address: '0x1234...5678',\n          totalValue: 150000.50,\n          tokens: [\n            { symbol: 'ETH', balance: '50.5', value: 100000 },\n            { symbol: 'USDC', balance: '50000', value: 50000 }\n          ]\n        })\n      }]\n    }));\n    \n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'mock',\n        summarizeTools: ['get_evm_balances'],\n        minContentLength: 50\n      }\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { \n      plugins: [summaryPlugin] \n    });\n    \n    const result = await proxiedServer.callTool('get_evm_balances', {\n      walletAddress: '0x1234567890123456789012345678901234567890'\n    });\n    \n    // Verify AI enhancement\n    expect(result._meta?.summarized).toBe(true);\n    expect(result.content[0].text).toContain('Summary:');\n    expect(result._meta?.originalStorageKey).toBeDefined();\n  });\n});","content-platform-with-usage-limits#Content Platform with Usage Limits":"A content creation platform with user tier management:\n// Content platform with AI summarization and memory\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    summarizeTools: ['advanced-article'], // Only summarize premium content\n    minContentLength: 500,\n    saveOriginal: true\n  }\n});\nconst memoryPlugin = new ChatMemoryPlugin();\nmemoryPlugin.updateConfig({\n  options: {\n    saveResponses: true,\n    enableChat: true,\n    maxEntries: 3000, // Store lots of content\n    sessionTimeout: 30 * 24 * 60 * 60 * 1000, // 30 days for content work\n    excludeTools: [] // Save all content creation\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [memoryPlugin, summaryPlugin]\n});\n// Content creation tools with user tiers\nproxiedServer.tool('simple-blog-post', {\n  topic: z.string(),\n  tone: z.enum(['professional', 'casual', 'humorous']).default('professional'),\n  length: z.enum(['short', 'medium', 'long']).default('medium'),\n  userId: z.string()\n}, async (args) => {\n  const post = await generateBlogPost(args);\n  return { content: [{ type: 'text', text: post }] };\n});\nproxiedServer.tool('advanced-article', {  // Premium only\n  topic: z.string(),\n  sources: z.array(z.string()),\n  seoKeywords: z.array(z.string()),\n  targetAudience: z.string(),\n  userId: z.string()\n}, async (args) => {\n  const article = await generateAdvancedArticle(args);\n  return { content: [{ type: 'text', text: article }] };\n});","production-configuration-examples#Production Configuration Examples":"","environment-based-setup#Environment-Based Setup":"// config/index.ts\ninterface Config {\n  database: string;\n  logLevel: string;\n  rateLimits: Record<string, number>;\n  cacheSettings: {\n    ttl: number;\n    maxSize: number;\n  };\n}\nconst configs: Record<string, Config> = {\n  development: {\n    database: 'sqlite:./dev.db',\n    logLevel: 'debug',\n    rateLimits: {\n      'free': 10,\n      'premium': 1000\n    },\n    cacheSettings: {\n      ttl: 60000,  // 1 minute for testing\n      maxSize: 100\n    }\n  },\n  production: {\n    database: process.env.DATABASE_URL!,\n    logLevel: 'info',\n    rateLimits: {\n      'free': 100,\n      'premium': 10000,\n      'enterprise': 100000\n    },\n    cacheSettings: {\n      ttl: 300000,  // 5 minutes\n      maxSize: 10000\n    }\n  }\n};\nexport const config = configs[process.env.NODE_ENV || 'development'];","docker-deployment#Docker Deployment":"# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml\nversion: '3.8'\nservices:\n  mcp-server:\n    build: .\n    ports:\n      - \"3000:3000\"\n    environment:\n      - NODE_ENV=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/mcpserver\n      - LOG_LEVEL=info\n      - CACHE_TTL=300000\n    depends_on:\n      - db\n  \n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: mcpserver\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\nvolumes:\n  postgres_data:","testing-examples#Testing Examples":"","integration-testing#Integration Testing":"// tests/integration.test.ts\nimport { describe, test, expect, beforeEach } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\ndescribe('Plugin Integration Tests', () => {\n  let proxiedServer: any;\n  \n  beforeEach(async () => {\n    // Create test server with sample tool\n    const server = new McpServer('test-server', '1.0.0');\n    \n    server.tool('test-tool', {\n      text: z.string()\n    }, async (args) => {\n      return {\n        content: [{ \n          type: 'text', \n          text: `This is a long response that should be summarized because it exceeds the minimum length: ${args.text}` \n        }]\n      };\n    });\n    \n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'mock', // Use mock for testing\n        minContentLength: 50,\n        summarizeTools: ['test-tool']\n      }\n    });\n    \n    const memoryPlugin = new ChatMemoryPlugin();\n    memoryPlugin.updateConfig({\n      options: {\n        saveResponses: true,\n        maxEntries: 10\n      }\n    });\n    \n    proxiedServer = await wrapWithProxy(server, { \n      plugins: [memoryPlugin, summaryPlugin] \n    });\n  });\n  test('summarizes long responses', async () => {\n    const result = await proxiedServer.callTool('test-tool', {\n      text: 'This is a long response that should be summarized by the plugin because it exceeds the minimum length threshold for summarization.'\n    });\n    expect(result.result._meta?.summarized).toBe(true);\n    expect(result.result.content[0].text).toContain('Summary:');\n  });\n  test('saves responses to memory', async () => {\n    await proxiedServer.callTool('test-tool', { text: 'Test content', userId: 'user123' });\n    \n    const memoryPlugin = proxiedServer.plugins.find(p => p.name === 'chat-memory-plugin');\n    const history = memoryPlugin.getConversationHistory('user123', 10);\n    expect(history.length).toBe(1);\n    expect(history[0].response.content).toContain('Test content');\n  });\n});","load-testing#Load Testing":"// tests/load.test.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\nasync function loadTest() {\n  // Create test server\n  const server = new McpServer('load-test-server', '1.0.0');\n  \n  server.tool('test-tool', {\n    data: z.string()\n  }, async (args) => {\n    return {\n      content: [{ type: 'text', text: `Processed: ${args.data}` }]\n    };\n  });\n  \n  const proxiedServer = await wrapWithProxy(server, { plugins: [] });\n  \n  const promises = [];\n  const startTime = Date.now();\n  \n  // Simulate 100 concurrent calls\n  for (let i = 0; i < 100; i++) {\n    promises.push(proxiedServer.callTool('test-tool', { data: `test-${i}` }));\n  }\n  \n  await Promise.all(promises);\n  const duration = Date.now() - startTime;\n  \n  console.log(`Processed 100 calls in ${duration}ms`);\n  console.log(`Average: ${duration / 100}ms per call`);\n}","common-use-cases#Common Use Cases":"Monetize AI analysis, generation, and processing tools\nWeb3 data analysis with AI summaries and memory\nCode review, security scanning, and development tools\nWriting, design, and creative tools with freemium models\nMulti-tenant platforms with enhanced functionality\nReady to implement? These examples show real production patterns that you can adapt for your specific use case.","next-steps#Next Steps":"API Reference: Complete API documentation\nDeployment: Production deployment guide\nPlugins: Explore available plugins\nGetting Started: Basic setup guide"}},"/getting-started":{"title":"Getting Started","data":{"":"Add enterprise features to any existing MCP server without changing a single line of your original code. Complete setup in under 5 minutes.\nThe MCP Proxy Wrapper requires Node.js 18+ and works with any existing MCP server without code changes.","-5-minute-setup#🚀 5-Minute Setup":"","step-1-install-30-seconds#Step 1: Install (30 seconds)":"npm install mcp-proxy-wrapper","step-2-wrap-your-existing-server-3-minutes#Step 2: Wrap Your Existing Server (3 minutes)":"Your existing server code (NO CHANGES REQUIRED):\n// server.js - Your existing MCP server\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { z } from 'zod';\nconst server = new McpServer({\n  name: 'my-existing-server',\n  version: '1.0.0'\n});\n// Your existing tools work exactly as before\nserver.tool('getData', {\n  query: z.string()\n}, async (args) => {\n  const data = await fetchData(args.query);\n  return {\n    content: [{ type: 'text', text: data }]\n  };\n});\nAdd proxy wrapper (new file: enhanced-server.js):\n// enhanced-server.js - Your enhanced server with zero changes to existing code\nimport { wrapWithProxy, LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nimport { server } from './server.js'; // Import your existing server\n// Configure the summarization plugin\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai',\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    minContentLength: 200 // Auto-summarize responses over 200 chars\n  }\n});\n// Enhance your existing server with AI and monitoring\nconst enhancedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin],\n  hooks: {\n    beforeToolCall: async (context) => {\n      console.log(`🔧 [${new Date().toISOString()}] Calling: ${context.toolName}`);\n      console.log(`📝 Args:`, context.args);\n    },\n    afterToolCall: async (context, result) => {\n      console.log(`✅ [${new Date().toISOString()}] Completed: ${context.toolName}`);\n      return result;\n    }\n  }\n});\n// Your existing tools are now enhanced with:\n// ✅ AI-powered summarization for long responses  \n// ✅ Automatic request/response logging\n// ✅ Performance monitoring\n// ✅ Plugin extensibility","step-3-use-your-enhanced-server-1-minute#Step 3: Use Your Enhanced Server (1 minute)":"import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\n// Start your enhanced server\nconst transport = new StdioServerTransport();\nawait enhancedServer.connect(transport);\n🎉 That's it! Your server now has enterprise features without any changes to your original code.","-see-the-difference#🔍 See the Difference":"","before-your-original-server#Before (Your Original Server)":"// Call your original tool\nconst result = await client.callTool({\n  name: 'getData',\n  arguments: { query: 'AI trends' }\n});\nconsole.log(result.content[0].text); \n// Output: \"Artificial intelligence trends include machine learning, natural language processing, computer vision...\"\n// (No logging, no summarization, no monitoring)","after-with-proxy-wrapper#After (With Proxy Wrapper)":"// Same tool call, enhanced results\nconst result = await client.callTool({\n  name: 'getData',\n  arguments: { query: 'AI trends' }\n});\n// Console output shows automatic logging:\n// 🔧 [2024-01-15T10:30:00.000Z] Calling: getData\n// 📝 Args: { query: 'AI trends' }\n// ✅ [2024-01-15T10:30:02.000Z] Completed: getData\nconsole.log(result.content[0].text);\n// Output: \"Summary: Key AI trends include ML advances, NLP breakthroughs...\"\n// (Automatically summarized by AI!)\nconsole.log(result._meta);\n// {\n//   summarized: true,\n//   originalLength: 1200,\n//   summaryLength: 150,\n//   processedAt: \"2024-01-15T10:30:02.000Z\"\n// }\n✨ Your server instantly gained:\n🤖 AI Summarization - Long responses automatically summarized\n📊 Request Logging - Full visibility into tool usage\n⚡ Performance Monitoring - Response times and metadata\n🔧 Extensibility - Easy to add more plugins\n🛡️ Enterprise Ready - Authentication and rate limiting hooks available","your-first-plugin#Your First Plugin":"Let's add the LLM Summarization plugin to enhance your tools:","configure-the-plugin#Configure the Plugin":"import { LLMSummarizationPlugin } from 'mcp-proxy-wrapper';\nconst summaryPlugin = new LLMSummarizationPlugin();\nsummaryPlugin.updateConfig({\n  options: {\n    provider: 'openai', // or 'mock' for testing\n    openaiApiKey: process.env.OPENAI_API_KEY,\n    model: 'gpt-4o-mini',\n    maxTokens: 150,\n    temperature: 0.3,\n    summarizeTools: ['long-analysis'],\n    minContentLength: 100\n  }\n});\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [summaryPlugin]\n});","test-summarization#Test Summarization":"// This tool now has automatic summarization\nproxiedServer.tool('long-analysis', {\n  data: z.string()\n}, async (args) => {\n  const result = await performLongAnalysis(args.data);\n  // Plugin automatically summarizes long responses\n  return result;\n});","development-workflow#Development Workflow":"","environment-setup#Environment Setup":"Create a .env file for your configuration:\n# OpenAI API key for LLM plugins\nOPENAI_API_KEY=sk-your-openai-key-here\n# Optional: Logging level\nLOG_LEVEL=debug","project-structure#Project Structure":"my-mcp-server/\n├── src/\n│   ├── index.ts          # Main server file\n│   ├── tools/            # Your tool implementations\n│   └── config/           # Configuration\n├── package.json\n├── .env                  # Environment variables\n└── tsconfig.json","sample-server-implementation#Sample Server Implementation":"// src/index.ts\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nimport { wrapWithProxy, LLMSummarizationPlugin, ChatMemoryPlugin } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\nasync function main() {\n  // Create base server\n  const server = new McpServer({\n    name: 'my-ai-tools',\n    version: '1.0.0'\n  });\n  // Configure plugins\n  const plugins = [];\n  \n  if (process.env.OPENAI_API_KEY) {\n    const summaryPlugin = new LLMSummarizationPlugin();\n    summaryPlugin.updateConfig({\n      options: {\n        provider: 'openai',\n        openaiApiKey: process.env.OPENAI_API_KEY,\n        model: 'gpt-4o-mini',\n        maxTokens: 150\n      }\n    });\n    plugins.push(summaryPlugin);\n    \n    const memoryPlugin = new ChatMemoryPlugin();\n    memoryPlugin.updateConfig({\n      options: {\n        saveResponses: true,\n        maxEntries: 100,\n        enableChat: true\n      }\n    });\n    plugins.push(memoryPlugin);\n  }\n  // Wrap with proxy\n  const proxiedServer = await wrapWithProxy(server, { plugins });\n  // Register tools\n  proxiedServer.tool('text-analysis', {\n    text: z.string(),\n    analysisType: z.enum(['sentiment', 'summary', 'keywords'])\n  }, async (args) => {\n    // Your AI analysis logic here\n    const result = await analyzeText(args.text, args.analysisType);\n    \n    return {\n      content: [{\n        type: 'text',\n        text: JSON.stringify(result, null, 2)\n      }]\n    };\n  });\n  // Start server\n  const transport = new StdioServerTransport();\n  await proxiedServer.connect(transport);\n}\nmain().catch(console.error);","testing-your-server#Testing Your Server":"","manual-testing-with-mcp-inspector#Manual Testing with MCP Inspector":"# Install MCP Inspector\nnpm install -g @modelcontextprotocol/inspector\n# Test your server\nmcp-inspector node dist/index.js","automated-testing#Automated Testing":"// tests/server.test.ts\nimport { describe, test, expect } from '@jest/globals';\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js';\nimport { wrapWithProxy } from 'mcp-proxy-wrapper';\nimport { z } from 'zod';\ndescribe('My MCP Server', () => {\n  test('tool returns expected result', async () => {\n    // Create test server\n    const server = new McpServer('test-server', '1.0.0');\n    \n    // Register test tool\n    server.tool('text-analysis', {\n      text: z.string(),\n      analysisType: z.enum(['sentiment', 'readability'])\n    }, async (args) => {\n      return {\n        content: [{ type: 'text', text: `Analysis result: ${args.analysisType} is positive` }]\n      };\n    });\n    \n    const proxiedServer = await wrapWithProxy(server, { plugins: [] });\n    \n    const result = await proxiedServer.callTool('text-analysis', {\n      text: 'This is great!',\n      analysisType: 'sentiment'\n    });\n    \n    expect(result.content[0].text).toContain('positive');\n  });\n});","transport-options#Transport Options":"The proxy wrapper supports all MCP transport methods:\n// STDIO (most common for CLI tools)\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';\nconst transport = new StdioServerTransport();\nawait proxiedServer.connect(transport);\n// WebSocket (for web applications)\nimport { WebSocketTransport } from '@modelcontextprotocol/sdk/server/websocket.js';\nconst transport = new WebSocketTransport({ port: 3000 });\nawait proxiedServer.connect(transport);\n// HTTP with SSE (for REST APIs)\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\nconst transport = new SSEServerTransport('/mcp', (request, response) => {\n  // Handle HTTP requests\n});\nawait proxiedServer.connect(transport);\n// InMemory (for testing)\nimport { InMemoryTransport } from '@modelcontextprotocol/sdk/inMemory.js';\nconst { client, server: transport } = InMemoryTransport.create();\nawait proxiedServer.connect(transport);","common-patterns#Common Patterns":"","environment-based-configuration#Environment-Based Configuration":"const config = {\n  development: {\n    logLevel: 'debug',\n    plugins: []\n  },\n  production: {\n    logLevel: 'info', \n    plugins: [\n      (() => {\n        const plugin = new LLMSummarizationPlugin();\n        plugin.updateConfig({\n          options: {\n            provider: 'openai',\n            openaiApiKey: process.env.OPENAI_API_KEY!, // Set via environment\n            model: 'gpt-4o-mini'\n          }\n        });\n        return plugin;\n      })()\n    ]\n  }\n};\nconst currentConfig = config[process.env.NODE_ENV || 'development'];","security-best-practices#Security Best Practices":"API Key Security: Never commit API keys to version control. Always use environment variables or secure secrets management.","environment-variables#Environment Variables":"Create a .env file for local development (never commit this file):\n# .env (add to .gitignore!)\nNODE_ENV=development\nOPENAI_API_KEY=sk-your-openai-key-here\nBLOCKCHAIN_API_KEY=your-blockchain-api-key-here\nDATABASE_URL=postgresql://user:pass@localhost:5432/myapp","git-security#Git Security":"Ensure your .gitignore includes:\n# Environment files\n.env\n.env.local\n.env.production\n.env.*.local\n# API keys and secrets\n**/config/secrets.json\n**/config/*.key\n*.pem\n# Build artifacts with embedded secrets\ndist/\nbuild/","production-deployment#Production Deployment":"Use secure environment variable injection:\n# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY dist/ ./dist/\n# Never COPY .env files into Docker images!\n# Use runtime environment variables instead\nCMD [\"node\", \"dist/index.js\"]\n# docker-compose.yml or Kubernetes manifests\nversion: '3.8'\nservices:\n  mcp-server:\n    build: .\n    environment:\n      - NODE_ENV=production\n      # Reference external secrets, never inline API keys\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - BLOCKCHAIN_API_KEY=${BLOCKCHAIN_API_KEY}\n    # Use Docker secrets or external secret management\n    secrets:\n      - openai_api_key\n      - blockchain_api_key","validation-and-sanitization#Validation and Sanitization":"// Always validate sensitive inputs\nproxiedServer.tool('api-call', {\n  apiKey: z.string().min(20).max(200), // Validate API key format\n  endpoint: z.string().url(), // Ensure valid URLs only\n  data: z.object({}).passthrough() // Validate data structure\n}, async ({ apiKey, endpoint, data }) => {\n  // Additional validation\n  if (!endpoint.startsWith('https://')) {\n    throw new Error('Only HTTPS endpoints allowed');\n  }\n  \n  // Use the validated inputs safely\n  return await makeSecureApiCall(endpoint, data, apiKey);\n});","error-handling#Error Handling":"proxiedServer.tool('risky-operation', schema, async (args) => {\n  try {\n    return await performRiskyOperation(args);\n  } catch (error) {\n    // Plugin errors are handled automatically\n    // Tool errors should return MCP error format\n    return {\n      content: [{\n        type: 'text',\n        text: 'Operation failed'\n      }],\n      isError: true\n    };\n  }\n});","multiple-plugins#Multiple Plugins":"const proxiedServer = await wrapWithProxy(server, {\n  plugins: [\n    { plugin: memoryPlugin, priority: 20 },      // Memory first (higher priority)\n    { plugin: summaryPlugin, priority: 10 }      // Then summarization (lower priority)\n  ]\n});","next-steps#Next Steps":"Your server is now enhanced with plugin capabilities! Explore our other guides to add more functionality.\nHow It Works: Understand the proxy wrapper architecture\nPlugins: Add summarization, memory, and more\nExamples: See real-world implementations\nAPI Reference: Complete API documentation\nDeployment: Deploy to production","troubleshooting#Troubleshooting":"","common-issues#Common Issues":"Plugin not loading:\n# Check your environment variables\necho $OPENAI_API_KEY\n# Verify plugin configuration\nnpm run test\nTool calls failing:\n// Add debug logging\nconst proxiedServer = await wrapWithProxy(server, {\n  plugins: [plugin],\n  debug: true\n});\nTypeScript errors:\n# Ensure you have the latest types\nnpm install --save-dev @types/node\nNeed more help? Check our troubleshooting guide or open an issue on GitHub."}}}